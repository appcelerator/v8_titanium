diff --git a/BUILD.gn b/BUILD.gn
index 1dc6c7d5ac..aeb0f7334f 100644
--- a/BUILD.gn
+++ b/BUILD.gn
@@ -110,10 +110,10 @@ declare_args() {
   v8_enable_31bit_smis_on_64bit_arch = false
 
   # Disable arguments adaptor frame (sets -dV8_NO_ARGUMENTS_ADAPTOR).
-  v8_disable_arguments_adaptor =
-      v8_current_cpu == "x86" || v8_current_cpu == "x64" ||
-      v8_current_cpu == "arm" || v8_current_cpu == "arm64" ||
-      v8_current_cpu == "mipsel" || v8_current_cpu == "mips64el"
+  v8_disable_arguments_adaptor = false
+
+  # Reverse JS arguments order in the stack (sets -dV8_REVERSE_JSARGS).
+  v8_enable_reverse_jsargs = false
 
   # Sets -dOBJECT_PRINT.
   v8_enable_object_print = ""
@@ -373,6 +373,10 @@ assert(!v8_use_multi_snapshots || !v8_control_flow_integrity,
 assert(!v8_enable_heap_sandbox || v8_enable_pointer_compression,
        "V8 Heap Sandbox requires pointer compression")
 
+assert(
+    !v8_disable_arguments_adaptor || v8_enable_reverse_jsargs,
+    "Disabling the arguments adaptor frame requires reversing the JS arguments stack")
+
 assert(!v8_enable_unconditional_write_barriers || !v8_disable_write_barriers,
        "Write barriers can't be both enabled and disabled")
 
@@ -531,6 +535,9 @@ config("v8_header_features") {
   if (v8_disable_arguments_adaptor) {
     defines += [ "V8_NO_ARGUMENTS_ADAPTOR" ]
   }
+  if (v8_enable_reverse_jsargs) {
+    defines += [ "V8_REVERSE_JSARGS" ]
+  }
 }
 
 # Put defines here that are only used in our internal files and NEVER in
diff --git a/include/v8.h b/include/v8.h
index 8578f81a8b..e4b2df1a1c 100644
--- a/include/v8.h
+++ b/include/v8.h
@@ -11474,7 +11474,11 @@ template<typename T>
 Local<Value> FunctionCallbackInfo<T>::operator[](int i) const {
   // values_ points to the first argument (not the receiver).
   if (i < 0 || length_ <= i) return Local<Value>(*Undefined(GetIsolate()));
+#ifdef V8_REVERSE_JSARGS
   return Local<Value>(reinterpret_cast<Value*>(values_ + i));
+#else
+  return Local<Value>(reinterpret_cast<Value*>(values_ - i));
+#endif
 }
 
 template<typename T>
@@ -11485,7 +11489,11 @@ Local<Function> FunctionCallbackInfo<T>::Callee() const {
 template<typename T>
 Local<Object> FunctionCallbackInfo<T>::This() const {
   // values_ points to the first argument (not the receiver).
+#ifdef V8_REVERSE_JSARGS
   return Local<Object>(reinterpret_cast<Object*>(values_ - 1));
+#else
+  return Local<Object>(reinterpret_cast<Object*>(values_ + 1));
+#endif
 }
 
 
diff --git a/src/ast/ast.cc b/src/ast/ast.cc
index e8c7796abc..630001901e 100644
--- a/src/ast/ast.cc
+++ b/src/ast/ast.cc
@@ -223,6 +223,12 @@ bool FunctionLiteral::AllowsLazyCompilation() {
   return scope()->AllowsLazyCompilation();
 }
 
+bool FunctionLiteral::SafeToSkipArgumentsAdaptor() const {
+  return language_mode() == LanguageMode::kStrict &&
+         scope()->arguments() == nullptr &&
+         scope()->rest_parameter() == nullptr;
+}
+
 int FunctionLiteral::start_position() const {
   return scope()->start_position();
 }
diff --git a/src/ast/ast.h b/src/ast/ast.h
index 7b70181e6a..4213c60f24 100644
--- a/src/ast/ast.h
+++ b/src/ast/ast.h
@@ -2160,6 +2160,18 @@ class FunctionLiteral final : public Expression {
     return false;
   }
 
+  // We can safely skip the arguments adaptor frame setup even
+  // in case of arguments mismatches for strict mode functions,
+  // as long as there's
+  //
+  //   1. no use of the arguments object (either explicitly or
+  //      potentially implicitly via a direct eval() call), and
+  //   2. rest parameters aren't being used in the function.
+  //
+  // See http://bit.ly/v8-faster-calls-with-arguments-mismatch
+  // for the details here (https://crbug.com/v8/8895).
+  bool SafeToSkipArgumentsAdaptor() const;
+
   // Returns either name or inferred name as a cstring.
   std::unique_ptr<char[]> GetDebugName() const;
 
diff --git a/src/builtins/arm/builtins-arm.cc b/src/builtins/arm/builtins-arm.cc
index 5a0a59d879..310f830283 100644
--- a/src/builtins/arm/builtins-arm.cc
+++ b/src/builtins/arm/builtins-arm.cc
@@ -102,6 +102,7 @@ void Generate_JSBuiltinsConstructStubHelper(MacroAssembler* masm) {
     // correct position (including any undefined), instead of delaying this to
     // InvokeFunction.
 
+#ifdef V8_REVERSE_JSARGS
     // Set up pointer to last argument (skip receiver).
     __ add(
         r4, fp,
@@ -110,6 +111,14 @@ void Generate_JSBuiltinsConstructStubHelper(MacroAssembler* masm) {
     __ PushArray(r4, r0, r5);
     // The receiver for the builtin/api call.
     __ PushRoot(RootIndex::kTheHoleValue);
+#else
+    // The receiver for the builtin/api call.
+    __ PushRoot(RootIndex::kTheHoleValue);
+    // Set up pointer to last argument.
+    __ add(r4, fp, Operand(StandardFrameConstants::kCallerSPOffset));
+    // Copy arguments and receiver to the expression stack.
+    __ PushArray(r4, r0, r5);
+#endif
 
     // Call the function.
     // r0: number of arguments (untagged)
@@ -200,6 +209,7 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
   // Restore new target.
   __ Pop(r3);
 
+#ifdef V8_REVERSE_JSARGS
   // Push the allocated receiver to the stack.
   __ Push(r0);
   // We need two copies because we may have to return the original one
@@ -211,7 +221,16 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
 
   // Set up pointer to first argument (skip receiver).
   __ add(r4, fp,
-         Operand(StandardFrameConstants::kCallerSPOffset + kSystemPointerSize));
+      Operand(StandardFrameConstants::kCallerSPOffset + kSystemPointerSize));
+#else
+  // Push the allocated receiver to the stack. We need two copies
+  // because we may have to return the original one and the calling
+  // conventions dictate that the called function pops the receiver.
+  __ Push(r0, r0);
+
+  // Set up pointer to last argument.
+  __ add(r4, fp, Operand(StandardFrameConstants::kCallerSPOffset));
+#endif
 
   // Restore constructor function and argument count.
   __ ldr(r1, MemOperand(fp, ConstructFrameConstants::kConstructorOffset));
@@ -229,8 +248,10 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
   // Copy arguments to the expression stack.
   __ PushArray(r4, r0, r5);
 
+#ifdef V8_REVERSE_JSARGS
   // Push implicit receiver.
   __ Push(r6);
+#endif
 
   // Call the function.
   __ InvokeFunctionWithNewTarget(r1, r3, r0, CALL_FUNCTION);
@@ -366,6 +387,12 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
   __ cmp(sp, scratch);
   __ b(lo, &stack_overflow);
 
+#ifndef V8_REVERSE_JSARGS
+  // Push receiver.
+  __ ldr(scratch, FieldMemOperand(r1, JSGeneratorObject::kReceiverOffset));
+  __ Push(scratch);
+#endif
+
   // ----------- S t a t e -------------
   //  -- r1    : the JSGeneratorObject to resume
   //  -- r4    : generator function
@@ -381,6 +408,7 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
   __ ldr(r2,
          FieldMemOperand(r1, JSGeneratorObject::kParametersAndRegistersOffset));
   {
+#ifdef V8_REVERSE_JSARGS
     Label done_loop, loop;
     __ mov(r6, r3);
 
@@ -397,6 +425,21 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
     // Push receiver.
     __ ldr(scratch, FieldMemOperand(r1, JSGeneratorObject::kReceiverOffset));
     __ Push(scratch);
+#else
+    Label done_loop, loop;
+    __ mov(r6, Operand(0));
+
+    __ bind(&loop);
+    __ cmp(r6, r3);
+    __ b(ge, &done_loop);
+    __ add(scratch, r2, Operand(r6, LSL, kTaggedSizeLog2));
+    __ ldr(scratch, FieldMemOperand(scratch, FixedArray::kHeaderSize));
+    __ Push(scratch);
+    __ add(r6, r6, Operand(1));
+    __ b(&loop);
+
+    __ bind(&done_loop);
+#endif
   }
 
   // Underlying function needs to have bytecode available.
@@ -714,6 +757,7 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
     // r3: receiver
     // r0: argc
     // r4: argv, i.e. points to first arg
+#ifdef V8_REVERSE_JSARGS
     Label loop, entry;
     __ add(r6, r4, Operand(r0, LSL, kSystemPointerSizeLog2));
     // r6 points past last arg.
@@ -729,6 +773,23 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
 
     // Push the receiver.
     __ Push(r3);
+#else
+    // Push the receiver.
+    __ Push(r3);
+
+    Label loop, entry;
+    __ add(r3, r4, Operand(r0, LSL, kSystemPointerSizeLog2));
+    // r3 points past last arg.
+    __ b(&entry);
+    __ bind(&loop);
+    __ ldr(r5, MemOperand(r4, kSystemPointerSize,
+                          PostIndex));                    // read next parameter
+    __ ldr(r5, MemOperand(r5));                           // dereference handle
+    __ push(r5);                                          // push parameter
+    __ bind(&entry);
+    __ cmp(r4, r3);
+    __ b(ne, &loop);
+#endif
 
     // Setup new.target and function.
     __ mov(r3, r1);
@@ -1228,8 +1289,12 @@ static void Generate_InterpreterPushArgs(MacroAssembler* masm,
   __ mov(scratch, Operand(scratch, LSL, kSystemPointerSizeLog2));
   __ sub(start_address, start_address, scratch);
   // Push the arguments.
+#ifdef V8_REVERSE_JSARGS
   __ PushArray(start_address, num_args, scratch,
                TurboAssembler::PushArrayOrder::kReverse);
+#else
+  __ PushArray(start_address, num_args, scratch);
+#endif
 }
 
 // static
@@ -1246,15 +1311,18 @@ void Builtins::Generate_InterpreterPushArgsThenCallImpl(
   // -----------------------------------
   Label stack_overflow;
 
+#ifdef V8_REVERSE_JSARGS
   if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
     // The spread argument should not be pushed.
     __ sub(r0, r0, Operand(1));
   }
+#endif
 
   __ add(r3, r0, Operand(1));  // Add one for receiver.
 
   __ StackOverflowCheck(r3, r4, &stack_overflow);
 
+#ifdef V8_REVERSE_JSARGS
   if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
     // Don't copy receiver. Argument count is correct.
     __ mov(r3, r0);
@@ -1275,6 +1343,21 @@ void Builtins::Generate_InterpreterPushArgsThenCallImpl(
     __ sub(r2, r2, Operand(kSystemPointerSize));
     __ ldr(r2, MemOperand(r2));
   }
+#else
+  // Push "undefined" as the receiver arg if we need to.
+  if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
+    __ PushRoot(RootIndex::kUndefinedValue);
+    __ mov(r3, r0);  // Argument count is correct.
+  }
+
+  // Push the arguments. r2 and r4 will be modified.
+  Generate_InterpreterPushArgs(masm, r3, r2, r4);
+
+  if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
+    __ Pop(r2);                  // Pass the spread in a register
+    __ sub(r0, r0, Operand(1));  // Subtract one for spread
+  }
+#endif
 
   // Call the target.
   if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
@@ -1309,6 +1392,7 @@ void Builtins::Generate_InterpreterPushArgsThenConstructImpl(
 
   __ StackOverflowCheck(r5, r6, &stack_overflow);
 
+#ifdef V8_REVERSE_JSARGS
   if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
     // The spread argument should not be pushed.
     __ sub(r0, r0, Operand(1));
@@ -1330,6 +1414,21 @@ void Builtins::Generate_InterpreterPushArgsThenConstructImpl(
   } else {
     __ AssertUndefinedOrAllocationSite(r2, r5);
   }
+#else
+  // Push a slot for the receiver to be constructed.
+  __ mov(r5, Operand::Zero());
+  __ push(r5);
+
+  // Push the arguments. r4 and r5 will be modified.
+  Generate_InterpreterPushArgs(masm, r0, r4, r5);
+
+  if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
+    __ Pop(r2);                  // Pass the spread in a register
+    __ sub(r0, r0, Operand(1));  // Subtract one for spread
+  } else {
+    __ AssertUndefinedOrAllocationSite(r2, r5);
+  }
+#endif
 
   if (mode == InterpreterPushArgsMode::kArrayFunction) {
     __ AssertFunction(r1);
@@ -1493,6 +1592,7 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
   UseScratchRegisterScope temps(masm);
   Register scratch = temps.Acquire();  // Temp register is not allocatable.
   if (with_result) {
+#ifdef V8_REVERSE_JSARGS
     if (java_script_builtin) {
       __ mov(scratch, r0);
     } else {
@@ -1504,6 +1604,14 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
               sp, config->num_allocatable_general_registers() * kPointerSize +
                       BuiltinContinuationFrameConstants::kFixedFrameSize));
     }
+#else
+    // Overwrite the hole inserted by the deoptimizer with the return value from
+    // the LAZY deopt point.
+    __ str(r0,
+           MemOperand(
+               sp, config->num_allocatable_general_registers() * kPointerSize +
+                       BuiltinContinuationFrameConstants::kFixedFrameSize));
+#endif
   }
   for (int i = allocatable_register_count - 1; i >= 0; --i) {
     int code = config->GetAllocatableGeneralCode(i);
@@ -1512,6 +1620,7 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
       __ SmiUntag(Register::from_code(code));
     }
   }
+#ifdef V8_REVERSE_JSARGS
   if (java_script_builtin && with_result) {
     // Overwrite the hole inserted by the deoptimizer with the return value from
     // the LAZY deopt point. r0 contains the arguments count, the return value
@@ -1521,6 +1630,7 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
     // Recover arguments count.
     __ sub(r0, r0, Operand(BuiltinContinuationFrameConstants::kFixedSlotCount));
   }
+#endif
   __ ldr(fp, MemOperand(
                  sp, BuiltinContinuationFrameConstants::kFixedFrameSizeFromFp));
   // Load builtin index (stored as a Smi) and use it to get the builtin start
@@ -1618,11 +1728,20 @@ void Builtins::Generate_FunctionPrototypeApply(MacroAssembler* masm) {
   {
     __ LoadRoot(r5, RootIndex::kUndefinedValue);
     __ mov(r2, r5);
+#ifdef V8_REVERSE_JSARGS
     __ ldr(r1, MemOperand(sp, 0));  // receiver
     __ cmp(r0, Operand(1));
     __ ldr(r5, MemOperand(sp, kSystemPointerSize), ge);  // thisArg
     __ cmp(r0, Operand(2), ge);
     __ ldr(r2, MemOperand(sp, 2 * kSystemPointerSize), ge);  // argArray
+#else
+    __ ldr(r1, MemOperand(sp, r0, LSL, kSystemPointerSizeLog2));  // receiver
+    __ sub(r4, r0, Operand(1), SetCC);
+    __ ldr(r5, MemOperand(sp, r4, LSL, kSystemPointerSizeLog2), ge);  // thisArg
+    __ sub(r4, r4, Operand(1), SetCC, ge);
+    __ ldr(r2, MemOperand(sp, r4, LSL, kSystemPointerSizeLog2),
+           ge);  // argArray
+#endif
     __ add(sp, sp, Operand(r0, LSL, kSystemPointerSizeLog2));
     __ str(r5, MemOperand(sp, 0));
   }
@@ -1657,6 +1776,7 @@ void Builtins::Generate_FunctionPrototypeApply(MacroAssembler* masm) {
 
 // static
 void Builtins::Generate_FunctionPrototypeCall(MacroAssembler* masm) {
+#ifdef V8_REVERSE_JSARGS
   // 1. Get the callable to call (passed as receiver) from the stack.
   __ Pop(r1);
 
@@ -1673,6 +1793,45 @@ void Builtins::Generate_FunctionPrototypeCall(MacroAssembler* masm) {
 
   // 3. Adjust the actual number of arguments.
   __ sub(r0, r0, Operand(1));
+#else
+  // 1. Make sure we have at least one argument.
+  // r0: actual number of arguments
+  {
+    Label done;
+    __ cmp(r0, Operand::Zero());
+    __ b(ne, &done);
+    __ PushRoot(RootIndex::kUndefinedValue);
+    __ add(r0, r0, Operand(1));
+    __ bind(&done);
+  }
+
+  // 2. Get the callable to call (passed as receiver) from the stack.
+  // r0: actual number of arguments
+  __ ldr(r1, __ ReceiverOperand(r0));
+
+  // 3. Shift arguments and return address one slot down on the stack
+  //    (overwriting the original receiver).  Adjust argument count to make
+  //    the original first argument the new receiver.
+  // r0: actual number of arguments
+  // r1: callable
+  {
+    Register scratch = r3;
+    Label loop;
+    // Calculate the copy start address (destination). Copy end address is sp.
+    __ add(r2, sp, Operand(r0, LSL, kSystemPointerSizeLog2));
+
+    __ bind(&loop);
+    __ ldr(scratch, MemOperand(r2, -kSystemPointerSize));
+    __ str(scratch, MemOperand(r2));
+    __ sub(r2, r2, Operand(kSystemPointerSize));
+    __ cmp(r2, sp);
+    __ b(ne, &loop);
+    // Adjust the actual number of arguments and remove the top element
+    // (which is a copy of the last argument).
+    __ sub(r0, r0, Operand(1));
+    __ pop();
+  }
+#endif
 
   // 4. Call the callable.
   __ Jump(masm->isolate()->builtins()->Call(), RelocInfo::CODE_TARGET);
@@ -1694,12 +1853,23 @@ void Builtins::Generate_ReflectApply(MacroAssembler* masm) {
     __ LoadRoot(r1, RootIndex::kUndefinedValue);
     __ mov(r5, r1);
     __ mov(r2, r1);
+#ifdef V8_REVERSE_JSARGS
     __ cmp(r0, Operand(1));
     __ ldr(r1, MemOperand(sp, kSystemPointerSize), ge);  // target
     __ cmp(r0, Operand(2), ge);
     __ ldr(r5, MemOperand(sp, 2 * kSystemPointerSize), ge);  // thisArgument
     __ cmp(r0, Operand(3), ge);
     __ ldr(r2, MemOperand(sp, 3 * kSystemPointerSize), ge);  // argumentsList
+#else
+    __ sub(r4, r0, Operand(1), SetCC);
+    __ ldr(r1, MemOperand(sp, r4, LSL, kSystemPointerSizeLog2), ge);  // target
+    __ sub(r4, r4, Operand(1), SetCC, ge);
+    __ ldr(r5, MemOperand(sp, r4, LSL, kSystemPointerSizeLog2),
+           ge);  // thisArgument
+    __ sub(r4, r4, Operand(1), SetCC, ge);
+    __ ldr(r2, MemOperand(sp, r4, LSL, kSystemPointerSizeLog2),
+           ge);  // argumentsList
+#endif
     __ add(sp, sp, Operand(r0, LSL, kSystemPointerSizeLog2));
     __ str(r5, MemOperand(sp, 0));
   }
@@ -1735,6 +1905,7 @@ void Builtins::Generate_ReflectConstruct(MacroAssembler* masm) {
   {
     __ LoadRoot(r1, RootIndex::kUndefinedValue);
     __ mov(r2, r1);
+#ifdef V8_REVERSE_JSARGS
     __ mov(r4, r1);
     __ cmp(r0, Operand(1));
     __ ldr(r1, MemOperand(sp, kSystemPointerSize), ge);  // target
@@ -1745,6 +1916,19 @@ void Builtins::Generate_ReflectConstruct(MacroAssembler* masm) {
     __ ldr(r3, MemOperand(sp, 3 * kSystemPointerSize), ge);  // new.target
     __ add(sp, sp, Operand(r0, LSL, kSystemPointerSizeLog2));
     __ str(r4, MemOperand(sp, 0));  // set undefined to the receiver
+#else
+    __ str(r2, MemOperand(sp, r0, LSL, kSystemPointerSizeLog2));  // receiver
+    __ sub(r4, r0, Operand(1), SetCC);
+    __ ldr(r1, MemOperand(sp, r4, LSL, kSystemPointerSizeLog2), ge);  // target
+    __ mov(r3, r1);  // new.target defaults to target
+    __ sub(r4, r4, Operand(1), SetCC, ge);
+    __ ldr(r2, MemOperand(sp, r4, LSL, kSystemPointerSizeLog2),
+           ge);  // argumentsList
+    __ sub(r4, r4, Operand(1), SetCC, ge);
+    __ ldr(r3, MemOperand(sp, r4, LSL, kSystemPointerSizeLog2),
+           ge);  // new.target
+    __ add(sp, sp, Operand(r0, LSL, kSystemPointerSizeLog2));
+#endif
   }
 
   // ----------- S t a t e -------------
@@ -1824,6 +2008,7 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
   Label stack_overflow;
   __ StackOverflowCheck(r4, scratch, &stack_overflow);
 
+#ifdef V8_REVERSE_JSARGS
   // Move the arguments already in the stack,
   // including the receiver and the return address.
   {
@@ -1843,6 +2028,7 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
     __ bind(&check);
     __ b(ge, &copy);
   }
+#endif
 
   // Copy arguments onto the stack (thisArgument is already on the stack).
   {
@@ -1857,7 +2043,11 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
     __ cmp(scratch, r5);
     // Turn the hole into undefined as we go.
     __ LoadRoot(scratch, RootIndex::kUndefinedValue, eq);
+#ifdef V8_REVERSE_JSARGS
     __ str(scratch, MemOperand(r9, kSystemPointerSize, PostIndex));
+#else
+    __ Push(scratch);
+#endif
     __ add(r6, r6, Operand(1));
     __ b(&loop);
     __ bind(&done);
@@ -1951,6 +2141,7 @@ void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
     __ StackOverflowCheck(r5, scratch, &stack_overflow);
 
     // Forward the arguments from the caller frame.
+#ifdef V8_REVERSE_JSARGS
     // Point to the first argument to copy (skipping the receiver).
     __ add(r4, r4,
            Operand(CommonFrameConstants::kFixedFrameSizeAboveFp +
@@ -1977,17 +2168,26 @@ void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
       __ bind(&check);
       __ b(ge, &copy);
     }
+#endif
     // Copy arguments from the caller frame.
     // TODO(victorgomes): Consider using forward order as potentially more cache
     // friendly.
     {
       Label loop;
+#ifndef V8_REVERSE_JSARGS
+      // Skips frame pointer.
+      __ add(r4, r4, Operand(CommonFrameConstants::kFixedFrameSizeAboveFp));
+#endif
       __ add(r0, r0, r5);
       __ bind(&loop);
       {
         __ sub(r5, r5, Operand(1), SetCC);
         __ ldr(scratch, MemOperand(r4, r5, LSL, kSystemPointerSizeLog2));
+#ifdef V8_REVERSE_JSARGS
         __ str(scratch, MemOperand(r2, r5, LSL, kSystemPointerSizeLog2));
+#else
+        __ push(scratch);
+#endif
         __ b(ne, &loop);
       }
     }
@@ -2157,6 +2357,7 @@ void Generate_PushBoundArguments(MacroAssembler* masm) {
       __ bind(&done);
     }
 
+#ifdef V8_REVERSE_JSARGS
     // Pop receiver.
     __ Pop(r5);
 
@@ -2174,6 +2375,39 @@ void Generate_PushBoundArguments(MacroAssembler* masm) {
 
     // Push receiver.
     __ Push(r5);
+#else
+    // Reserve stack space for the [[BoundArguments]].
+    __ AllocateStackSpace(scratch);
+
+    // Relocate arguments down the stack.
+    {
+      Label loop, done_loop;
+      __ mov(r5, Operand(0));
+      __ bind(&loop);
+      __ cmp(r5, r0);
+      __ b(gt, &done_loop);
+      __ ldr(scratch, MemOperand(sp, r4, LSL, kSystemPointerSizeLog2));
+      __ str(scratch, MemOperand(sp, r5, LSL, kSystemPointerSizeLog2));
+      __ add(r4, r4, Operand(1));
+      __ add(r5, r5, Operand(1));
+      __ b(&loop);
+      __ bind(&done_loop);
+    }
+
+    // Copy [[BoundArguments]] to the stack (below the arguments).
+    {
+      Label loop;
+      __ ldr(r4, FieldMemOperand(r2, FixedArray::kLengthOffset));
+      __ SmiUntag(r4);
+      __ add(r2, r2, Operand(FixedArray::kHeaderSize - kHeapObjectTag));
+      __ bind(&loop);
+      __ sub(r4, r4, Operand(1), SetCC);
+      __ ldr(scratch, MemOperand(r2, r4, LSL, kPointerSizeLog2));
+      __ str(scratch, MemOperand(sp, r0, LSL, kPointerSizeLog2));
+      __ add(r0, r0, Operand(1));
+      __ b(gt, &loop);
+    }
+#endif
   }
   __ bind(&no_bound_arguments);
 }
@@ -2361,12 +2595,19 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
   //  -- r3 : new target (passed through to callee)
   // -----------------------------------
 
-  Label dont_adapt_arguments, stack_overflow;
+  Label dont_adapt_arguments, stack_overflow, skip_adapt_arguments;
   __ cmp(r2, Operand(kDontAdaptArgumentsSentinel));
   __ b(eq, &dont_adapt_arguments);
   __ ldr(r4, FieldMemOperand(r1, JSFunction::kSharedFunctionInfoOffset));
   __ ldr(r4, FieldMemOperand(r4, SharedFunctionInfo::kFlagsOffset));
 
+#ifndef V8_REVERSE_JSARGS
+  // This optimization is disabled when the arguments are reversed.
+  __ tst(r4,
+         Operand(SharedFunctionInfo::IsSafeToSkipArgumentsAdaptorBit::kMask));
+  __ b(ne, &skip_adapt_arguments);
+#endif
+
   // -------------------------------------------
   // Adapt arguments.
   // -------------------------------------------
@@ -2386,7 +2627,11 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
       // r1: function
       // r2: expected number of arguments
       // r3: new target (passed through to callee)
+#ifdef V8_REVERSE_JSARGS
       __ add(r0, fp, Operand(r2, LSL, kSystemPointerSizeLog2));
+#else
+      __ add(r0, fp, Operand::PointerOffsetFromSmiKey(r0));
+#endif
       // adjust for return address and receiver
       __ add(r0, r0, Operand(2 * kSystemPointerSize));
       __ sub(r4, r0, Operand(r2, LSL, kSystemPointerSizeLog2));
@@ -2415,6 +2660,7 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
       EnterArgumentsAdaptorFrame(masm);
       __ StackOverflowCheck(r2, r5, &stack_overflow);
 
+#ifdef V8_REVERSE_JSARGS
       // Fill the remaining expected arguments with undefined.
       // r0: actual number of arguments as a smi
       // r1: function
@@ -2456,6 +2702,47 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
       __ cmp(r0, fp);  // Compare before moving to next argument.
       __ sub(r0, r0, Operand(kPointerSize));
       __ b(ne, &copy);
+#else
+      // Calculate copy start address into r0 and copy end address is fp.
+      // r0: actual number of arguments as a smi
+      // r1: function
+      // r2: expected number of arguments
+      // r3: new target (passed through to callee)
+      __ add(r0, fp, Operand::PointerOffsetFromSmiKey(r0));
+
+      // Copy the arguments (including the receiver) to the new stack frame.
+      // r0: copy start address
+      // r1: function
+      // r2: expected number of arguments
+      // r3: new target (passed through to callee)
+      Label copy;
+      __ bind(&copy);
+
+      // Adjust load for return address and receiver.
+      __ ldr(r5, MemOperand(r0, 2 * kPointerSize));
+      __ push(r5);
+
+      __ cmp(r0, fp);  // Compare before moving to next argument.
+      __ sub(r0, r0, Operand(kPointerSize));
+      __ b(ne, &copy);
+
+      // Fill the remaining expected arguments with undefined.
+      // r1: function
+      // r2: expected number of arguments
+      // r3: new target (passed through to callee)
+      __ LoadRoot(r5, RootIndex::kUndefinedValue);
+      __ sub(r4, fp, Operand(r2, LSL, kPointerSizeLog2));
+      // Adjust for frame.
+      __ sub(r4, r4,
+             Operand(ArgumentsAdaptorFrameConstants::kFixedFrameSizeFromFp +
+                     kPointerSize));
+
+      Label fill;
+      __ bind(&fill);
+      __ push(r5);
+      __ cmp(sp, r4);
+      __ b(ne, &fill);
+#endif
     }
 
     // Call the entry point.
@@ -2477,6 +2764,41 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
     __ Jump(lr);
   }
 
+  // -------------------------------------------
+  // Skip adapt arguments.
+  // -------------------------------------------
+  __ bind(&skip_adapt_arguments);
+  {
+    // The callee cannot observe the actual arguments, so it's safe to just
+    // pass the expected arguments by massaging the stack appropriately. See
+    // http://bit.ly/v8-faster-calls-with-arguments-mismatch for details.
+    Label under_application, over_application;
+    __ cmp(r0, r2);
+    __ b(lt, &under_application);
+
+    __ bind(&over_application);
+    {
+      // Remove superfluous parameters from the stack.
+      __ sub(r4, r0, r2);
+      __ mov(r0, r2);
+      __ add(sp, sp, Operand(r4, LSL, kPointerSizeLog2));
+      __ b(&dont_adapt_arguments);
+    }
+
+    __ bind(&under_application);
+    {
+      // Fill remaining expected arguments with undefined values.
+      Label fill;
+      __ LoadRoot(r4, RootIndex::kUndefinedValue);
+      __ bind(&fill);
+      __ add(r0, r0, Operand(1));
+      __ push(r4);
+      __ cmp(r0, r2);
+      __ b(lt, &fill);
+      __ b(&dont_adapt_arguments);
+    }
+  }
+
   // -------------------------------------------
   // Dont adapt arguments.
   // -------------------------------------------
@@ -2998,7 +3320,12 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
 
   // FunctionCallbackInfo::values_ (points at the first varargs argument passed
   // on the stack).
+#ifdef V8_REVERSE_JSARGS
   __ add(scratch, scratch, Operand((FCA::kArgsLength + 1) * kPointerSize));
+#else
+  __ add(scratch, scratch, Operand((FCA::kArgsLength - 1) * kPointerSize));
+  __ add(scratch, scratch, Operand(argc, LSL, kPointerSizeLog2));
+#endif
   __ str(scratch, MemOperand(sp, 2 * kPointerSize));
 
   // FunctionCallbackInfo::length_.
diff --git a/src/builtins/arm64/builtins-arm64.cc b/src/builtins/arm64/builtins-arm64.cc
index 92c1fefa0a..2b39387bac 100644
--- a/src/builtins/arm64/builtins-arm64.cc
+++ b/src/builtins/arm64/builtins-arm64.cc
@@ -120,6 +120,11 @@ void Generate_JSBuiltinsConstructStubHelper(MacroAssembler* masm) {
     // stack to which arguments will be later copied.
     __ SlotAddress(x2, argc);
 
+#ifndef V8_REVERSE_JSARGS
+    // Poke the hole (receiver) in the highest slot.
+    __ Str(x4, MemOperand(x2));
+#endif
+
     // Store padding, if needed.
     __ Tbnz(slot_count_without_rounding, 0, &already_aligned);
     __ Str(padreg, MemOperand(x2, 1 * kSystemPointerSize));
@@ -136,12 +141,16 @@ void Generate_JSBuiltinsConstructStubHelper(MacroAssembler* masm) {
       Register dst = x10;
       Register src = x11;
       __ SlotAddress(dst, 0);
+#ifdef V8_REVERSE_JSARGS
       // Poke the hole (receiver).
       __ Str(x4, MemOperand(dst));
       __ Add(dst, dst, kSystemPointerSize);  // Skip receiver.
       __ Add(src, fp,
              StandardFrameConstants::kCallerSPOffset +
                  kSystemPointerSize);  // Skip receiver.
+#else
+      __ Add(src, fp, StandardFrameConstants::kCallerSPOffset);
+#endif
       __ Mov(count, argc);
       __ CopyDoubleWords(dst, src, count);
     }
@@ -323,10 +332,15 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
     Register dst = x10;
     Register src = x11;
     __ Mov(count, x12);
-    __ Poke(x0, 0);          // Add the receiver.
-    __ SlotAddress(dst, 1);  // Skip receiver.
-    __ Add(src, fp,
-           StandardFrameConstants::kCallerSPOffset + kSystemPointerSize);
+#ifdef V8_REVERSE_JSARGS
+      __ Poke(x0, 0);          // Add the receiver.
+      __ SlotAddress(dst, 1);  // Skip receiver.
+      __ Add(src, fp,
+             StandardFrameConstants::kCallerSPOffset + kSystemPointerSize);
+#else
+      __ SlotAddress(dst, 0);
+      __ Add(src, fp, StandardFrameConstants::kCallerSPOffset);
+#endif
     __ CopyDoubleWords(dst, src, count);
   }
 
@@ -501,6 +515,7 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
   {
     Label loop, done;
     __ Cbz(x10, &done);
+#ifdef V8_REVERSE_JSARGS
     __ SlotAddress(x12, x10);
     __ Add(x5, x5, Operand(x10, LSL, kTaggedSizeLog2));
     __ Add(x5, x5, Operand(FixedArray::kHeaderSize - kHeapObjectTag));
@@ -508,6 +523,15 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
     __ Sub(x10, x10, 1);
     __ LoadAnyTaggedField(x11, MemOperand(x5, -kTaggedSize, PreIndex));
     __ Str(x11, MemOperand(x12, -kSystemPointerSize, PostIndex));
+#else
+    __ Mov(x12, 0);
+    __ Bind(&loop);
+    __ Sub(x10, x10, 1);
+    __ Add(x11, x5, Operand(x12, LSL, kTaggedSizeLog2));
+    __ LoadAnyTaggedField(x11, FieldMemOperand(x11, FixedArray::kHeaderSize));
+    __ Poke(x11, Operand(x10, LSL, kSystemPointerSizeLog2));
+    __ Add(x12, x12, 1);
+#endif
     __ Cbnz(x10, &loop);
     __ Bind(&done);
   }
@@ -846,11 +870,17 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
     __ SlotAddress(scratch, slots_to_claim);
     __ Str(padreg, MemOperand(scratch, -kSystemPointerSize));
 
+#ifdef V8_REVERSE_JSARGS
     // Store receiver on the stack.
     __ Poke(receiver, 0);
     // Store function on the stack.
     __ SlotAddress(scratch, argc);
     __ Str(function, MemOperand(scratch, kSystemPointerSize));
+#else
+    // Store receiver and function on the stack.
+    __ SlotAddress(scratch, argc);
+    __ Stp(receiver, function, MemOperand(scratch));
+#endif
 
     // Copy arguments to the stack in a loop, in reverse order.
     // x4: argc.
@@ -862,6 +892,7 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
 
     // scratch has been set to point to the location of the function, which
     // marks the end of the argument copy.
+#ifdef V8_REVERSE_JSARGS
     __ SlotAddress(x0, 1);  // Skips receiver.
     __ Bind(&loop);
     // Load the handle.
@@ -873,6 +904,18 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
     // Loop if we've not reached the end of copy marker.
     __ Cmp(x0, scratch);
     __ B(le, &loop);
+#else
+    __ Bind(&loop);
+    // Load the handle.
+    __ Ldr(x11, MemOperand(argv, kSystemPointerSize, PostIndex));
+    // Dereference the handle.
+    __ Ldr(x11, MemOperand(x11));
+    // Poke the result into the stack.
+    __ Str(x11, MemOperand(scratch, -kSystemPointerSize, PreIndex));
+    // Loop if we've not reached the end of copy marker.
+    __ Cmp(sp, scratch);
+    __ B(lt, &loop);
+#endif
 
     __ Bind(&done);
 
@@ -1443,6 +1486,7 @@ static void Generate_InterpreterPushArgs(MacroAssembler* masm,
     __ Poke(padreg, Operand(scratch, LSL, kSystemPointerSizeLog2));
   }
 
+#ifdef V8_REVERSE_JSARGS
   if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
     __ Mov(slots_to_copy, num_args);
     __ SlotAddress(stack_addr, 1);
@@ -1471,6 +1515,33 @@ static void Generate_InterpreterPushArgs(MacroAssembler* masm,
     __ LoadRoot(receiver, RootIndex::kUndefinedValue);
     __ Poke(receiver, 0);
   }
+#else   // !V8_REVERSE_JSARGS
+  if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
+    // Store "undefined" as the receiver arg if we need to.
+    Register receiver = x14;
+    __ LoadRoot(receiver, RootIndex::kUndefinedValue);
+    __ SlotAddress(stack_addr, num_args);
+    __ Str(receiver, MemOperand(stack_addr));
+    __ Mov(slots_to_copy, num_args);
+  } else {
+    // If we're not given an explicit receiver to store, we'll need to copy it
+    // together with the rest of the arguments.
+    __ Add(slots_to_copy, num_args, 1);
+  }
+
+  __ Sub(last_arg_addr, first_arg_index,
+         Operand(slots_to_copy, LSL, kSystemPointerSizeLog2));
+  __ Add(last_arg_addr, last_arg_addr, kSystemPointerSize);
+
+  // Load the final spread argument into spread_arg_out, if necessary.
+  if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
+    __ Ldr(spread_arg_out, MemOperand(last_arg_addr, -kSystemPointerSize));
+  }
+
+  // Copy the rest of the arguments.
+  __ SlotAddress(stack_addr, 0);
+  __ CopyDoubleWords(stack_addr, last_arg_addr, slots_to_copy);
+#endif  // !V8_REVERSE_JSARGS
 }
 
 // static
@@ -1695,6 +1766,7 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
   __ Add(fp, sp, frame_size);
 
   if (with_result) {
+#ifdef V8_REVERSE_JSARGS
     if (java_script_builtin) {
       __ mov(scratch, x0);
     } else {
@@ -1703,6 +1775,12 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
       __ Str(x0, MemOperand(
                      fp, BuiltinContinuationFrameConstants::kCallerSPOffset));
     }
+#else
+    // Overwrite the hole inserted by the deoptimizer with the return value from
+    // the LAZY deopt point.
+    __ Str(x0,
+           MemOperand(fp, BuiltinContinuationFrameConstants::kCallerSPOffset));
+#endif
   }
 
   // Restore registers in pairs.
@@ -1725,6 +1803,7 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
 
   if (java_script_builtin) __ SmiUntag(kJavaScriptCallArgCountRegister);
 
+#ifdef V8_REVERSE_JSARGS
   if (java_script_builtin && with_result) {
     // Overwrite the hole inserted by the deoptimizer with the return value from
     // the LAZY deopt point. r0 contains the arguments count, the return value
@@ -1738,6 +1817,7 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
            BuiltinContinuationFrameConstants::kCallerSPOffset /
                kSystemPointerSize);
   }
+#endif
 
   // Load builtin index (stored as a Smi) and use it to get the builtin start
   // address from the builtins table.
@@ -1846,6 +1926,7 @@ void Builtins::Generate_FunctionPrototypeApply(MacroAssembler* masm) {
   // 1. Load receiver into x1, argArray into x2 (if present), remove all
   // arguments from the stack (including the receiver), and push thisArg (if
   // present) instead.
+#ifdef V8_REVERSE_JSARGS
   {
     Label done;
     __ Mov(this_arg, undefined_value);
@@ -1858,6 +1939,32 @@ void Builtins::Generate_FunctionPrototypeApply(MacroAssembler* masm) {
     __ Peek(arg_array, 2 * kSystemPointerSize);
     __ bind(&done);
   }
+#else   // !V8_REVERSE_JSARGS
+  {
+    Register scratch = x11;
+
+    // Push two undefined values on the stack, to put it in a consistent state
+    // so that we can always read three arguments from it.
+    __ Push(undefined_value, undefined_value);
+
+    // The state of the stack (with arrows pointing to the slots we will read)
+    // is as follows:
+    //
+    //       argc = 0               argc = 1                argc = 2
+    // -> sp[16]: receiver    -> sp[24]: receiver     -> sp[32]: receiver
+    // -> sp[8]:  undefined   -> sp[16]: this_arg     -> sp[24]: this_arg
+    // -> sp[0]:  undefined   -> sp[8]:  undefined    -> sp[16]: arg_array
+    //                           sp[0]:  undefined       sp[8]:  undefined
+    //                                                   sp[0]:  undefined
+    //
+    // There are now always three arguments to read, in the slots starting from
+    // slot argc.
+    __ SlotAddress(scratch, argc);
+    __ Ldp(arg_array, this_arg, MemOperand(scratch));
+    __ Ldr(receiver, MemOperand(scratch, 2 * kSystemPointerSize));
+    __ Drop(2);  // Drop the undefined values we pushed above.
+  }
+#endif  // !V8_REVERSE_JSARGS
   __ DropArguments(argc, TurboAssembler::kCountExcludesReceiver);
   __ PushArgument(this_arg);
 
@@ -1916,6 +2023,7 @@ void Builtins::Generate_FunctionPrototypeCall(MacroAssembler* masm) {
   }
 
   Label arguments_ready;
+#ifdef V8_REVERSE_JSARGS
   // 3. Shift arguments. It depends if the arguments is even or odd.
   // That is if padding exists or not.
   {
@@ -1944,6 +2052,30 @@ void Builtins::Generate_FunctionPrototypeCall(MacroAssembler* masm) {
                        TurboAssembler::kSrcLessThanDst);
     __ Drop(2);
   }
+#else   // !V8_REVERSE_JSARGS
+  // 3. Overwrite the receiver with padding. If argc is odd, this is all we
+  //    need to do.
+  __ Poke(padreg, Operand(argc, LSL, kXRegSizeLog2));
+  __ Tbnz(argc, 0, &arguments_ready);
+
+  // 4. If argc is even:
+  //    Copy arguments two slots higher in memory, overwriting the original
+  //    receiver and padding.
+  {
+    Register copy_from = x10;
+    Register copy_to = x11;
+    Register count = x12;
+    Register last_arg_slot = x13;
+    __ Mov(count, argc);
+    __ Sub(last_arg_slot, argc, 1);
+    __ SlotAddress(copy_from, last_arg_slot);
+    __ Add(copy_to, copy_from, 2 * kSystemPointerSize);
+    __ CopyDoubleWords(copy_to, copy_from, count,
+                       TurboAssembler::kSrcLessThanDst);
+    // Drop two slots. These are copies of the last two arguments.
+    __ Drop(2);
+  }
+#endif  // !V8_REVERSE_JSARGS
 
   // 5. Adjust argument count to make the original first argument the new
   //    receiver and call the callable.
@@ -1974,6 +2106,7 @@ void Builtins::Generate_ReflectApply(MacroAssembler* masm) {
   // 1. Load target into x1 (if present), argumentsList into x2 (if present),
   // remove all arguments from the stack (including the receiver), and push
   // thisArgument (if present) instead.
+#ifdef V8_REVERSE_JSARGS
   {
     Label done;
     __ Mov(target, undefined_value);
@@ -1989,6 +2122,45 @@ void Builtins::Generate_ReflectApply(MacroAssembler* masm) {
     __ Peek(arguments_list, 3 * kSystemPointerSize);
     __ bind(&done);
   }
+#else   // !V8_REVERSE_JSARGS
+  {
+    // Push four undefined values on the stack, to put it in a consistent state
+    // so that we can always read the three arguments we need from it. The
+    // fourth value is used for stack alignment.
+    __ Push(undefined_value, undefined_value, undefined_value, undefined_value);
+
+    // The state of the stack (with arrows pointing to the slots we will read)
+    // is as follows:
+    //
+    //       argc = 0               argc = 1                argc = 2
+    //    sp[32]: receiver       sp[40]: receiver        sp[48]: receiver
+    // -> sp[24]: undefined   -> sp[32]: target       -> sp[40]: target
+    // -> sp[16]: undefined   -> sp[24]: undefined    -> sp[32]: this_argument
+    // -> sp[8]:  undefined   -> sp[16]: undefined    -> sp[24]: undefined
+    //    sp[0]:  undefined      sp[8]:  undefined       sp[16]: undefined
+    //                           sp[0]:  undefined       sp[8]:  undefined
+    //                                                   sp[0]:  undefined
+    //       argc = 3
+    //    sp[56]: receiver
+    // -> sp[48]: target
+    // -> sp[40]: this_argument
+    // -> sp[32]: arguments_list
+    //    sp[24]: undefined
+    //    sp[16]: undefined
+    //    sp[8]:  undefined
+    //    sp[0]:  undefined
+    //
+    // There are now always three arguments to read, in the slots starting from
+    // slot (argc + 1).
+    Register scratch = x10;
+    __ SlotAddress(scratch, argc);
+    __ Ldp(arguments_list, this_argument,
+           MemOperand(scratch, 1 * kSystemPointerSize));
+    __ Ldr(target, MemOperand(scratch, 3 * kSystemPointerSize));
+
+    __ Drop(4);  // Drop the undefined values we pushed above.
+  }
+#endif  // !V8_REVERSE_JSARGS
   __ DropArguments(argc, TurboAssembler::kCountExcludesReceiver);
   __ PushArgument(this_argument);
 
@@ -2030,6 +2202,7 @@ void Builtins::Generate_ReflectConstruct(MacroAssembler* masm) {
   // new.target into x3 (if present, otherwise use target), remove all
   // arguments from the stack (including the receiver), and push thisArgument
   // (if present) instead.
+#ifdef V8_REVERSE_JSARGS
   {
     Label done;
     __ Mov(target, undefined_value);
@@ -2046,6 +2219,48 @@ void Builtins::Generate_ReflectConstruct(MacroAssembler* masm) {
     __ Peek(new_target, 3 * kSystemPointerSize);
     __ bind(&done);
   }
+#else   // !V8_REVERSE_JSARGS
+  {
+    // Push four undefined values on the stack, to put it in a consistent state
+    // so that we can always read the three arguments we need from it. The
+    // fourth value is used for stack alignment.
+    __ Push(undefined_value, undefined_value, undefined_value, undefined_value);
+
+    // The state of the stack (with arrows pointing to the slots we will read)
+    // is as follows:
+    //
+    //       argc = 0               argc = 1                argc = 2
+    //    sp[32]: receiver       sp[40]: receiver        sp[48]: receiver
+    // -> sp[24]: undefined   -> sp[32]: target       -> sp[40]: target
+    // -> sp[16]: undefined   -> sp[24]: undefined    -> sp[32]: arguments_list
+    // -> sp[8]:  undefined   -> sp[16]: undefined    -> sp[24]: undefined
+    //    sp[0]:  undefined      sp[8]:  undefined       sp[16]: undefined
+    //                           sp[0]:  undefined       sp[8]:  undefined
+    //                                                   sp[0]:  undefined
+    //       argc = 3
+    //    sp[56]: receiver
+    // -> sp[48]: target
+    // -> sp[40]: arguments_list
+    // -> sp[32]: new_target
+    //    sp[24]: undefined
+    //    sp[16]: undefined
+    //    sp[8]:  undefined
+    //    sp[0]:  undefined
+    //
+    // There are now always three arguments to read, in the slots starting from
+    // slot (argc + 1).
+    Register scratch = x10;
+    __ SlotAddress(scratch, argc);
+    __ Ldp(new_target, arguments_list,
+           MemOperand(scratch, 1 * kSystemPointerSize));
+    __ Ldr(target, MemOperand(scratch, 3 * kSystemPointerSize));
+
+    __ Cmp(argc, 2);
+    __ CmovX(new_target, target, ls);  // target if argc <= 2.
+
+    __ Drop(4);  // Drop the undefined values we pushed above.
+  }
+#endif  // !V8_REVERSE_JSARGS
 
   __ DropArguments(argc, TurboAssembler::kCountExcludesReceiver);
 
@@ -2103,7 +2318,9 @@ void LeaveArgumentsAdaptorFrame(MacroAssembler* masm) {
 // one slot up or one slot down, as needed.
 void Generate_PrepareForCopyingVarargs(MacroAssembler* masm, Register argc,
                                        Register len) {
-  Label exit, even;
+  Label exit;
+#ifdef V8_REVERSE_JSARGS
+  Label even;
   Register slots_to_copy = x10;
   Register slots_to_claim = x12;
 
@@ -2135,6 +2352,59 @@ void Generate_PrepareForCopyingVarargs(MacroAssembler* masm, Register argc,
     __ SlotAddress(dst, 0);
     __ CopyDoubleWords(dst, src, slots_to_copy);
   }
+#else   // !V8_REVERSE_JSARGS
+  Label len_odd;
+  Register slots_to_copy = x10;  // If needed.
+  __ Add(slots_to_copy, argc, 1);
+  __ Add(argc, argc, len);
+  __ Tbnz(len, 0, &len_odd);
+  __ Claim(len);
+  __ B(&exit);
+
+  __ Bind(&len_odd);
+  // Claim space we need. If argc is even, slots_to_claim = len + 1, as we need
+  // one extra padding slot. If argc is odd, we know that the original arguments
+  // will have a padding slot we can reuse (since len is odd), so
+  // slots_to_claim = len - 1.
+  {
+    Register scratch = x11;
+    Register slots_to_claim = x12;
+    __ Add(slots_to_claim, len, 1);
+    __ And(scratch, argc, 1);
+    __ Sub(slots_to_claim, slots_to_claim, Operand(scratch, LSL, 1));
+    __ Claim(slots_to_claim);
+  }
+
+  Label copy_down;
+  __ Tbz(slots_to_copy, 0, &copy_down);
+
+  // Copy existing arguments one slot up.
+  {
+    Register src = x11;
+    Register dst = x12;
+    Register scratch = x13;
+    __ Sub(scratch, argc, 1);
+    __ SlotAddress(src, scratch);
+    __ SlotAddress(dst, argc);
+    __ CopyDoubleWords(dst, src, slots_to_copy,
+                       TurboAssembler::kSrcLessThanDst);
+  }
+  __ B(&exit);
+
+  // Copy existing arguments one slot down and add padding.
+  __ Bind(&copy_down);
+  {
+    Register src = x11;
+    Register dst = x12;
+    Register scratch = x13;
+    __ Add(src, len, 1);
+    __ Mov(dst, len);  // CopySlots will corrupt dst.
+    __ CopySlots(dst, src, slots_to_copy);
+    __ Add(scratch, argc, 1);
+    __ Poke(padreg,
+            Operand(scratch, LSL, kSystemPointerSizeLog2));  // Store padding.
+  }
+#endif  // !V8_REVERSE_JSARGS
   __ Bind(&exit);
 }
 
@@ -2195,6 +2465,7 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
     // We do not use the CompareRoot macro as it would do a LoadRoot behind the
     // scenes and we want to avoid that in a loop.
     // TODO(all): Consider using Ldp and Stp.
+#ifdef V8_REVERSE_JSARGS
     Register dst = x16;
     __ Add(dst, argc, Immediate(1));  // Consider the receiver as well.
     __ SlotAddress(dst, dst);
@@ -2206,6 +2477,15 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
     __ Csel(scratch, scratch, undefined_value, ne);
     __ Str(scratch, MemOperand(dst, kSystemPointerSize, PostIndex));
     __ Cbnz(len, &loop);
+#else
+    __ Bind(&loop);
+    __ Sub(len, len, 1);
+    __ LoadAnyTaggedField(scratch, MemOperand(src, kTaggedSize, PostIndex));
+    __ CmpTagged(scratch, the_hole_value);
+    __ Csel(scratch, scratch, undefined_value, ne);
+    __ Poke(scratch, Operand(len, LSL, kSystemPointerSizeLog2));
+    __ Cbnz(len, &loop);
+#endif
   }
   __ Bind(&done);
   // Tail-call to the actual Call or Construct builtin.
@@ -2300,6 +2580,7 @@ void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
   // Push varargs.
   {
     Register dst = x13;
+#ifdef V8_REVERSE_JSARGS
     // Point to the fist argument to copy from (skipping receiver).
     __ Add(args_fp, args_fp,
            CommonFrameConstants::kFixedFrameSizeAboveFp + kSystemPointerSize);
@@ -2310,6 +2591,10 @@ void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
     __ SlotAddress(dst, x10);
     // Update total number of arguments.
     __ Add(argc, argc, len);
+#else
+    __ Add(args_fp, args_fp, CommonFrameConstants::kFixedFrameSizeAboveFp);
+    __ SlotAddress(dst, 0);
+#endif
     __ CopyDoubleWords(dst, args_fp, len);
   }
   __ B(&stack_done);
@@ -2470,6 +2755,7 @@ void Generate_PushBoundArguments(MacroAssembler* masm) {
       __ Bind(&done);
     }
 
+#ifdef V8_REVERSE_JSARGS
     Label copy_bound_args;
     Register total_argc = x15;
     Register slots_to_claim = x12;
@@ -2545,6 +2831,80 @@ void Generate_PushBoundArguments(MacroAssembler* masm) {
     }
     // Update argc.
     __ Mov(argc, total_argc);
+#else   // !V8_REVERSE_JSARGS
+    // Check if we need padding.
+    Label copy_args, copy_bound_args;
+    Register total_argc = x15;
+    Register slots_to_claim = x12;
+    __ Add(total_argc, argc, bound_argc);
+    __ Mov(slots_to_claim, bound_argc);
+    __ Tbz(bound_argc, 0, &copy_args);
+
+    // Load receiver before we start moving the arguments. We will only
+    // need this in this path because the bound arguments are odd.
+    Register receiver = x14;
+    __ Peek(receiver, Operand(argc, LSL, kSystemPointerSizeLog2));
+
+    // Claim space we need. If argc is even, slots_to_claim = bound_argc + 1,
+    // as we need one extra padding slot. If argc is odd, we know that the
+    // original arguments will have a padding slot we can reuse (since
+    // bound_argc is odd), so slots_to_claim = bound_argc - 1.
+    {
+      Register scratch = x11;
+      __ Add(slots_to_claim, bound_argc, 1);
+      __ And(scratch, total_argc, 1);
+      __ Sub(slots_to_claim, slots_to_claim, Operand(scratch, LSL, 1));
+    }
+
+    // Copy bound arguments.
+    __ Bind(&copy_args);
+    // Skip claim and copy of existing arguments in the special case where we
+    // do not need to claim any slots (this will be the case when
+    // bound_argc == 1 and the existing arguments have padding we can reuse).
+    __ Cbz(slots_to_claim, &copy_bound_args);
+    __ Claim(slots_to_claim);
+    {
+      Register count = x10;
+      // Relocate arguments to a lower address.
+      __ Mov(count, argc);
+      __ CopySlots(0, slots_to_claim, count);
+
+      __ Bind(&copy_bound_args);
+      // Copy [[BoundArguments]] to the stack (below the arguments). The first
+      // element of the array is copied to the highest address.
+      {
+        Label loop;
+        Register counter = x10;
+        Register scratch = x11;
+        Register copy_to = x12;
+        __ Add(bound_argv, bound_argv,
+               FixedArray::kHeaderSize - kHeapObjectTag);
+        __ SlotAddress(copy_to, argc);
+        __ Add(argc, argc,
+               bound_argc);  // Update argc to include bound arguments.
+        __ Lsl(counter, bound_argc, kTaggedSizeLog2);
+        __ Bind(&loop);
+        __ Sub(counter, counter, kTaggedSize);
+        __ LoadAnyTaggedField(scratch, MemOperand(bound_argv, counter));
+        // Poke into claimed area of stack.
+        __ Str(scratch, MemOperand(copy_to, kSystemPointerSize, PostIndex));
+        __ Cbnz(counter, &loop);
+      }
+
+      {
+        Label done;
+        Register scratch = x10;
+        __ Tbz(bound_argc, 0, &done);
+        // Store receiver.
+        __ Add(scratch, sp, Operand(total_argc, LSL, kSystemPointerSizeLog2));
+        __ Str(receiver, MemOperand(scratch, kSystemPointerSize, PostIndex));
+        __ Tbnz(total_argc, 0, &done);
+        // Store padding.
+        __ Str(padreg, MemOperand(scratch));
+        __ Bind(&done);
+      }
+    }
+#endif  // !V8_REVERSE_JSARGS
   }
   __ Bind(&no_bound_arguments);
 }
@@ -2805,6 +3165,26 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
   __ Cmp(argc_expected, kDontAdaptArgumentsSentinel);
   __ B(eq, &dont_adapt_arguments);
 
+#ifndef V8_REVERSE_JSARGS
+  // This optimization is disabled when the arguments are reversed.
+  Label adapt_arguments_in_place;
+  Register argc_actual_minus_expected = x5;
+
+  // When the difference between argc_actual and argc_expected is odd, we
+  // create an arguments adaptor frame.
+  __ Sub(argc_actual_minus_expected, argc_actual, argc_expected);
+  __ Tbnz(argc_actual_minus_expected, 0, &create_adaptor_frame);
+
+  // When the difference is even, check if we are allowed to adjust the
+  // existing frame instead.
+  __ LoadTaggedPointerField(
+      x4, FieldMemOperand(x1, JSFunction::kSharedFunctionInfoOffset));
+  __ Ldr(w4, FieldMemOperand(x4, SharedFunctionInfo::kFlagsOffset));
+  __ TestAndBranchIfAnySet(
+      w4, SharedFunctionInfo::IsSafeToSkipArgumentsAdaptorBit::kMask,
+      &adapt_arguments_in_place);
+#endif
+
   // -------------------------------------------
   // Create an arguments adaptor frame.
   // -------------------------------------------
@@ -2831,6 +3211,7 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
     __ Bic(scratch1, scratch1, 1);
     __ Claim(scratch1, kSystemPointerSize);
 
+#ifdef V8_REVERSE_JSARGS
     // If we don't have enough arguments, fill the remaining expected
     // arguments with undefined, otherwise skip this step.
     Label enough_arguments;
@@ -2875,6 +3256,84 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
     __ Add(copy_from, fp, 2 * kSystemPointerSize);
     __ CopyDoubleWords(copy_to, copy_from, argc_to_copy);
 
+#else  // !V8_REVERSE_JSARGS
+    Register argc_unused_actual = x14;
+    Register scratch2 = x16;
+
+    // Preparing the expected arguments is done in four steps, the order of
+    // which is chosen so we can use LDP/STP and avoid conditional branches as
+    // much as possible.
+
+    __ Mov(copy_to, sp);
+
+    // (1) If we don't have enough arguments, fill the remaining expected
+    // arguments with undefined, otherwise skip this step.
+    Label enough_arguments;
+    __ Subs(scratch1, argc_actual, argc_expected);
+    __ Csel(argc_unused_actual, xzr, scratch1, lt);
+    __ Csel(argc_to_copy, argc_expected, argc_actual, ge);
+    __ B(ge, &enough_arguments);
+
+    // Fill the remaining expected arguments with undefined.
+    __ RecordComment("-- Fill slots with undefined --");
+    __ Sub(copy_end, copy_to, Operand(scratch1, LSL, kSystemPointerSizeLog2));
+    __ LoadRoot(scratch1, RootIndex::kUndefinedValue);
+
+    Label fill;
+    __ Bind(&fill);
+    __ Stp(scratch1, scratch1,
+           MemOperand(copy_to, 2 * kSystemPointerSize, PostIndex));
+    // We might write one slot extra, but that is ok because we'll overwrite it
+    // below.
+    __ Cmp(copy_end, copy_to);
+    __ B(hi, &fill);
+
+    // Correct copy_to, for the case where we wrote one additional slot.
+    __ Mov(copy_to, copy_end);
+
+    __ Bind(&enough_arguments);
+    // (2) Copy all of the actual arguments, or as many as we need.
+    Label skip_copy;
+    __ RecordComment("-- Copy actual arguments --");
+    __ Cbz(argc_to_copy, &skip_copy);
+    __ Add(copy_end, copy_to,
+           Operand(argc_to_copy, LSL, kSystemPointerSizeLog2));
+    __ Add(copy_from, fp, 2 * kSystemPointerSize);
+    // Adjust for difference between actual and expected arguments.
+    __ Add(copy_from, copy_from,
+           Operand(argc_unused_actual, LSL, kSystemPointerSizeLog2));
+
+    // Copy arguments. We use load/store pair instructions, so we might
+    // overshoot by one slot, but since we copy the arguments starting from the
+    // last one, if we do overshoot, the extra slot will be overwritten later by
+    // the receiver.
+    Label copy_2_by_2;
+    __ Bind(&copy_2_by_2);
+    __ Ldp(scratch1, scratch2,
+           MemOperand(copy_from, 2 * kSystemPointerSize, PostIndex));
+    __ Stp(scratch1, scratch2,
+           MemOperand(copy_to, 2 * kSystemPointerSize, PostIndex));
+    __ Cmp(copy_end, copy_to);
+    __ B(hi, &copy_2_by_2);
+    __ Bind(&skip_copy);
+
+    // (3) Store padding, which might be overwritten by the receiver, if it is
+    // not necessary.
+    __ RecordComment("-- Store padding --");
+    __ Str(padreg, MemOperand(fp, -5 * kSystemPointerSize));
+
+    // (4) Store receiver. Calculate target address from the sp to avoid
+    // checking for padding. Storing the receiver will overwrite either the
+    // extra slot we copied with the actual arguments, if we did copy one, or
+    // the padding we stored above.
+    __ RecordComment("-- Store receiver --");
+    __ Add(copy_from, fp, 2 * kSystemPointerSize);
+    __ Ldr(scratch1,
+           MemOperand(copy_from, argc_actual, LSL, kSystemPointerSizeLog2));
+    __ Str(scratch1,
+           MemOperand(sp, argc_expected, LSL, kSystemPointerSizeLog2));
+#endif
+
     // Arguments have been adapted. Now call the entry point.
     __ RecordComment("-- Call entry point --");
     __ Mov(argc_actual, argc_expected);
@@ -2895,6 +3354,46 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
     __ Ret();
   }
 
+  #ifndef V8_REVERSE_JSARGS
+  // -----------------------------------------
+  // Adapt arguments in the existing frame.
+  // -----------------------------------------
+  __ Bind(&adapt_arguments_in_place);
+  {
+    __ RecordComment("-- Update arguments in place --");
+    // The callee cannot observe the actual arguments, so it's safe to just
+    // pass the expected arguments by massaging the stack appropriately. See
+    // http://bit.ly/v8-faster-calls-with-arguments-mismatch for details.
+    Label under_application, over_application;
+    __ Tbnz(argc_actual_minus_expected, kXSignBit, &under_application);
+
+    __ Bind(&over_application);
+    {
+      // Remove superfluous arguments from the stack. The number of superflous
+      // arguments is even.
+      __ RecordComment("-- Over-application --");
+      __ Mov(argc_actual, argc_expected);
+      __ Drop(argc_actual_minus_expected);
+      __ B(&dont_adapt_arguments);
+    }
+
+    __ Bind(&under_application);
+    {
+      // Fill remaining expected arguments with undefined values.
+      __ RecordComment("-- Under-application --");
+      Label fill;
+      Register undef_value = x16;
+      __ LoadRoot(undef_value, RootIndex::kUndefinedValue);
+      __ Bind(&fill);
+      __ Add(argc_actual, argc_actual, 2);
+      __ Push(undef_value, undef_value);
+      __ Cmp(argc_actual, argc_expected);
+      __ B(lt, &fill);
+      __ B(&dont_adapt_arguments);
+    }
+  }
+#endif
+
   // -------------------------------------------
   // Dont adapt arguments.
   // -------------------------------------------
@@ -3495,8 +3994,14 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
 
   // FunctionCallbackInfo::values_ (points at the first varargs argument passed
   // on the stack).
+#ifdef V8_REVERSE_JSARGS
   __ Add(scratch, scratch,
          Operand((FCA::kArgsLength + 1) * kSystemPointerSize));
+#else
+  __ Add(scratch, scratch,
+         Operand((FCA::kArgsLength - 1) * kSystemPointerSize));
+  __ Add(scratch, scratch, Operand(argc, LSL, kSystemPointerSizeLog2));
+#endif
   __ Str(scratch, MemOperand(sp, 2 * kSystemPointerSize));
 
   // FunctionCallbackInfo::length_.
diff --git a/src/builtins/builtins-api.cc b/src/builtins/builtins-api.cc
index e42760d4d2..d9a1adfd30 100644
--- a/src/builtins/builtins-api.cc
+++ b/src/builtins/builtins-api.cc
@@ -206,6 +206,7 @@ MaybeHandle<Object> Builtins::InvokeApiFunction(Isolate* isolate,
   } else {
     argv = new Address[frame_argc];
   }
+#ifdef V8_REVERSE_JSARGS
   argv[BuiltinArguments::kNewTargetOffset] = new_target->ptr();
   argv[BuiltinArguments::kTargetOffset] = function->ptr();
   argv[BuiltinArguments::kArgcOffset] = Smi::FromInt(frame_argc).ptr();
@@ -216,6 +217,19 @@ MaybeHandle<Object> Builtins::InvokeApiFunction(Isolate* isolate,
   for (int i = 0; i < argc; ++i) {
     argv[cursor++] = args[i]->ptr();
   }
+#else
+  int cursor = frame_argc - 1;
+  argv[cursor--] = receiver->ptr();
+  for (int i = 0; i < argc; ++i) {
+    argv[cursor--] = args[i]->ptr();
+  }
+  DCHECK_EQ(cursor, BuiltinArguments::kPaddingOffset);
+  argv[BuiltinArguments::kPaddingOffset] =
+      ReadOnlyRoots(isolate).the_hole_value().ptr();
+  argv[BuiltinArguments::kArgcOffset] = Smi::FromInt(frame_argc).ptr();
+  argv[BuiltinArguments::kTargetOffset] = function->ptr();
+  argv[BuiltinArguments::kNewTargetOffset] = new_target->ptr();
+#endif
   MaybeHandle<Object> result;
   {
     RelocatableArguments arguments(isolate, frame_argc, &argv[frame_argc - 1]);
diff --git a/src/builtins/builtins-utils-inl.h b/src/builtins/builtins-utils-inl.h
index 10f03a3d91..82d5fe2873 100644
--- a/src/builtins/builtins-utils-inl.h
+++ b/src/builtins/builtins-utils-inl.h
@@ -23,12 +23,20 @@ Handle<Object> BuiltinArguments::atOrUndefined(Isolate* isolate,
 Handle<Object> BuiltinArguments::receiver() const { return at<Object>(0); }
 
 Handle<JSFunction> BuiltinArguments::target() const {
+#ifdef V8_REVERSE_JSARGS
   int index = kTargetOffset;
+#else
+  int index = Arguments::length() - 1 - kTargetOffset;
+#endif
   return Handle<JSFunction>(address_of_arg_at(index));
 }
 
 Handle<HeapObject> BuiltinArguments::new_target() const {
+#ifdef V8_REVERSE_JSARGS
   int index = kNewTargetOffset;
+#else
+  int index = Arguments::length() - 1 - kNewTargetOffset;
+#endif
   return Handle<JSFunction>(address_of_arg_at(index));
 }
 
diff --git a/src/builtins/builtins-utils.h b/src/builtins/builtins-utils.h
index e5f420a20d..3bed3bc651 100644
--- a/src/builtins/builtins-utils.h
+++ b/src/builtins/builtins-utils.h
@@ -52,7 +52,12 @@ class BuiltinArguments : public JavaScriptArguments {
 
   static constexpr int kNumExtraArgs = 4;
   static constexpr int kNumExtraArgsWithReceiver = 5;
+
+#ifdef V8_REVERSE_JSARGS
   static constexpr int kArgsOffset = 4;
+#else
+  static constexpr int kArgsOffset = 0;
+#endif
 
   inline Handle<Object> atOrUndefined(Isolate* isolate, int index) const;
   inline Handle<Object> receiver() const;
diff --git a/src/builtins/ia32/builtins-ia32.cc b/src/builtins/ia32/builtins-ia32.cc
index d5f82cd3d9..8dce43516e 100644
--- a/src/builtins/ia32/builtins-ia32.cc
+++ b/src/builtins/ia32/builtins-ia32.cc
@@ -94,11 +94,7 @@ void Generate_JSBuiltinsConstructStubHelper(MacroAssembler* masm) {
     __ push(eax);
     __ SmiUntag(eax);
 
-    // TODO(victorgomes): When the arguments adaptor is completely removed, we
-    // should get the formal parameter count and copy the arguments in its
-    // correct position (including any undefined), instead of delaying this to
-    // InvokeFunction.
-
+#ifdef V8_REVERSE_JSARGS
     // Set up pointer to first argument (skip receiver).
     __ lea(esi, Operand(ebp, StandardFrameConstants::kCallerSPOffset +
                                  kSystemPointerSize));
@@ -106,6 +102,14 @@ void Generate_JSBuiltinsConstructStubHelper(MacroAssembler* masm) {
     __ PushArray(esi, eax, ecx);
     // The receiver for the builtin/api call.
     __ PushRoot(RootIndex::kTheHoleValue);
+#else
+    // The receiver for the builtin/api call.
+    __ PushRoot(RootIndex::kTheHoleValue);
+    // Set up pointer to last argument. We are using esi as scratch register.
+    __ lea(esi, Operand(ebp, StandardFrameConstants::kCallerSPOffset));
+    // Copy arguments to the expression stack.
+    __ PushArray(esi, eax, ecx);
+#endif
 
     // Call the function.
     // eax: number of arguments (untagged)
@@ -208,16 +212,26 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
   // Push the allocated receiver to the stack.
   __ Push(eax);
 
-  // We need two copies because we may have to return the original one
-  // and the calling conventions dictate that the called function pops the
-  // receiver. The second copy is pushed after the arguments, we saved in r8
-  // since rax needs to store the number of arguments before
-  // InvokingFunction.
-  __ movd(xmm0, eax);
+#ifdef V8_REVERSE_JSARGS
+    // We need two copies because we may have to return the original one
+    // and the calling conventions dictate that the called function pops the
+    // receiver. The second copy is pushed after the arguments, we saved in r8
+    // since rax needs to store the number of arguments before
+    // InvokingFunction.
+    __ movd(xmm0, eax);
 
-  // Set up pointer to first argument (skip receiver).
-  __ lea(edi, Operand(ebp, StandardFrameConstants::kCallerSPOffset +
-                               kSystemPointerSize));
+    // Set up pointer to first argument (skip receiver).
+    __ lea(edi, Operand(ebp, StandardFrameConstants::kCallerSPOffset +
+                                 kSystemPointerSize));
+#else
+    // We need two copies because we may have to return the original one
+    // and the calling conventions dictate that the called function pops the
+    // receiver.
+    __ Push(eax);
+
+    // Set up pointer to last argument.
+    __ lea(edi, Operand(ebp, StandardFrameConstants::kCallerSPOffset));
+#endif
 
   // Restore argument count.
   __ mov(eax, Operand(ebp, ConstructFrameConstants::kLengthOffset));
@@ -236,9 +250,11 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
   // Copy arguments to the expression stack.
   __ PushArray(edi, eax, ecx);
 
-  // Push implicit receiver.
-  __ movd(ecx, xmm0);
-  __ Push(ecx);
+#ifdef V8_REVERSE_JSARGS
+    // Push implicit receiver.
+    __ movd(ecx, xmm0);
+    __ Push(ecx);
+#endif
 
   // Restore and and call the constructor function.
   __ mov(edi, Operand(ebp, ConstructFrameConstants::kConstructorOffset));
@@ -475,6 +491,11 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
     // Push the function.
     __ push(Operand(scratch1, EntryFrameConstants::kFunctionArgOffset));
 
+#ifndef V8_REVERSE_JSARGS
+    // And the receiver onto the stack.
+    __ push(Operand(scratch1, EntryFrameConstants::kReceiverArgOffset));
+#endif
+
     // Load the number of arguments and setup pointer to the arguments.
     __ mov(eax, Operand(scratch1, EntryFrameConstants::kArgcOffset));
     __ mov(scratch1, Operand(scratch1, EntryFrameConstants::kArgvOffset));
@@ -493,6 +514,7 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
     __ bind(&enough_stack_space);
 
     // Copy arguments to the stack in a loop.
+#ifdef V8_REVERSE_JSARGS
     Label loop, entry;
     __ Move(ecx, eax);
     __ jmp(&entry, Label::kNear);
@@ -503,12 +525,27 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
     __ bind(&entry);
     __ dec(ecx);
     __ j(greater_equal, &loop);
+#else
+    Label loop, entry;
+    __ Move(ecx, Immediate(0));
+    __ jmp(&entry, Label::kNear);
+    __ bind(&loop);
+    // Push the parameter from argv.
+    __ mov(scratch2, Operand(scratch1, ecx, times_system_pointer_size, 0));
+    __ push(Operand(scratch2, 0));  // dereference handle
+    __ inc(ecx);
+    __ bind(&entry);
+    __ cmp(ecx, eax);
+    __ j(not_equal, &loop);
+#endif
 
     // Load the previous frame pointer to access C arguments
     __ mov(scratch2, Operand(ebp, 0));
 
+#ifdef V8_REVERSE_JSARGS
     // Push the receiver onto the stack.
     __ push(Operand(scratch2, EntryFrameConstants::kReceiverArgOffset));
+#endif
 
     // Get the new.target and function from the frame.
     __ mov(edx, Operand(scratch2, EntryFrameConstants::kNewTargetArgOffset));
@@ -599,6 +636,11 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
   // Pop return address.
   __ PopReturnAddressTo(eax);
 
+#ifndef V8_REVERSE_JSARGS
+  // Push receiver.
+  __ Push(FieldOperand(edx, JSGeneratorObject::kReceiverOffset));
+#endif
+
   // ----------- S t a t e -------------
   //  -- eax    : return address
   //  -- edx    : the JSGeneratorObject to resume
@@ -615,6 +657,7 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
                         ecx, SharedFunctionInfo::kFormalParameterCountOffset));
     __ mov(ebx,
            FieldOperand(edx, JSGeneratorObject::kParametersAndRegistersOffset));
+#ifdef V8_REVERSE_JSARGS
     {
       Label done_loop, loop;
       __ mov(edi, ecx);
@@ -631,6 +674,22 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
 
     // Push receiver.
     __ Push(FieldOperand(edx, JSGeneratorObject::kReceiverOffset));
+#else
+    {
+      Label done_loop, loop;
+      __ Set(edi, 0);
+
+      __ bind(&loop);
+      __ cmp(edi, ecx);
+      __ j(greater_equal, &done_loop);
+      __ Push(
+          FieldOperand(ebx, edi, times_tagged_size, FixedArray::kHeaderSize));
+      __ add(edi, Immediate(1));
+      __ jmp(&loop);
+
+      __ bind(&done_loop);
+    }
+#endif
 
     // Restore registers.
     __ mov(edi, FieldOperand(edx, JSGeneratorObject::kFunctionOffset));
@@ -1188,11 +1247,19 @@ static void Generate_InterpreterPushArgs(MacroAssembler* masm,
   Label loop_header, loop_check;
   __ jmp(&loop_check);
   __ bind(&loop_header);
+#ifdef V8_REVERSE_JSARGS
   __ Push(Operand(array_limit, 0));
   __ bind(&loop_check);
   __ add(array_limit, Immediate(kSystemPointerSize));
   __ cmp(array_limit, start_address);
   __ j(below_equal, &loop_header, Label::kNear);
+#else
+  __ Push(Operand(start_address, 0));
+  __ sub(start_address, Immediate(kSystemPointerSize));
+  __ bind(&loop_check);
+  __ cmp(start_address, array_limit);
+  __ j(above, &loop_header, Label::kNear);
+#endif
 }
 
 // static
@@ -1212,10 +1279,12 @@ void Builtins::Generate_InterpreterPushArgsThenCallImpl(
   const Register argv = ecx;
 
   Label stack_overflow;
+#ifdef V8_REVERSE_JSARGS
   if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
     // The spread argument should not be pushed.
     __ dec(eax);
   }
+#endif
 
   // Add a stack check before pushing the arguments.
   __ StackOverflowCheck(eax, scratch, &stack_overflow, true);
@@ -1228,6 +1297,7 @@ void Builtins::Generate_InterpreterPushArgsThenCallImpl(
   // Pop return address to allow tail-call after pushing arguments.
   __ PopReturnAddressTo(eax);
 
+#ifdef V8_REVERSE_JSARGS
   if (receiver_mode != ConvertReceiverMode::kNullOrUndefined) {
     __ add(scratch, Immediate(1));  // Add one for receiver.
   }
@@ -1251,12 +1321,34 @@ void Builtins::Generate_InterpreterPushArgsThenCallImpl(
   if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
     __ PushRoot(RootIndex::kUndefinedValue);
   }
+#else
+  __ add(scratch, Immediate(1));  // Add one for receiver.
+
+  // Push "undefined" as the receiver arg if we need to.
+  if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
+    __ PushRoot(RootIndex::kUndefinedValue);
+    __ sub(scratch, Immediate(1));  // Subtract one for receiver.
+  }
+
+  // Find the address of the last argument.
+  __ shl(scratch, kSystemPointerSizeLog2);
+  __ neg(scratch);
+  __ add(scratch, argv);
+  Generate_InterpreterPushArgs(masm, scratch, argv);
+
+  if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
+    __ Pop(ecx);                // Pass the spread in a register
+  }
+#endif
 
   __ PushReturnAddressFrom(eax);
   __ movd(eax, xmm0);  // Restore number of arguments.
 
   // Call the target.
   if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
+#ifndef V8_REVERSE_JSARGS
+    __ sub(eax, Immediate(1));  // Subtract one for spread
+#endif
     __ Jump(BUILTIN_CODE(masm->isolate(), CallWithSpread),
             RelocInfo::CODE_TARGET);
   } else {
@@ -1317,6 +1409,7 @@ void Generate_InterpreterPushZeroAndArgsAndReturnAddress(
   // Step 3 copy arguments to correct locations.
   // Slot meant for receiver contains return address. Reset it so that
   // we will not incorrectly interpret return address as an object.
+#ifdef V8_REVERSE_JSARGS
   __ mov(Operand(esp, (num_slots_to_move + 1) * kSystemPointerSize),
          Immediate(0));
   __ mov(scratch1, Immediate(0));
@@ -1333,6 +1426,25 @@ void Generate_InterpreterPushZeroAndArgsAndReturnAddress(
   __ inc(scratch1);
   __ cmp(scratch1, eax);
   __ j(less_equal, &loop_header, Label::kNear);
+#else
+  __ mov(Operand(esp, num_args, times_system_pointer_size,
+                 (num_slots_to_move + 1) * kSystemPointerSize),
+         Immediate(0));
+  __ mov(scratch1, num_args);
+
+  Label loop_header, loop_check;
+  __ jmp(&loop_check);
+  __ bind(&loop_header);
+  __ mov(scratch2, Operand(start_addr, 0));
+  __ mov(Operand(esp, scratch1, times_system_pointer_size,
+                 num_slots_to_move * kSystemPointerSize),
+         scratch2);
+  __ sub(start_addr, Immediate(kSystemPointerSize));
+  __ sub(scratch1, Immediate(1));
+  __ bind(&loop_check);
+  __ cmp(scratch1, Immediate(0));
+  __ j(greater, &loop_header, Label::kNear);
+#endif
 }
 
 }  // anonymous namespace
@@ -1352,10 +1464,12 @@ void Builtins::Generate_InterpreterPushArgsThenConstructImpl(
   // -----------------------------------
   Label stack_overflow;
 
+#ifdef V8_REVERSE_JSARGS
   if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
     // The spread argument should not be pushed.
     __ dec(eax);
   }
+#endif
 
   // Push arguments and move return address and stack spill slots to the top of
   // stack. The eax register is readonly. The ecx register will be modified. edx
@@ -1391,10 +1505,17 @@ void Builtins::Generate_InterpreterPushArgsThenConstructImpl(
     __ Drop(1);  // The allocation site is unused.
     __ Pop(kJavaScriptCallNewTargetRegister);
     __ Pop(kJavaScriptCallTargetRegister);
+#ifdef V8_REVERSE_JSARGS
     // Pass the spread in the register ecx, overwriting ecx.
     __ mov(ecx, Operand(ecx, 0));
     __ PushReturnAddressFrom(eax);
     __ movd(eax, xmm0);  // Reload number of arguments.
+#else
+    __ Pop(ecx);  // Pop the spread (i.e. the first argument), overwriting ecx.
+    __ PushReturnAddressFrom(eax);
+    __ movd(eax, xmm0);         // Reload number of arguments.
+    __ sub(eax, Immediate(1));  // The actual argc thus decrements by one.
+#endif
     __ Jump(BUILTIN_CODE(masm->isolate(), ConstructWithSpread),
             RelocInfo::CODE_TARGET);
   } else {
@@ -1551,6 +1672,7 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
   const RegisterConfiguration* config(RegisterConfiguration::Default());
   int allocatable_register_count = config->num_allocatable_general_registers();
   if (with_result) {
+#ifdef V8_REVERSE_JSARGS
     if (java_script_builtin) {
       // xmm0 is not included in the allocateable registers.
       __ movd(xmm0, eax);
@@ -1563,6 +1685,14 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
                            BuiltinContinuationFrameConstants::kFixedFrameSize),
           eax);
     }
+#else
+    // Overwrite the hole inserted by the deoptimizer with the return value from
+    // the LAZY deopt point.
+    __ mov(Operand(esp, config->num_allocatable_general_registers() *
+                                kSystemPointerSize +
+                            BuiltinContinuationFrameConstants::kFixedFrameSize),
+           eax);
+#endif
   }
 
   // Replace the builtin index Smi on the stack with the start address of the
@@ -1580,6 +1710,7 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
       __ SmiUntag(Register::from_code(code));
     }
   }
+#ifdef V8_REVERSE_JSARGS
   if (with_result && java_script_builtin) {
     // Overwrite the hole inserted by the deoptimizer with the return value from
     // the LAZY deopt point. eax contains the arguments count, the return value
@@ -1588,6 +1719,7 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
                     BuiltinContinuationFrameConstants::kFixedFrameSize),
             xmm0);
   }
+#endif
   __ mov(
       ebp,
       Operand(esp, BuiltinContinuationFrameConstants::kFixedFrameSizeFromFp));
@@ -1711,6 +1843,7 @@ void Builtins::Generate_FunctionPrototypeCall(MacroAssembler* masm) {
   // esp[8 * (n + 1)] : Argument n
   // eax contains the number of arguments, n, not counting the receiver.
 
+#ifdef V8_REVERSE_JSARGS
   // 1. Get the callable to call (passed as receiver) from the stack.
   {
     StackArgumentsAccessor args(eax);
@@ -1735,6 +1868,41 @@ void Builtins::Generate_FunctionPrototypeCall(MacroAssembler* masm) {
   // original callable), making the original first argument the new receiver.
   __ PushReturnAddressFrom(edx);
   __ dec(eax);  // One fewer argument (first argument is new receiver).
+#else
+  // 1. Make sure we have at least one argument.
+  {
+    Label done;
+    __ test(eax, eax);
+    __ j(not_zero, &done, Label::kNear);
+    __ PopReturnAddressTo(edx);
+    __ PushRoot(RootIndex::kUndefinedValue);
+    __ PushReturnAddressFrom(edx);
+    __ inc(eax);
+    __ bind(&done);
+  }
+
+  // 2. Get the callable to call (passed as receiver) from the stack.
+  {
+    StackArgumentsAccessor args(eax);
+    __ mov(edi, args.GetReceiverOperand());
+  }
+
+  // 3. Shift arguments and return address one slot down on the stack
+  //    (overwriting the original receiver).  Adjust argument count to make
+  //    the original first argument the new receiver.
+  {
+    Label loop;
+    __ mov(ecx, eax);
+    __ bind(&loop);
+    __ mov(edx, Operand(esp, ecx, times_system_pointer_size, 0));
+    __ mov(Operand(esp, ecx, times_system_pointer_size, kSystemPointerSize),
+           edx);
+    __ dec(ecx);
+    __ j(not_sign, &loop);  // While non-negative (to copy return address).
+    __ pop(edx);            // Discard copy of return address.
+    __ dec(eax);  // One fewer argument (first argument is new receiver).
+  }
+#endif
 
   // 5. Call the callable.
   __ Jump(masm->isolate()->builtins()->Call(), RelocInfo::CODE_TARGET);
@@ -1948,6 +2116,7 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
   Label stack_overflow;
   __ StackOverflowCheck(kArgumentsLength, edx, &stack_overflow);
 
+#ifdef V8_REVERSE_JSARGS
   __ movd(xmm4, kArgumentsList);  // Spill the arguments list.
 
   // Move the arguments already in the stack,
@@ -1996,6 +2165,29 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
     __ jmp(&loop);
     __ bind(&done);
   }
+#else   // !V8_REVERSE_JSARGS
+  // Push additional arguments onto the stack.
+  {
+    __ PopReturnAddressTo(edx);
+    __ Move(eax, Immediate(0));
+    Label done, push, loop;
+    __ bind(&loop);
+    __ cmp(eax, kArgumentsLength);
+    __ j(equal, &done, Label::kNear);
+    // Turn the hole into undefined as we go.
+    __ mov(edi, FieldOperand(kArgumentsList, eax, times_tagged_size,
+                             FixedArray::kHeaderSize));
+    __ CompareRoot(edi, RootIndex::kTheHoleValue);
+    __ j(not_equal, &push, Label::kNear);
+    __ LoadRoot(edi, RootIndex::kUndefinedValue);
+    __ bind(&push);
+    __ Push(edi);
+    __ inc(eax);
+    __ jmp(&loop);
+    __ bind(&done);
+    __ PushReturnAddressFrom(edx);
+  }
+#endif  // !V8_REVERSE_JSARGS
 
   // Restore eax, edi and edx.
   __ movd(esi, xmm3);  // Restore the context.
@@ -2097,6 +2289,7 @@ void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
     // -----------------------------------
 
     // Forward the arguments from the caller frame.
+#ifdef V8_REVERSE_JSARGS
     __ movd(xmm2, edi);  // Preserve the target to call.
     __ StackOverflowCheck(edx, edi, &stack_overflow);
     __ movd(xmm3, ebx);  // Preserve root register.
@@ -2152,6 +2345,20 @@ void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
 
     __ movd(ebx, xmm3);  // Restore root register.
     __ movd(edi, xmm2);  // Restore the target to call.
+#else
+    __ StackOverflowCheck(edx, ecx, &stack_overflow);
+    Label loop;
+    __ add(eax, edx);
+    __ PopReturnAddressTo(ecx);
+    __ bind(&loop);
+    {
+      __ dec(edx);
+      __ Push(Operand(scratch, edx, times_system_pointer_size,
+                      kFPOnStackSize + kPCOnStackSize));
+      __ j(not_zero, &loop);
+    }
+    __ PushReturnAddressFrom(ecx);
+#endif
   }
   __ bind(&stack_done);
 
@@ -2162,7 +2369,9 @@ void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
   __ Jump(code, RelocInfo::CODE_TARGET);
 
   __ bind(&stack_overflow);
+#ifdef V8_REVERSE_JSARGS
   __ movd(edi, xmm2);  // Restore the target to call.
+#endif
   __ movd(esi, xmm0);  // Restore the context.
   __ TailCallRuntime(Runtime::kThrowStackOverflow);
 }
@@ -2290,6 +2499,7 @@ void Generate_PushBoundArguments(MacroAssembler* masm) {
   __ SmiUntag(edx);
   __ test(edx, edx);
   __ j(zero, &no_bound_arguments);
+#ifdef V8_REVERSE_JSARGS
   {
     // ----------- S t a t e -------------
     //  -- eax  : the number of arguments (not including the receiver)
@@ -2349,6 +2559,85 @@ void Generate_PushBoundArguments(MacroAssembler* masm) {
     // Restore context.
     __ movd(esi, xmm3);
   }
+#else  // !V8_REVERSE_JSARGS
+  {
+    // ----------- S t a t e -------------
+    //  -- eax  : the number of arguments (not including the receiver)
+    //  -- xmm0 : new.target (only in case of [[Construct]])
+    //  -- edi  : target (checked to be a JSBoundFunction)
+    //  -- ecx  : the [[BoundArguments]] (implemented as FixedArray)
+    //  -- edx  : the number of [[BoundArguments]]
+    // -----------------------------------
+
+    // Reserve stack space for the [[BoundArguments]].
+    {
+      Label done;
+      __ lea(ecx, Operand(edx, times_system_pointer_size, 0));
+      __ sub(esp, ecx);  // Not Windows-friendly, but corrected below.
+      // Check the stack for overflow. We are not trying to catch interruptions
+      // (i.e. debug break and preemption) here, so check the "real stack
+      // limit".
+      __ CompareStackLimit(esp, StackLimitKind::kRealStackLimit);
+      __ j(above_equal, &done, Label::kNear);
+      // Restore the stack pointer.
+      __ lea(esp, Operand(esp, edx, times_system_pointer_size, 0));
+      {
+        FrameScope scope(masm, StackFrame::MANUAL);
+        __ EnterFrame(StackFrame::INTERNAL);
+        __ CallRuntime(Runtime::kThrowStackOverflow);
+      }
+      __ bind(&done);
+    }
+#if V8_OS_WIN
+    // Correctly allocate the stack space that was checked above.
+    {
+      Label win_done;
+      __ cmp(ecx, TurboAssemblerBase::kStackPageSize);
+      __ j(less_equal, &win_done, Label::kNear);
+      // Reset esp and walk through the range touching every page.
+      __ lea(esp, Operand(esp, edx, times_system_pointer_size, 0));
+      __ AllocateStackSpace(ecx);
+      __ bind(&win_done);
+    }
+#endif
+
+    // Adjust effective number of arguments to include return address.
+    __ inc(eax);
+
+    // Relocate arguments and return address down the stack.
+    {
+      Label loop;
+      __ Set(ecx, 0);
+      __ lea(edx, Operand(esp, edx, times_system_pointer_size, 0));
+      __ bind(&loop);
+      __ movd(xmm1, Operand(edx, ecx, times_system_pointer_size, 0));
+      __ movd(Operand(esp, ecx, times_system_pointer_size, 0), xmm1);
+      __ inc(ecx);
+      __ cmp(ecx, eax);
+      __ j(less, &loop);
+    }
+
+    // Copy [[BoundArguments]] to the stack (below the arguments).
+    {
+      Label loop;
+      __ mov(ecx, FieldOperand(edi, JSBoundFunction::kBoundArgumentsOffset));
+      __ mov(edx, FieldOperand(ecx, FixedArray::kLengthOffset));
+      __ SmiUntag(edx);
+      __ bind(&loop);
+      __ dec(edx);
+      __ movd(xmm1, FieldOperand(ecx, edx, times_tagged_size,
+                                 FixedArray::kHeaderSize));
+      __ movd(Operand(esp, eax, times_system_pointer_size, 0), xmm1);
+      __ lea(eax, Operand(eax, 1));
+      __ j(greater, &loop);
+    }
+
+    // Adjust effective number of arguments (eax contains the number of
+    // arguments from the call plus return address plus the number of
+    // [[BoundArguments]]), so we need to subtract one for the return address.
+    __ dec(eax);
+  }
+#endif  // !V8_REVERSE_JSARGS
 
   __ bind(&no_bound_arguments);
   __ movd(edx, xmm0);  // Reload edx.
@@ -2576,7 +2865,11 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
 
     // Copy receiver and all expected arguments.
     const int offset = StandardFrameConstants::kCallerSPOffset;
+#ifdef V8_REVERSE_JSARGS
     __ lea(edi, Operand(ebp, ecx, times_system_pointer_size, offset));
+#else
+    __ lea(edi, Operand(ebp, eax, times_system_pointer_size, offset));
+#endif
     __ mov(eax, -1);  // account for receiver
 
     Label copy;
@@ -2601,6 +2894,7 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
     // Remember expected arguments in xmm0.
     __ movd(xmm0, kExpectedNumberOfArgumentsRegister);
 
+#ifdef V8_REVERSE_JSARGS
     // Remember new target.
     __ movd(xmm1, edx);
 
@@ -2628,6 +2922,32 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
 
     // Restore new.target
     __ movd(edx, xmm1);
+#else   // !V8_REVERSE_JSARGS
+    // Copy receiver and all actual arguments.
+    const int offset = StandardFrameConstants::kCallerSPOffset;
+    __ lea(edi, Operand(ebp, eax, times_system_pointer_size, offset));
+    // ecx = expected - actual.
+    __ sub(kExpectedNumberOfArgumentsRegister, eax);
+    // eax = -actual - 1
+    __ neg(eax);
+    __ sub(eax, Immediate(1));
+
+    Label copy;
+    __ bind(&copy);
+    __ inc(eax);
+    __ push(Operand(edi, 0));
+    __ sub(edi, Immediate(kSystemPointerSize));
+    __ test(eax, eax);
+    __ j(not_zero, &copy);
+
+    // Fill remaining expected arguments with undefined values.
+    Label fill;
+    __ bind(&fill);
+    __ inc(eax);
+    __ Push(Immediate(masm->isolate()->factory()->undefined_value()));
+    __ cmp(eax, kExpectedNumberOfArgumentsRegister);
+    __ j(less, &fill);
+#endif  // !V8_REVERSE_JSARGS
 
     // Restore expected arguments.
     __ movd(eax, xmm0);
@@ -3289,8 +3609,13 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
 
   // FunctionCallbackInfo::values_ (points at the first varargs argument passed
   // on the stack).
+#ifdef V8_REVERSE_JSARGS
   __ lea(scratch,
          Operand(scratch, (FCA::kArgsLength + 1) * kSystemPointerSize));
+#else
+  __ lea(scratch, Operand(scratch, argc, times_system_pointer_size,
+                          (FCA::kArgsLength - 1) * kSystemPointerSize));
+#endif
   __ mov(ApiParameterOperand(kApiArgc + 1), scratch);
 
   // FunctionCallbackInfo::length_.
diff --git a/src/builtins/mips/builtins-mips.cc b/src/builtins/mips/builtins-mips.cc
index cba65817a4..a0b5117876 100644
--- a/src/builtins/mips/builtins-mips.cc
+++ b/src/builtins/mips/builtins-mips.cc
@@ -86,6 +86,7 @@ void Generate_JSBuiltinsConstructStubHelper(MacroAssembler* masm) {
     __ SmiTag(a0);
     __ Push(cp, a0);
     __ SmiUntag(a0);
+#ifdef V8_REVERSE_JSARGS
     // Set up pointer to last argument (skip receiver).
     __ Addu(
         t2, fp,
@@ -94,6 +95,15 @@ void Generate_JSBuiltinsConstructStubHelper(MacroAssembler* masm) {
     __ PushArray(t2, a0, t3, t0);
     // The receiver for the builtin/api call.
     __ PushRoot(RootIndex::kTheHoleValue);
+#else
+    // The receiver for the builtin/api call.
+    __ PushRoot(RootIndex::kTheHoleValue);
+    // Set up pointer to last argument.
+    __ Addu(t2, fp, Operand(StandardFrameConstants::kCallerSPOffset));
+
+    // Copy arguments and receiver to the expression stack.
+    __ PushArray(t2, a0, t3, t0);
+#endif
 
     // Call the function.
     // a0: number of arguments (untagged)
@@ -179,6 +189,7 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
     // Restore new target.
     __ Pop(a3);
 
+#ifdef V8_REVERSE_JSARGS
     // Push the allocated receiver to the stack.
     __ Push(v0);
     // We need two copies because we may have to return the original one
@@ -191,6 +202,15 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
     __ Addu(
         t2, fp,
         Operand(StandardFrameConstants::kCallerSPOffset + kSystemPointerSize));
+#else
+    // Push the allocated receiver to the stack. We need two copies
+    // because we may have to return the original one and the calling
+    // conventions dictate that the called function pops the receiver.
+    __ Push(v0, v0);
+
+    // Set up pointer to last argument.
+    __ Addu(t2, fp, Operand(StandardFrameConstants::kCallerSPOffset));
+#endif
 
     // ----------- S t a t e -------------
     //  --                 r3: new target
@@ -227,10 +247,12 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
 
     // Copy arguments and receiver to the expression stack.
     __ PushArray(t2, a0, t0, t1);
+#ifdef V8_REVERSE_JSARGS
     // We need two copies because we may have to return the original one
     // and the calling conventions dictate that the called function pops the
     // receiver. The second copy is pushed after the arguments.
     __ Push(s0);
+#endif
 
     // Call the function.
     __ InvokeFunctionWithNewTarget(a1, a3, a0, CALL_FUNCTION);
@@ -542,6 +564,7 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
     // Copy arguments to the stack in a loop.
     // a0: argc
     // s0: argv, i.e. points to first arg
+#ifdef V8_REVERSE_JSARGS
     Label loop, entry;
     __ Lsa(t2, s0, a0, kPointerSizeLog2);
     __ b(&entry);
@@ -557,6 +580,23 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
 
     // Push the receiver.
     __ Push(a3);
+#else
+    // Push the receiver.
+    __ Push(a3);
+
+    Label loop, entry;
+    __ Lsa(t2, s0, a0, kPointerSizeLog2);
+    __ b(&entry);
+    __ nop();  // Branch delay slot nop.
+    // t2 points past last arg.
+    __ bind(&loop);
+    __ lw(t0, MemOperand(s0));  // Read next parameter.
+    __ addiu(s0, s0, kPointerSize);
+    __ lw(t0, MemOperand(t0));  // Dereference handle.
+    __ push(t0);                // Push parameter.
+    __ bind(&entry);
+    __ Branch(&loop, ne, s0, Operand(t2));
+#endif
 
     // a0: argc
     // a1: function
@@ -658,11 +698,18 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
                     MacroAssembler::StackLimitKind::kRealStackLimit);
   __ Branch(&stack_overflow, lo, sp, Operand(kScratchReg));
 
+#ifndef V8_REVERSE_JSARGS
+  // Push receiver.
+  __ lw(t1, FieldMemOperand(a1, JSGeneratorObject::kReceiverOffset));
+  __ Push(t1);
+#endif
+
   // ----------- S t a t e -------------
   //  -- a1    : the JSGeneratorObject to resume
   //  -- t0    : generator function
   //  -- cp    : generator context
   //  -- ra    : return address
+  //  -- sp[0] : generator receiver
   // -----------------------------------
 
   // Copy the function arguments from the generator object's register file.
@@ -673,6 +720,7 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
   __ lw(t1,
         FieldMemOperand(a1, JSGeneratorObject::kParametersAndRegistersOffset));
   {
+#ifdef V8_REVERSE_JSARGS
     Label done_loop, loop;
     __ bind(&loop);
     __ Subu(a3, a3, Operand(1));
@@ -685,6 +733,19 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
     // Push receiver.
     __ Lw(kScratchReg, FieldMemOperand(a1, JSGeneratorObject::kReceiverOffset));
     __ Push(kScratchReg);
+#else
+    Label done_loop, loop;
+    __ Move(t2, zero_reg);
+    __ bind(&loop);
+    __ Subu(a3, a3, Operand(1));
+    __ Branch(&done_loop, lt, a3, Operand(zero_reg));
+    __ Lsa(kScratchReg, t1, t2, kPointerSizeLog2);
+    __ lw(kScratchReg, FieldMemOperand(kScratchReg, FixedArray::kHeaderSize));
+    __ Push(kScratchReg);
+    __ Addu(t2, t2, Operand(1));
+    __ Branch(&loop);
+    __ bind(&done_loop);
+#endif
   }
 
   // Underlying function needs to have bytecode available.
@@ -1199,8 +1260,12 @@ static void Generate_InterpreterPushArgs(MacroAssembler* masm,
   __ Subu(start_address, start_address, scratch);
 
   // Push the arguments.
+#ifdef V8_REVERSE_JSARGS
   __ PushArray(start_address, num_args, scratch, scratch2,
                TurboAssembler::PushArrayOrder::kReverse);
+#else
+  __ PushArray(start_address, num_args, scratch, scratch2);
+#endif
 }
 
 // static
@@ -1216,15 +1281,19 @@ void Builtins::Generate_InterpreterPushArgsThenCallImpl(
   //  -- a1 : the target to call (can be any Object).
   // -----------------------------------
   Label stack_overflow;
+
+#ifdef V8_REVERSE_JSARGS
   if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
     // The spread argument should not be pushed.
     __ Subu(a0, a0, Operand(1));
   }
+#endif
 
   __ Addu(t0, a0, Operand(1));  // Add one for receiver.
 
   __ StackOverflowCheck(t0, t4, t1, &stack_overflow);
 
+#ifdef V8_REVERSE_JSARGS
   if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
     // Don't copy receiver.
     __ mov(t0, a0);
@@ -1243,6 +1312,21 @@ void Builtins::Generate_InterpreterPushArgsThenCallImpl(
     // is below that.
     __ Lw(a2, MemOperand(a2, -kSystemPointerSize));
   }
+#else
+  // Push "undefined" as the receiver arg if we need to.
+  if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
+    __ PushRoot(RootIndex::kUndefinedValue);
+    __ mov(t0, a0);  // No receiver.
+  }
+
+  // This function modifies a2, t4 and t1.
+  Generate_InterpreterPushArgs(masm, t0, a2, t4, t1);
+
+  if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
+    __ Pop(a2);                   // Pass the spread in a register
+    __ Subu(a0, a0, Operand(1));  // Subtract one for spread
+  }
+#endif
 
   // Call the target.
   if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
@@ -1275,6 +1359,7 @@ void Builtins::Generate_InterpreterPushArgsThenConstructImpl(
   __ addiu(t2, a0, 1);
   __ StackOverflowCheck(t2, t1, t0, &stack_overflow);
 
+#ifdef V8_REVERSE_JSARGS
   if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
     // The spread argument should not be pushed.
     __ Subu(a0, a0, Operand(1));
@@ -1294,6 +1379,20 @@ void Builtins::Generate_InterpreterPushArgsThenConstructImpl(
   } else {
     __ AssertUndefinedOrAllocationSite(a2, t0);
   }
+#else
+  // Push a slot for the receiver.
+  __ push(zero_reg);
+
+  // This function modified t4, t1 and t0.
+  Generate_InterpreterPushArgs(masm, a0, t4, t1, t0);
+
+  if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
+    __ Pop(a2);                   // Pass the spread in a register
+    __ Subu(a0, a0, Operand(1));  // Subtract one for spread
+  } else {
+    __ AssertUndefinedOrAllocationSite(a2, t0);
+  }
+#endif
 
   if (mode == InterpreterPushArgsMode::kArrayFunction) {
     __ AssertFunction(a1);
@@ -1457,6 +1556,7 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
   Register scratch = temps.Acquire();  // Temp register is not allocatable.
   // Register scratch = t3;
   if (with_result) {
+#ifdef V8_REVERSE_JSARGS
     if (java_script_builtin) {
       __ mov(scratch, v0);
     } else {
@@ -1467,6 +1567,15 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
                 sp, config->num_allocatable_general_registers() * kPointerSize +
                         BuiltinContinuationFrameConstants::kFixedFrameSize));
     }
+#else
+    // Overwrite the hole inserted by the deoptimizer with the return value from
+    // the LAZY deopt point.
+    __ sw(v0,
+          MemOperand(
+              sp, config->num_allocatable_general_registers() * kPointerSize +
+                      BuiltinContinuationFrameConstants::kFixedFrameSize));
+    USE(scratch);
+#endif
   }
   for (int i = allocatable_register_count - 1; i >= 0; --i) {
     int code = config->GetAllocatableGeneralCode(i);
@@ -1476,6 +1585,7 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
     }
   }
 
+#ifdef V8_REVERSE_JSARGS
   if (with_result && java_script_builtin) {
     // Overwrite the hole inserted by the deoptimizer with the return value from
     // the LAZY deopt point. t0 contains the arguments count, the return value
@@ -1488,6 +1598,7 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
     __ Subu(a0, a0,
             Operand(BuiltinContinuationFrameConstants::kFixedSlotCount));
   }
+#endif
 
   __ lw(fp, MemOperand(
                 sp, BuiltinContinuationFrameConstants::kFixedFrameSizeFromFp));
@@ -1570,9 +1681,9 @@ void Builtins::Generate_InterpreterOnStackReplacement(MacroAssembler* masm) {
 void Builtins::Generate_FunctionPrototypeApply(MacroAssembler* masm) {
   // ----------- S t a t e -------------
   //  -- a0    : argc
-  //  -- sp[0] : receiver
+  //  -- sp[0] : argArray
   //  -- sp[4] : thisArg
-  //  -- sp[8] : argArray
+  //  -- sp[8] : receiver
   // -----------------------------------
 
   // 1. Load receiver into a1, argArray into a2 (if present), remove all
@@ -1583,6 +1694,7 @@ void Builtins::Generate_FunctionPrototypeApply(MacroAssembler* masm) {
     __ LoadRoot(a2, RootIndex::kUndefinedValue);
     __ mov(a3, a2);
     // Lsa() cannot be used hare as scratch value used later.
+#ifdef V8_REVERSE_JSARGS
     __ lw(a1, MemOperand(sp));  // receiver
     __ Branch(&no_arg, eq, a0, Operand(zero_reg));
     __ lw(a3, MemOperand(sp, kSystemPointerSize));  // thisArg
@@ -1591,6 +1703,22 @@ void Builtins::Generate_FunctionPrototypeApply(MacroAssembler* masm) {
     __ bind(&no_arg);
     __ Lsa(sp, sp, a0, kPointerSizeLog2);
     __ sw(a3, MemOperand(sp));
+#else
+    Register scratch = t0;
+    __ sll(scratch, a0, kPointerSizeLog2);
+    __ Addu(a0, sp, Operand(scratch));
+    __ lw(a1, MemOperand(a0));  // receiver
+    __ Subu(a0, a0, Operand(kPointerSize));
+    __ Branch(&no_arg, lt, a0, Operand(sp));
+    __ lw(a2, MemOperand(a0));  // thisArg
+    __ Subu(a0, a0, Operand(kPointerSize));
+    __ Branch(&no_arg, lt, a0, Operand(sp));
+    __ lw(a3, MemOperand(a0));  // argArray
+    __ bind(&no_arg);
+    __ Addu(sp, sp, Operand(scratch));
+    __ sw(a2, MemOperand(sp));
+    __ mov(a2, a3);
+#endif
   }
 
   // ----------- S t a t e -------------
@@ -1623,6 +1751,7 @@ void Builtins::Generate_FunctionPrototypeApply(MacroAssembler* masm) {
 
 // static
 void Builtins::Generate_FunctionPrototypeCall(MacroAssembler* masm) {
+#ifdef V8_REVERSE_JSARGS
   // 1. Get the callable to call (passed as receiver) from the stack.
   __ Pop(a1);
 
@@ -1638,6 +1767,42 @@ void Builtins::Generate_FunctionPrototypeCall(MacroAssembler* masm) {
 
   // 3. Adjust the actual number of arguments.
   __ addiu(a0, a0, -1);
+#else
+  // 1. Make sure we have at least one argument.
+  // a0: actual number of arguments
+  {
+    Label done;
+    __ Branch(&done, ne, a0, Operand(zero_reg));
+    __ PushRoot(RootIndex::kUndefinedValue);
+    __ Addu(a0, a0, Operand(1));
+    __ bind(&done);
+  }
+
+  // 2. Get the function to call (passed as receiver) from the stack.
+  // a0: actual number of arguments
+  __ LoadReceiver(a1, a0);
+
+  // 3. Shift arguments and return address one slot down on the stack
+  //    (overwriting the original receiver).  Adjust argument count to make
+  //    the original first argument the new receiver.
+  // a0: actual number of arguments
+  // a1: function
+  {
+    Label loop;
+    // Calculate the copy start address (destination). Copy end address is sp.
+    __ Lsa(a2, sp, a0, kPointerSizeLog2);
+
+    __ bind(&loop);
+    __ lw(kScratchReg, MemOperand(a2, -kPointerSize));
+    __ sw(kScratchReg, MemOperand(a2));
+    __ Subu(a2, a2, Operand(kPointerSize));
+    __ Branch(&loop, ne, a2, Operand(sp));
+    // Adjust the actual number of arguments and remove the top element
+    // (which is a copy of the last argument).
+    __ Subu(a0, a0, Operand(1));
+    __ Pop();
+  }
+#endif
 
   // 4. Call the callable.
   __ Jump(masm->isolate()->builtins()->Call(), RelocInfo::CODE_TARGET);
@@ -1646,10 +1811,10 @@ void Builtins::Generate_FunctionPrototypeCall(MacroAssembler* masm) {
 void Builtins::Generate_ReflectApply(MacroAssembler* masm) {
   // ----------- S t a t e -------------
   //  -- a0     : argc
-  //  -- sp[0]  : receiver
-  //  -- sp[4]  : target         (if argc >= 1)
-  //  -- sp[8]  : thisArgument   (if argc >= 2)
-  //  -- sp[12] : argumentsList  (if argc == 3)
+  //  -- sp[0]  : argumentsList
+  //  -- sp[4]  : thisArgument
+  //  -- sp[8]  : target
+  //  -- sp[12] : receiver
   // -----------------------------------
 
   // 1. Load target into a1 (if present), argumentsList into a0 (if present),
@@ -1660,6 +1825,7 @@ void Builtins::Generate_ReflectApply(MacroAssembler* masm) {
     __ LoadRoot(a1, RootIndex::kUndefinedValue);
     __ mov(a2, a1);
     __ mov(a3, a1);
+#ifdef V8_REVERSE_JSARGS
     __ Branch(&no_arg, eq, a0, Operand(zero_reg));
     __ lw(a1, MemOperand(sp, kSystemPointerSize));  // target
     __ Branch(&no_arg, eq, a0, Operand(1));
@@ -1669,6 +1835,25 @@ void Builtins::Generate_ReflectApply(MacroAssembler* masm) {
     __ bind(&no_arg);
     __ Lsa(sp, sp, a0, kPointerSizeLog2);
     __ sw(a3, MemOperand(sp));
+#else
+    Register scratch = t0;
+    __ sll(scratch, a0, kPointerSizeLog2);
+    __ mov(a0, scratch);
+    __ Subu(a0, a0, Operand(kPointerSize));
+    __ Branch(&no_arg, lt, a0, Operand(zero_reg));
+    __ Addu(a0, sp, Operand(a0));
+    __ lw(a1, MemOperand(a0));  // target
+    __ Subu(a0, a0, Operand(kPointerSize));
+    __ Branch(&no_arg, lt, a0, Operand(sp));
+    __ lw(a2, MemOperand(a0));  // thisArgument
+    __ Subu(a0, a0, Operand(kPointerSize));
+    __ Branch(&no_arg, lt, a0, Operand(sp));
+    __ lw(a3, MemOperand(a0));  // argumentsList
+    __ bind(&no_arg);
+    __ Addu(sp, sp, Operand(scratch));
+    __ sw(a2, MemOperand(sp));
+    __ mov(a2, a3);
+#endif
   }
 
   // ----------- S t a t e -------------
@@ -1689,11 +1874,12 @@ void Builtins::Generate_ReflectApply(MacroAssembler* masm) {
 void Builtins::Generate_ReflectConstruct(MacroAssembler* masm) {
   // ----------- S t a t e -------------
   //  -- a0     : argc
-  //  -- sp[0]  : receiver
-  //  -- sp[4]  : target
-  //  -- sp[8]  : argumentsList
-  //  -- sp[12] : new.target (optional)
+  //  -- sp[0]  : new.target (optional)
+  //  -- sp[4]  : argumentsList
+  //  -- sp[8]  : target
+  //  -- sp[12] : receiver
   // -----------------------------------
+  // NOTE: The order of args in the stack are reversed if V8_REVERSE_JSARGS
 
   // 1. Load target into a1 (if present), argumentsList into a2 (if present),
   // new.target into a3 (if present, otherwise use target), remove all
@@ -1703,6 +1889,7 @@ void Builtins::Generate_ReflectConstruct(MacroAssembler* masm) {
     Label no_arg;
     __ LoadRoot(a1, RootIndex::kUndefinedValue);
     __ mov(a2, a1);
+#ifdef V8_REVERSE_JSARGS
     __ mov(t0, a1);
     __ Branch(&no_arg, eq, a0, Operand(zero_reg));
     __ lw(a1, MemOperand(sp, kSystemPointerSize));  // target
@@ -1714,6 +1901,25 @@ void Builtins::Generate_ReflectConstruct(MacroAssembler* masm) {
     __ bind(&no_arg);
     __ Lsa(sp, sp, a0, kPointerSizeLog2);
     __ sw(t0, MemOperand(sp));  // set undefined to the receiver
+#else
+    Register scratch = t0;
+    // Lsa() cannot be used hare as scratch value used later.
+    __ sll(scratch, a0, kPointerSizeLog2);
+    __ Addu(a0, sp, Operand(scratch));
+    __ sw(a2, MemOperand(a0));  // receiver
+    __ Subu(a0, a0, Operand(kPointerSize));
+    __ Branch(&no_arg, lt, a0, Operand(sp));
+    __ lw(a1, MemOperand(a0));  // target
+    __ mov(a3, a1);             // new.target defaults to target
+    __ Subu(a0, a0, Operand(kPointerSize));
+    __ Branch(&no_arg, lt, a0, Operand(sp));
+    __ lw(a2, MemOperand(a0));  // argumentsList
+    __ Subu(a0, a0, Operand(kPointerSize));
+    __ Branch(&no_arg, lt, a0, Operand(sp));
+    __ lw(a3, MemOperand(a0));  // new.target
+    __ bind(&no_arg);
+    __ Addu(sp, sp, Operand(scratch));
+#endif
   }
 
   // ----------- S t a t e -------------
@@ -1788,6 +1994,7 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
   Label stack_overflow;
   __ StackOverflowCheck(t0, kScratchReg, t1, &stack_overflow);
 
+#ifdef V8_REVERSE_JSARGS
   // Move the arguments already in the stack,
   // including the receiver and the return address.
   {
@@ -1808,6 +2015,7 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
     __ Addu(dest, dest, Operand(kSystemPointerSize));
     __ Branch(&copy, ge, t1, Operand(zero_reg));
   }
+#endif
 
   // Push arguments onto the stack (thisArgument is already on the stack).
   {
@@ -1822,8 +2030,12 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
     __ Branch(&push, ne, t1, Operand(kScratchReg));
     __ LoadRoot(kScratchReg, RootIndex::kUndefinedValue);
     __ bind(&push);
+#ifdef V8_REVERSE_JSARGS
     __ Sw(kScratchReg, MemOperand(t4, 0));
     __ Addu(t4, t4, Operand(kSystemPointerSize));
+#else
+    __ Push(kScratchReg);
+#endif
     __ Branch(&loop);
     __ bind(&done);
     __ Addu(a0, a0, t2);
@@ -1903,6 +2115,8 @@ void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
     __ StackOverflowCheck(t2, t0, t1, &stack_overflow);
 
     // Forward the arguments from the caller frame.
+
+#ifdef V8_REVERSE_JSARGS
     // Point to the first argument to copy (skipping the receiver).
     __ Addu(t3, t3,
             Operand(CommonFrameConstants::kFixedFrameSizeAboveFp +
@@ -1929,20 +2143,28 @@ void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
       __ Addu(dest, dest, Operand(kSystemPointerSize));
       __ Branch(&copy, ge, t7, Operand(zero_reg));
     }
+#endif
 
     // Copy arguments from the caller frame.
     // TODO(victorgomes): Consider using forward order as potentially more cache
     // friendly.
     {
       Label loop;
+#ifndef V8_REVERSE_JSARGS
+      __ Addu(t3, t3, Operand(CommonFrameConstants::kFixedFrameSizeAboveFp));
+#endif
       __ Addu(a0, a0, t2);
       __ bind(&loop);
       {
         __ Subu(t2, t2, Operand(1));
         __ Lsa(kScratchReg, t3, t2, kPointerSizeLog2);
         __ lw(kScratchReg, MemOperand(kScratchReg));
+#ifdef V8_REVERSE_JSARGS
         __ Lsa(t0, a2, t2, kPointerSizeLog2);
         __ Sw(kScratchReg, MemOperand(t0));
+#else
+        __ push(kScratchReg);
+#endif
         __ Branch(&loop, ne, t2, Operand(zero_reg));
       }
     }
@@ -2102,6 +2324,7 @@ void Builtins::Generate_CallBoundFunctionImpl(MacroAssembler* masm) {
     __ bind(&done);
   }
 
+#ifdef V8_REVERSE_JSARGS
   // Pop receiver.
   __ Pop(t1);
 
@@ -2122,6 +2345,42 @@ void Builtins::Generate_CallBoundFunctionImpl(MacroAssembler* masm) {
 
   // Push receiver.
   __ Push(t1);
+#else
+  __ mov(sp, t1);
+  // Relocate arguments down the stack.
+  {
+    Label loop, done_loop;
+    __ mov(t1, zero_reg);
+    __ bind(&loop);
+    __ Branch(&done_loop, gt, t1, Operand(a0));
+    __ Lsa(t2, sp, t0, kPointerSizeLog2);
+    __ lw(kScratchReg, MemOperand(t2));
+    __ Lsa(t2, sp, t1, kPointerSizeLog2);
+    __ sw(kScratchReg, MemOperand(t2));
+    __ Addu(t0, t0, Operand(1));
+    __ Addu(t1, t1, Operand(1));
+    __ Branch(&loop);
+    __ bind(&done_loop);
+  }
+
+  // Copy [[BoundArguments]] to the stack (below the arguments).
+  {
+    Label loop, done_loop;
+    __ lw(t0, FieldMemOperand(a2, FixedArray::kLengthOffset));
+    __ SmiUntag(t0);
+    __ Addu(a2, a2, Operand(FixedArray::kHeaderSize - kHeapObjectTag));
+    __ bind(&loop);
+    __ Subu(t0, t0, Operand(1));
+    __ Branch(&done_loop, lt, t0, Operand(zero_reg));
+    __ Lsa(t1, a2, t0, kPointerSizeLog2);
+    __ lw(kScratchReg, MemOperand(t1));
+    __ Lsa(t1, sp, a0, kPointerSizeLog2);
+    __ sw(kScratchReg, MemOperand(t1));
+    __ Addu(a0, a0, Operand(1));
+    __ Branch(&loop);
+    __ bind(&done_loop);
+  }
+#endif
 
   // Call the [[BoundTargetFunction]] via the Call builtin.
   __ lw(a1, FieldMemOperand(a1, JSBoundFunction::kBoundTargetFunctionOffset));
@@ -2244,6 +2503,7 @@ void Builtins::Generate_ConstructBoundFunction(MacroAssembler* masm) {
     __ bind(&done);
   }
 
+#ifdef V8_REVERSE_JSARGS
   // Pop receiver
   __ Pop(t1);
 
@@ -2264,6 +2524,42 @@ void Builtins::Generate_ConstructBoundFunction(MacroAssembler* masm) {
 
   // Push receiver.
   __ Push(t1);
+#else
+  __ mov(sp, t1);
+  // Relocate arguments down the stack.
+  {
+    Label loop, done_loop;
+    __ mov(t1, zero_reg);
+    __ bind(&loop);
+    __ Branch(&done_loop, ge, t1, Operand(a0));
+    __ Lsa(t2, sp, t0, kPointerSizeLog2);
+    __ lw(kScratchReg, MemOperand(t2));
+    __ Lsa(t2, sp, t1, kPointerSizeLog2);
+    __ sw(kScratchReg, MemOperand(t2));
+    __ Addu(t0, t0, Operand(1));
+    __ Addu(t1, t1, Operand(1));
+    __ Branch(&loop);
+    __ bind(&done_loop);
+  }
+
+  // Copy [[BoundArguments]] to the stack (below the arguments).
+  {
+    Label loop, done_loop;
+    __ lw(t0, FieldMemOperand(a2, FixedArray::kLengthOffset));
+    __ SmiUntag(t0);
+    __ Addu(a2, a2, Operand(FixedArray::kHeaderSize - kHeapObjectTag));
+    __ bind(&loop);
+    __ Subu(t0, t0, Operand(1));
+    __ Branch(&done_loop, lt, t0, Operand(zero_reg));
+    __ Lsa(t1, a2, t0, kPointerSizeLog2);
+    __ lw(kScratchReg, MemOperand(t1));
+    __ Lsa(t1, sp, a0, kPointerSizeLog2);
+    __ sw(kScratchReg, MemOperand(t1));
+    __ Addu(a0, a0, Operand(1));
+    __ Branch(&loop);
+    __ bind(&done_loop);
+  }
+#endif
 
   // Patch new.target to [[BoundTargetFunction]] if new.target equals target.
   {
@@ -2357,7 +2653,11 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
     __ StackOverflowCheck(a2, t1, kScratchReg, &stack_overflow);
 
     // Calculate copy start address into a0 and copy end address into t1.
+#ifdef V8_REVERSE_JSARGS
     __ Lsa(a0, fp, a2, kPointerSizeLog2);
+#else
+    __ Lsa(a0, fp, a0, kPointerSizeLog2 - kSmiTagSize);
+#endif
     // Adjust for return address and receiver.
     __ Addu(a0, a0, Operand(2 * kPointerSize));
     // Compute copy end address.
@@ -2386,6 +2686,7 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
     EnterArgumentsAdaptorFrame(masm);
     __ StackOverflowCheck(a2, t1, kScratchReg, &stack_overflow);
 
+#ifdef V8_REVERSE_JSARGS
     // Fill the remaining expected arguments with undefined.
     __ LoadRoot(t0, RootIndex::kUndefinedValue);
     __ SmiUntag(t2, a0);
@@ -2415,6 +2716,50 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
 
     __ Branch(USE_DELAY_SLOT, &copy, ne, a0, Operand(fp));
     __ Subu(a0, a0, Operand(kSystemPointerSize));
+#else
+    // Calculate copy start address into a0 and copy end address into t3.
+    // a0: actual number of arguments as a smi
+    // a1: function
+    // a2: expected number of arguments
+    // a3: new target (passed through to callee)
+    __ Lsa(a0, fp, a0, kPointerSizeLog2 - kSmiTagSize);
+    // Adjust for return address and receiver.
+    __ Addu(a0, a0, Operand(2 * kPointerSize));
+    // Compute copy end address. Also adjust for return address.
+    __ Addu(t3, fp, kPointerSize);
+
+    // Copy the arguments (including the receiver) to the new stack frame.
+    // a0: copy start address
+    // a1: function
+    // a2: expected number of arguments
+    // a3: new target (passed through to callee)
+    // t3: copy end address
+    Label copy;
+    __ bind(&copy);
+    __ lw(t0, MemOperand(a0));  // Adjusted above for return addr and receiver.
+    __ Subu(sp, sp, kPointerSize);
+    __ Subu(a0, a0, kPointerSize);
+    __ Branch(USE_DELAY_SLOT, &copy, ne, a0, Operand(t3));
+    __ sw(t0, MemOperand(sp));  // In the delay slot.
+
+    // Fill the remaining expected arguments with undefined.
+    // a1: function
+    // a2: expected number of arguments
+    // a3: new target (passed through to callee)
+    __ LoadRoot(t0, RootIndex::kUndefinedValue);
+    __ sll(t2, a2, kPointerSizeLog2);
+    __ Subu(t1, fp, Operand(t2));
+    // Adjust for frame.
+    __ Subu(t1, t1,
+            Operand(ArgumentsAdaptorFrameConstants::kFixedFrameSizeFromFp +
+                    kPointerSize));
+
+    Label fill;
+    __ bind(&fill);
+    __ Subu(sp, sp, kPointerSize);
+    __ Branch(USE_DELAY_SLOT, &fill, ne, sp, Operand(t1));
+    __ sw(t0, MemOperand(sp));
+#endif
   }
 
   // Call the entry point.
@@ -2909,10 +3254,11 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
   //  -- a2                  : arguments count (not including the receiver)
   //  -- a3                  : call data
   //  -- a0                  : holder
-  //  -- sp[0]               : receiver
-  //  -- sp[8]               : first argument
+  //  --
+  //  -- sp[0]               : last argument
   //  -- ...
-  //  -- sp[(argc) * 8]      : last argument
+  //  -- sp[(argc - 1) * 4]  : first argument
+  //  -- sp[(argc + 0) * 4]  : receiver
   // -----------------------------------
 
   Register api_function_address = a1;
@@ -2987,8 +3333,15 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
 
   // FunctionCallbackInfo::values_ (points at the first varargs argument passed
   // on the stack).
+#ifdef V8_REVERSE_JSARGS
   __ Addu(scratch, scratch,
           Operand((FCA::kArgsLength + 1) * kSystemPointerSize));
+#else
+  __ Addu(scratch, scratch,
+          Operand((FCA::kArgsLength - 1) * kSystemPointerSize));
+  __ sll(t2, argc, kSystemPointerSizeLog2);
+  __ Addu(scratch, scratch, t2);
+#endif
   __ sw(scratch, MemOperand(sp, 2 * kPointerSize));
 
   // FunctionCallbackInfo::length_.
diff --git a/src/builtins/mips64/builtins-mips64.cc b/src/builtins/mips64/builtins-mips64.cc
index 1027ec35e5..1d3eeefc3a 100644
--- a/src/builtins/mips64/builtins-mips64.cc
+++ b/src/builtins/mips64/builtins-mips64.cc
@@ -86,6 +86,7 @@ void Generate_JSBuiltinsConstructStubHelper(MacroAssembler* masm) {
     __ Push(cp, a0);
     __ SmiUntag(a0);
 
+#ifdef V8_REVERSE_JSARGS
     // Set up pointer to last argument (skip receiver).
     __ Daddu(
         t2, fp,
@@ -94,6 +95,15 @@ void Generate_JSBuiltinsConstructStubHelper(MacroAssembler* masm) {
     __ PushArray(t2, a0, t3, t0);
     // The receiver for the builtin/api call.
     __ PushRoot(RootIndex::kTheHoleValue);
+#else
+    // The receiver for the builtin/api call.
+    __ PushRoot(RootIndex::kTheHoleValue);
+    // Set up pointer to last argument.
+    __ Daddu(t2, fp, Operand(StandardFrameConstants::kCallerSPOffset));
+
+    // Copy arguments and receiver to the expression stack.
+    __ PushArray(t2, a0, t3, t0);
+#endif
 
     // Call the function.
     // a0: number of arguments (untagged)
@@ -180,6 +190,7 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
     // Restore new target.
     __ Pop(a3);
 
+#ifdef V8_REVERSE_JSARGS
     // Push the allocated receiver to the stack.
     __ Push(v0);
 
@@ -192,6 +203,15 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
     // Set up pointer to last argument.
     __ Daddu(t2, fp, Operand(StandardFrameConstants::kCallerSPOffset +
                              kSystemPointerSize));
+#else
+    // Push the allocated receiver to the stack. We need two copies
+    // because we may have to return the original one and the calling
+    // conventions dictate that the called function pops the receiver.
+    __ Push(v0, v0);
+
+    // Set up pointer to last argument.
+    __ Daddu(t2, fp, Operand(StandardFrameConstants::kCallerSPOffset));
+#endif
 
     // ----------- S t a t e -------------
     //  --                 r3: new target
@@ -228,10 +248,12 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
 
     // Copy arguments and receiver to the expression stack.
     __ PushArray(t2, a0, t0, t1);
+#ifdef V8_REVERSE_JSARGS
     // We need two copies because we may have to return the original one
     // and the calling conventions dictate that the called function pops the
     // receiver. The second copy is pushed after the arguments,
     __ Push(a6);
+#endif
 
     // Call the function.
     __ InvokeFunctionWithNewTarget(a1, a3, a0, CALL_FUNCTION);
@@ -353,11 +375,18 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
                     MacroAssembler::StackLimitKind::kRealStackLimit);
   __ Branch(&stack_overflow, lo, sp, Operand(kScratchReg));
 
+#ifndef V8_REVERSE_JSARGS
+  // Push receiver.
+  __ Ld(a5, FieldMemOperand(a1, JSGeneratorObject::kReceiverOffset));
+  __ Push(a5);
+#endif
+
   // ----------- S t a t e -------------
   //  -- a1    : the JSGeneratorObject to resume
   //  -- a4    : generator function
   //  -- cp    : generator context
   //  -- ra    : return address
+  //  -- sp[0] : generator receiver
   // -----------------------------------
 
   // Push holes for arguments to generator function. Since the parser forced
@@ -370,6 +399,7 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
   __ Ld(t1,
         FieldMemOperand(a1, JSGeneratorObject::kParametersAndRegistersOffset));
   {
+#ifdef V8_REVERSE_JSARGS
     Label done_loop, loop;
     __ bind(&loop);
     __ Dsubu(a3, a3, Operand(1));
@@ -382,6 +412,19 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
     // Push receiver.
     __ Ld(kScratchReg, FieldMemOperand(a1, JSGeneratorObject::kReceiverOffset));
     __ Push(kScratchReg);
+#else
+    Label done_loop, loop;
+    __ Move(t2, zero_reg);
+    __ bind(&loop);
+    __ Dsubu(a3, a3, Operand(1));
+    __ Branch(&done_loop, lt, a3, Operand(zero_reg));
+    __ Dlsa(kScratchReg, t1, t2, kPointerSizeLog2);
+    __ Ld(kScratchReg, FieldMemOperand(kScratchReg, FixedArray::kHeaderSize));
+    __ Push(kScratchReg);
+    __ Daddu(t2, t2, Operand(1));
+    __ Branch(&loop);
+    __ bind(&done_loop);
+#endif
   }
 
   // Underlying function needs to have bytecode available.
@@ -700,6 +743,7 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
     // Copy arguments to the stack in a loop.
     // a4: argc
     // a5: argv, i.e. points to first arg
+#ifdef V8_REVERSE_JSARGS
     Label loop, entry;
     __ Dlsa(s1, a5, a4, kPointerSizeLog2);
     __ b(&entry);
@@ -715,6 +759,24 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
 
     // Push the receive.
     __ Push(a3);
+#else
+    // Push the receive.
+    __ Push(a3);
+
+    Label loop, entry;
+    __ Dlsa(s1, a5, a4, kPointerSizeLog2);
+    __ b(&entry);
+    __ nop();  // Branch delay slot nop.
+    // s1 points past last arg.
+    __ bind(&loop);
+    __ Ld(s2, MemOperand(a5));  // Read next parameter.
+    __ daddiu(a5, a5, kPointerSize);
+    __ Ld(s2, MemOperand(s2));  // Dereference handle.
+    __ push(s2);                // Push parameter.
+    __ bind(&entry);
+    __ Branch(&loop, ne, a5, Operand(s1));
+
+#endif
 
     // a0: argc
     // a1: function
@@ -1218,8 +1280,12 @@ static void Generate_InterpreterPushArgs(MacroAssembler* masm,
   __ Dsubu(start_address, start_address, scratch);
 
   // Push the arguments.
-  __ PushArray(start_address, num_args, scratch, scratch2,
-               TurboAssembler::PushArrayOrder::kReverse);
+#ifdef V8_REVERSE_JSARGS
+    __ PushArray(start_address, num_args, scratch, scratch2,
+                 TurboAssembler::PushArrayOrder::kReverse);
+#else
+    __ PushArray(start_address, num_args, scratch, scratch2);
+#endif
 }
 
 // static
@@ -1235,15 +1301,19 @@ void Builtins::Generate_InterpreterPushArgsThenCallImpl(
   //  -- a1 : the target to call (can be any Object).
   // -----------------------------------
   Label stack_overflow;
+
+#ifdef V8_REVERSE_JSARGS
   if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
     // The spread argument should not be pushed.
     __ Dsubu(a0, a0, Operand(1));
   }
+#endif
 
   __ Daddu(a3, a0, Operand(1));  // Add one for receiver.
 
   __ StackOverflowCheck(a3, a4, t0, &stack_overflow);
 
+#ifdef V8_REVERSE_JSARGS
   if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
     // Don't copy receiver.
     __ mov(a3, a0);
@@ -1262,6 +1332,21 @@ void Builtins::Generate_InterpreterPushArgsThenCallImpl(
     // is below that.
     __ Ld(a2, MemOperand(a2, -kSystemPointerSize));
   }
+#else
+  // Push "undefined" as the receiver arg if we need to.
+  if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
+    __ PushRoot(RootIndex::kUndefinedValue);
+    __ mov(a3, a0);
+  }
+
+  // This function modifies a2, t0 and a4.
+  Generate_InterpreterPushArgs(masm, a3, a2, a4, t0);
+
+  if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
+    __ Pop(a2);                   // Pass the spread in a register
+    __ Dsubu(a0, a0, Operand(1));  // Subtract one for spread
+  }
+#endif
 
   // Call the target.
   if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
@@ -1294,6 +1379,7 @@ void Builtins::Generate_InterpreterPushArgsThenConstructImpl(
   __ daddiu(a6, a0, 1);
   __ StackOverflowCheck(a6, a5, t0, &stack_overflow);
 
+#ifdef V8_REVERSE_JSARGS
   if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
     // The spread argument should not be pushed.
     __ Dsubu(a0, a0, Operand(1));
@@ -1313,6 +1399,20 @@ void Builtins::Generate_InterpreterPushArgsThenConstructImpl(
   } else {
     __ AssertUndefinedOrAllocationSite(a2, t0);
   }
+#else
+  // Push a slot for the receiver.
+  __ push(zero_reg);
+
+  // This function modifies t0, a4 and a5.
+  Generate_InterpreterPushArgs(masm, a0, a4, a5, t0);
+
+  if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
+    __ Pop(a2);                   // Pass the spread in a register
+    __ Dsubu(a0, a0, Operand(1));  // Subtract one for spread
+  } else {
+    __ AssertUndefinedOrAllocationSite(a2, t0);
+  }
+#endif
 
   if (mode == InterpreterPushArgsMode::kArrayFunction) {
     __ AssertFunction(a1);
@@ -1473,6 +1573,7 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
   int allocatable_register_count = config->num_allocatable_general_registers();
   Register scratch = t3;
   if (with_result) {
+#ifdef V8_REVERSE_JSARGS
   if (java_script_builtin) {
     __ mov(scratch, v0);
   } else {
@@ -1483,6 +1584,15 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
               sp, config->num_allocatable_general_registers() * kPointerSize +
                       BuiltinContinuationFrameConstants::kFixedFrameSize));
   }
+#else
+    // Overwrite the hole inserted by the deoptimizer with the return value from
+    // the LAZY deopt point.
+    __ Sd(v0,
+          MemOperand(
+              sp, config->num_allocatable_general_registers() * kPointerSize +
+                      BuiltinContinuationFrameConstants::kFixedFrameSize));
+    USE(scratch);
+#endif
   }
   for (int i = allocatable_register_count - 1; i >= 0; --i) {
     int code = config->GetAllocatableGeneralCode(i);
@@ -1492,6 +1602,7 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
     }
   }
 
+#ifdef V8_REVERSE_JSARGS
   if (with_result && java_script_builtin) {
     // Overwrite the hole inserted by the deoptimizer with the return value from
     // the LAZY deopt point. t0 contains the arguments count, the return value
@@ -1504,6 +1615,7 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
     __ Dsubu(a0, a0,
             Operand(BuiltinContinuationFrameConstants::kFixedSlotCount));
   }
+#endif
 
   __ Ld(fp, MemOperand(
                 sp, BuiltinContinuationFrameConstants::kFixedFrameSizeFromFp));
@@ -1585,9 +1697,9 @@ void Builtins::Generate_InterpreterOnStackReplacement(MacroAssembler* masm) {
 void Builtins::Generate_FunctionPrototypeApply(MacroAssembler* masm) {
   // ----------- S t a t e -------------
   //  -- a0    : argc
-  //  -- sp[0] : receiver
+  //  -- sp[0] : argArray
   //  -- sp[4] : thisArg
-  //  -- sp[8] : argArray
+  //  -- sp[8] : receiver
   // -----------------------------------
 
   Register argc = a0;
@@ -1606,6 +1718,7 @@ void Builtins::Generate_FunctionPrototypeApply(MacroAssembler* masm) {
     // Claim (2 - argc) dummy arguments form the stack, to put the stack in a
     // consistent state for a simple pop operation.
 
+#ifdef V8_REVERSE_JSARGS
     __ mov(scratch, argc);
     __ Ld(this_arg, MemOperand(sp, kPointerSize));
     __ Ld(arg_array, MemOperand(sp, 2 * kPointerSize));
@@ -1616,6 +1729,18 @@ void Builtins::Generate_FunctionPrototypeApply(MacroAssembler* masm) {
     __ Ld(receiver, MemOperand(sp));
     __ Dlsa(sp, sp, argc, kSystemPointerSizeLog2);
     __ Sd(this_arg, MemOperand(sp));
+#else
+    __ Dsubu(sp, sp, Operand(2 * kPointerSize));
+    __ Dlsa(sp, sp, argc, kPointerSizeLog2);
+    __ mov(scratch, argc);
+    __ Pop(this_arg, arg_array);                   // Overwrite argc
+    __ Movz(arg_array, undefined_value, scratch);  // if argc == 0
+    __ Movz(this_arg, undefined_value, scratch);   // if argc == 0
+    __ Dsubu(scratch, scratch, Operand(1));
+    __ Movz(arg_array, undefined_value, scratch);  // if argc == 1
+    __ Ld(receiver, MemOperand(sp));
+    __ Sd(this_arg, MemOperand(sp));
+#endif
   }
 
   // ----------- S t a t e -------------
@@ -1650,6 +1775,7 @@ void Builtins::Generate_FunctionPrototypeApply(MacroAssembler* masm) {
 
 // static
 void Builtins::Generate_FunctionPrototypeCall(MacroAssembler* masm) {
+#ifdef V8_REVERSE_JSARGS
   // 1. Get the callable to call (passed as receiver) from the stack.
   {
     __ Pop(a1);
@@ -1667,6 +1793,42 @@ void Builtins::Generate_FunctionPrototypeCall(MacroAssembler* masm) {
 
   // 3. Adjust the actual number of arguments.
   __ daddiu(a0, a0, -1);
+#else
+  // 1. Make sure we have at least one argument.
+  // a0: actual number of arguments
+  {
+    Label done;
+    __ Branch(&done, ne, a0, Operand(zero_reg));
+    __ PushRoot(RootIndex::kUndefinedValue);
+    __ Daddu(a0, a0, Operand(1));
+    __ bind(&done);
+  }
+
+  // 2. Get the function to call (passed as receiver) from the stack.
+  // a0: actual number of arguments
+  __ LoadReceiver(a1, a0);
+
+  // 3. Shift arguments and return address one slot down on the stack
+  //    (overwriting the original receiver).  Adjust argument count to make
+  //    the original first argument the new receiver.
+  // a0: actual number of arguments
+  // a1: function
+  {
+    Label loop;
+    // Calculate the copy start address (destination). Copy end address is sp.
+    __ Dlsa(a2, sp, a0, kPointerSizeLog2);
+
+    __ bind(&loop);
+    __ Ld(kScratchReg, MemOperand(a2, -kPointerSize));
+    __ Sd(kScratchReg, MemOperand(a2));
+    __ Dsubu(a2, a2, Operand(kPointerSize));
+    __ Branch(&loop, ne, a2, Operand(sp));
+    // Adjust the actual number of arguments and remove the top element
+    // (which is a copy of the last argument).
+    __ Dsubu(a0, a0, Operand(1));
+    __ Pop();
+  }
+#endif
 
   // 4. Call the callable.
   __ Jump(masm->isolate()->builtins()->Call(), RelocInfo::CODE_TARGET);
@@ -1675,10 +1837,10 @@ void Builtins::Generate_FunctionPrototypeCall(MacroAssembler* masm) {
 void Builtins::Generate_ReflectApply(MacroAssembler* masm) {
   // ----------- S t a t e -------------
   //  -- a0     : argc
-  //  -- sp[0]  : receiver
-  //  -- sp[8]  : target         (if argc >= 1)
-  //  -- sp[16] : thisArgument   (if argc >= 2)
-  //  -- sp[24] : argumentsList  (if argc == 3)
+  //  -- sp[0]  : argumentsList  (if argc ==3)
+  //  -- sp[4]  : thisArgument   (if argc >=2)
+  //  -- sp[8]  : target         (if argc >=1)
+  //  -- sp[12] : receiver
   // -----------------------------------
 
   Register argc = a0;
@@ -1697,6 +1859,7 @@ void Builtins::Generate_ReflectApply(MacroAssembler* masm) {
     // Claim (3 - argc) dummy arguments form the stack, to put the stack in a
     // consistent state for a simple pop operation.
 
+#ifdef V8_REVERSE_JSARGS
     __ mov(scratch, argc);
     __ Ld(target, MemOperand(sp, kPointerSize));
     __ Ld(this_argument, MemOperand(sp, 2 * kPointerSize));
@@ -1712,6 +1875,22 @@ void Builtins::Generate_ReflectApply(MacroAssembler* masm) {
 
     __ Dlsa(sp, sp, argc, kSystemPointerSizeLog2);
     __ Sd(this_argument, MemOperand(sp, 0));  // Overwrite receiver
+#else
+    __ Dsubu(sp, sp, Operand(3 * kPointerSize));
+    __ Dlsa(sp, sp, argc, kPointerSizeLog2);
+    __ mov(scratch, argc);
+    __ Pop(target, this_argument, arguments_list);
+    __ Movz(arguments_list, undefined_value, scratch);  // if argc == 0
+    __ Movz(this_argument, undefined_value, scratch);   // if argc == 0
+    __ Movz(target, undefined_value, scratch);          // if argc == 0
+    __ Dsubu(scratch, scratch, Operand(1));
+    __ Movz(arguments_list, undefined_value, scratch);  // if argc == 1
+    __ Movz(this_argument, undefined_value, scratch);   // if argc == 1
+    __ Dsubu(scratch, scratch, Operand(1));
+    __ Movz(arguments_list, undefined_value, scratch);  // if argc == 2
+
+    __ Sd(this_argument, MemOperand(sp, 0));  // Overwrite receiver
+#endif
   }
 
   // ----------- S t a t e -------------
@@ -1733,11 +1912,12 @@ void Builtins::Generate_ReflectApply(MacroAssembler* masm) {
 void Builtins::Generate_ReflectConstruct(MacroAssembler* masm) {
   // ----------- S t a t e -------------
   //  -- a0     : argc
-  //  -- sp[0]   : receiver
-  //  -- sp[8]   : target
-  //  -- sp[16]  : argumentsList
-  //  -- sp[24]  : new.target (optional)
+  //  -- sp[0]  : new.target (optional) (dummy value if argc <= 2)
+  //  -- sp[4]  : argumentsList         (dummy value if argc <= 1)
+  //  -- sp[8]  : target                (dummy value if argc == 0)
+  //  -- sp[12] : receiver
   // -----------------------------------
+  // NOTE: The order of args in the stack are reversed if V8_REVERSE_JSARGS
 
   Register argc = a0;
   Register arguments_list = a2;
@@ -1756,6 +1936,7 @@ void Builtins::Generate_ReflectConstruct(MacroAssembler* masm) {
     // Claim (3 - argc) dummy arguments form the stack, to put the stack in a
     // consistent state for a simple pop operation.
 
+#ifdef V8_REVERSE_JSARGS
     __ mov(scratch, argc);
     __ Ld(target, MemOperand(sp, kPointerSize));
     __ Ld(arguments_list, MemOperand(sp, 2 * kPointerSize));
@@ -1771,6 +1952,22 @@ void Builtins::Generate_ReflectConstruct(MacroAssembler* masm) {
 
     __ Dlsa(sp, sp, argc, kSystemPointerSizeLog2);
     __ Sd(undefined_value, MemOperand(sp, 0));    // Overwrite receiver
+#else
+    __ Dsubu(sp, sp, Operand(3 * kPointerSize));
+    __ Dlsa(sp, sp, argc, kPointerSizeLog2);
+    __ mov(scratch, argc);
+    __ Pop(target, arguments_list, new_target);
+    __ Movz(arguments_list, undefined_value, scratch);  // if argc == 0
+    __ Movz(new_target, undefined_value, scratch);      // if argc == 0
+    __ Movz(target, undefined_value, scratch);          // if argc == 0
+    __ Dsubu(scratch, scratch, Operand(1));
+    __ Movz(arguments_list, undefined_value, scratch);  // if argc == 1
+    __ Movz(new_target, target, scratch);               // if argc == 1
+    __ Dsubu(scratch, scratch, Operand(1));
+    __ Movz(new_target, target, scratch);               // if argc == 2
+
+    __ Sd(undefined_value, MemOperand(sp, 0));  // Overwrite receiver
+#endif
   }
 
   // ----------- S t a t e -------------
@@ -1849,6 +2046,7 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
   Label stack_overflow;
   __ StackOverflowCheck(len, kScratchReg, a5, &stack_overflow);
 
+#ifdef V8_REVERSE_JSARGS
   // Move the arguments already in the stack,
   // including the receiver and the return address.
   {
@@ -1869,6 +2067,7 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
     __ Daddu(dest, dest, Operand(kSystemPointerSize));
     __ Branch(&copy, ge, t0, Operand(zero_reg));
   }
+#endif
 
   // Push arguments onto the stack (thisArgument is already on the stack).
   {
@@ -1888,9 +2087,13 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
     __ Branch(&push, ne, a5, Operand(t1));
     __ LoadRoot(a5, RootIndex::kUndefinedValue);
     __ bind(&push);
+#ifdef V8_REVERSE_JSARGS
     __ Sd(a5, MemOperand(a7, 0));
     __ Daddu(a7, a7, Operand(kSystemPointerSize));
     __ Daddu(scratch, scratch, Operand(kSystemPointerSize));
+#else
+    __ Push(a5);
+#endif
     __ Branch(&loop, ne, scratch, Operand(sp));
     __ bind(&done);
   }
@@ -1970,6 +2173,7 @@ void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
 
     // Forward the arguments from the caller frame.
 
+#ifdef V8_REVERSE_JSARGS
     // Point to the first argument to copy (skipping the receiver).
     __ Daddu(a6, a6,
              Operand(CommonFrameConstants::kFixedFrameSizeAboveFp +
@@ -1996,20 +2200,28 @@ void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
       __ Daddu(dest, dest, Operand(kSystemPointerSize));
       __ Branch(&copy, ge, t2, Operand(zero_reg));
     }
+#endif
 
     // Copy arguments from the caller frame.
     // TODO(victorgomes): Consider using forward order as potentially more cache
     // friendly.
     {
       Label loop;
+#ifndef V8_REVERSE_JSARGS
+      __ Daddu(a6, a6, Operand(CommonFrameConstants::kFixedFrameSizeAboveFp));
+#endif
       __ Daddu(a0, a0, a7);
       __ bind(&loop);
       {
         __ Subu(a7, a7, Operand(1));
         __ Dlsa(t0, a6, a7, kPointerSizeLog2);
         __ Ld(kScratchReg, MemOperand(t0));
+#ifdef V8_REVERSE_JSARGS
         __ Dlsa(t0, a2, a7, kPointerSizeLog2);
         __ Sd(kScratchReg, MemOperand(t0));
+#else
+        __ push(kScratchReg);
+#endif
         __ Branch(&loop, ne, a7, Operand(zero_reg));
       }
     }
@@ -2168,6 +2380,7 @@ void Builtins::Generate_CallBoundFunctionImpl(MacroAssembler* masm) {
     __ bind(&done);
   }
 
+#ifdef V8_REVERSE_JSARGS
   // Pop receiver.
   __ Pop(t0);
 
@@ -2189,6 +2402,41 @@ void Builtins::Generate_CallBoundFunctionImpl(MacroAssembler* masm) {
 
   // Push receiver.
   __ Push(t0);
+#else
+  __ mov(sp, t0);
+  // Relocate arguments down the stack.
+  {
+    Label loop, done_loop;
+    __ mov(a5, zero_reg);
+    __ bind(&loop);
+    __ Branch(&done_loop, gt, a5, Operand(a0));
+    __ Dlsa(a6, sp, a4, kPointerSizeLog2);
+    __ Ld(kScratchReg, MemOperand(a6));
+    __ Dlsa(a6, sp, a5, kPointerSizeLog2);
+    __ Sd(kScratchReg, MemOperand(a6));
+    __ Daddu(a4, a4, Operand(1));
+    __ Daddu(a5, a5, Operand(1));
+    __ Branch(&loop);
+    __ bind(&done_loop);
+  }
+
+  // Copy [[BoundArguments]] to the stack (below the arguments).
+  {
+    Label loop, done_loop;
+    __ SmiUntag(a4, FieldMemOperand(a2, FixedArray::kLengthOffset));
+    __ Daddu(a2, a2, Operand(FixedArray::kHeaderSize - kHeapObjectTag));
+    __ bind(&loop);
+    __ Dsubu(a4, a4, Operand(1));
+    __ Branch(&done_loop, lt, a4, Operand(zero_reg));
+    __ Dlsa(a5, a2, a4, kPointerSizeLog2);
+    __ Ld(kScratchReg, MemOperand(a5));
+    __ Dlsa(a5, sp, a0, kPointerSizeLog2);
+    __ Sd(kScratchReg, MemOperand(a5));
+    __ Daddu(a0, a0, Operand(1));
+    __ Branch(&loop);
+    __ bind(&done_loop);
+  }
+#endif
 
   // Call the [[BoundTargetFunction]] via the Call builtin.
   __ Ld(a1, FieldMemOperand(a1, JSBoundFunction::kBoundTargetFunctionOffset));
@@ -2308,6 +2556,7 @@ void Builtins::Generate_ConstructBoundFunction(MacroAssembler* masm) {
     __ bind(&done);
   }
 
+#ifdef V8_REVERSE_JSARGS
   // Pop receiver.
   __ Pop(t0);
 
@@ -2329,6 +2578,41 @@ void Builtins::Generate_ConstructBoundFunction(MacroAssembler* masm) {
 
   // Push receiver.
   __ Push(t0);
+#else
+  __ mov(sp, t0);
+  // Relocate arguments down the stack.
+  {
+    Label loop, done_loop;
+    __ mov(a5, zero_reg);
+    __ bind(&loop);
+    __ Branch(&done_loop, ge, a5, Operand(a0));
+    __ Dlsa(a6, sp, a4, kPointerSizeLog2);
+    __ Ld(kScratchReg, MemOperand(a6));
+    __ Dlsa(a6, sp, a5, kPointerSizeLog2);
+    __ Sd(kScratchReg, MemOperand(a6));
+    __ Daddu(a4, a4, Operand(1));
+    __ Daddu(a5, a5, Operand(1));
+    __ Branch(&loop);
+    __ bind(&done_loop);
+  }
+
+  // Copy [[BoundArguments]] to the stack (below the arguments).
+  {
+    Label loop, done_loop;
+    __ SmiUntag(a4, FieldMemOperand(a2, FixedArray::kLengthOffset));
+    __ Daddu(a2, a2, Operand(FixedArray::kHeaderSize - kHeapObjectTag));
+    __ bind(&loop);
+    __ Dsubu(a4, a4, Operand(1));
+    __ Branch(&done_loop, lt, a4, Operand(zero_reg));
+    __ Dlsa(a5, a2, a4, kPointerSizeLog2);
+    __ Ld(kScratchReg, MemOperand(a5));
+    __ Dlsa(a5, sp, a0, kPointerSizeLog2);
+    __ Sd(kScratchReg, MemOperand(a5));
+    __ Daddu(a0, a0, Operand(1));
+    __ Branch(&loop);
+    __ bind(&done_loop);
+  }
+#endif
 
   // Patch new.target to [[BoundTargetFunction]] if new.target equals target.
   {
@@ -2422,8 +2706,13 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
     __ StackOverflowCheck(a2, a5, kScratchReg, &stack_overflow);
 
     // Calculate copy start address into a0 and copy end address into a4.
+#ifdef V8_REVERSE_JSARGS
     __ dsll(a0, a2, kPointerSizeLog2);
     __ Daddu(a0, fp, a0);
+#else
+    __ SmiScale(a0, a0, kPointerSizeLog2);
+    __ Daddu(a0, fp, a0);
+#endif
 
     // Adjust for return address and receiver.
     __ Daddu(a0, a0, Operand(2 * kPointerSize));
@@ -2453,6 +2742,7 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
     EnterArgumentsAdaptorFrame(masm);
     __ StackOverflowCheck(a2, a5, kScratchReg, &stack_overflow);
 
+#ifdef V8_REVERSE_JSARGS
     // Fill the remaining expected arguments with undefined.
     __ LoadRoot(t0, RootIndex::kUndefinedValue);
     __ SmiUntag(t1, a0);
@@ -2483,6 +2773,51 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
 
     __ Branch(USE_DELAY_SLOT, &copy, ne, a0, Operand(fp));
     __ Dsubu(a0, a0, Operand(kSystemPointerSize));
+#else
+    // Calculate copy start address into a0 and copy end address into a7.
+    // a0: actual number of arguments as a smi
+    // a1: function
+    // a2: expected number of arguments
+    // a3: new target (passed through to callee)
+    __ SmiScale(a0, a0, kPointerSizeLog2);
+    __ Daddu(a0, fp, a0);
+    // Adjust for return address and receiver.
+    __ Daddu(a0, a0, Operand(2 * kPointerSize));
+    // Compute copy end address. Also adjust for return address.
+    __ Daddu(a7, fp, kPointerSize);
+
+    // Copy the arguments (including the receiver) to the new stack frame.
+    // a0: copy start address
+    // a1: function
+    // a2: expected number of arguments
+    // a3: new target (passed through to callee)
+    // a7: copy end address
+    Label copy;
+    __ bind(&copy);
+    __ Ld(a4, MemOperand(a0));  // Adjusted above for return addr and receiver.
+    __ Dsubu(sp, sp, kPointerSize);
+    __ Dsubu(a0, a0, kPointerSize);
+    __ Branch(USE_DELAY_SLOT, &copy, ne, a0, Operand(a7));
+    __ Sd(a4, MemOperand(sp));  // In the delay slot.
+
+    // Fill the remaining expected arguments with undefined.
+    // a1: function
+    // a2: expected number of arguments
+    // a3: new target (passed through to callee)
+    __ LoadRoot(a5, RootIndex::kUndefinedValue);
+    __ dsll(a6, a2, kPointerSizeLog2);
+    __ Dsubu(a4, fp, Operand(a6));
+    // Adjust for frame.
+    __ Dsubu(a4, a4,
+             Operand(ArgumentsAdaptorFrameConstants::kFixedFrameSizeFromFp +
+                     kPointerSize));
+
+    Label fill;
+    __ bind(&fill);
+    __ Dsubu(sp, sp, kPointerSize);
+    __ Branch(USE_DELAY_SLOT, &fill, ne, sp, Operand(a4));
+    __ Sd(a5, MemOperand(sp));
+#endif
   }
 
   // Call the entry point.
@@ -2979,10 +3314,11 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
   //  -- a2                  : arguments count (not including the receiver)
   //  -- a3                  : call data
   //  -- a0                  : holder
-  //  -- sp[0]               : receiver
-  //  -- sp[8]               : first argument
+  //  --
+  //  -- sp[0]               : last argument
   //  -- ...
-  //  -- sp[(argc) * 8]      : last argument
+  //  -- sp[(argc - 1) * 8]  : first argument
+  //  -- sp[(argc + 0) * 8]  : receiver
   // -----------------------------------
 
   Register api_function_address = a1;
@@ -3059,8 +3395,15 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
 
   // FunctionCallbackInfo::values_ (points at the first varargs argument passed
   // on the stack).
+#ifdef V8_REVERSE_JSARGS
   __ Daddu(scratch, scratch,
           Operand((FCA::kArgsLength + 1) * kSystemPointerSize));
+#else
+  __ Daddu(scratch, scratch,
+          Operand((FCA::kArgsLength - 1) * kSystemPointerSize));
+  __ dsll(t2, argc, kSystemPointerSizeLog2);
+  __ Daddu(scratch, scratch, t2);
+#endif
 
   __ Sd(scratch, MemOperand(sp, 2 * kPointerSize));
 
diff --git a/src/builtins/ppc/builtins-ppc.cc b/src/builtins/ppc/builtins-ppc.cc
index efd65e2971..f51652b5d3 100644
--- a/src/builtins/ppc/builtins-ppc.cc
+++ b/src/builtins/ppc/builtins-ppc.cc
@@ -123,6 +123,7 @@ void Generate_JSBuiltinsConstructStubHelper(MacroAssembler* masm) {
     __ Push(cp, r3);
     __ SmiUntag(r3, SetRC);
 
+#ifdef V8_REVERSE_JSARGS
     // Set up pointer to last argument (skip receiver).
     __ addi(
         r7, fp,
@@ -131,6 +132,15 @@ void Generate_JSBuiltinsConstructStubHelper(MacroAssembler* masm) {
     __ PushArray(r7, r3, r8, r0);
     // The receiver for the builtin/api call.
     __ PushRoot(RootIndex::kTheHoleValue);
+#else
+    // The receiver for the builtin/api call.
+    __ PushRoot(RootIndex::kTheHoleValue);
+    // Set up pointer to last argument.
+    __ addi(r7, fp, Operand(StandardFrameConstants::kCallerSPOffset));
+
+    // Copy arguments and receiver to the expression stack.
+    __ PushArray(r7, r3, r8, r0);
+#endif
 
     // Call the function.
     // r3: number of arguments (untagged)
@@ -230,6 +240,7 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
     // Restore new target.
     __ Pop(r6);
 
+#ifdef V8_REVERSE_JSARGS
     // Push the allocated receiver to the stack.
     __ Push(r3);
     // We need two copies because we may have to return the original one
@@ -243,6 +254,15 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
     __ addi(
         r7, fp,
         Operand(StandardFrameConstants::kCallerSPOffset + kSystemPointerSize));
+#else
+    // Push the allocated receiver to the stack. We need two copies
+    // because we may have to return the original one and the calling
+    // conventions dictate that the called function pops the receiver.
+    __ Push(r3, r3);
+
+    // Set up pointer to last argument.
+    __ addi(r7, fp, Operand(StandardFrameConstants::kCallerSPOffset));
+#endif
 
     // ----------- S t a t e -------------
     //  --                 r6: new target
@@ -275,8 +295,10 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
     // Copy arguments and receiver to the expression stack.
     __ PushArray(r7, r3, r8, r0);
 
+#ifdef V8_REVERSE_JSARGS
     // Push implicit receiver.
     __ Push(r9);
+#endif
 
     // Call the function.
     {
@@ -413,11 +435,19 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
   __ cmpl(sp, scratch);
   __ blt(&stack_overflow);
 
+#ifndef V8_REVERSE_JSARGS
+  // Push receiver.
+  __ LoadTaggedPointerField(
+      scratch, FieldMemOperand(r4, JSGeneratorObject::kReceiverOffset));
+  __ Push(scratch);
+#endif
+
   // ----------- S t a t e -------------
   //  -- r4    : the JSGeneratorObject to resume
   //  -- r7    : generator function
   //  -- cp    : generator context
   //  -- lr    : return address
+  //  -- sp[0] : generator receiver
   // -----------------------------------
 
   // Copy the function arguments from the generator object's register file.
@@ -429,7 +459,9 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
       r5,
       FieldMemOperand(r4, JSGeneratorObject::kParametersAndRegistersOffset));
   {
+#ifdef V8_REVERSE_JSARGS
     Label done_loop, loop;
+
     __ mr(r9, r6);
 
     __ bind(&loop);
@@ -449,6 +481,24 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
     __ LoadAnyTaggedField(
         scratch, FieldMemOperand(r4, JSGeneratorObject::kReceiverOffset));
     __ Push(scratch);
+#else
+    Label loop, done_loop;
+    __ cmpi(r6, Operand::Zero());
+    __ ble(&done_loop);
+
+    // setup r9 to first element address - kTaggedSize
+    __ addi(r9, r5,
+            Operand(FixedArray::kHeaderSize - kHeapObjectTag - kTaggedSize));
+
+    __ mtctr(r6);
+    __ bind(&loop);
+    __ LoadAnyTaggedField(scratch, MemOperand(r9, kTaggedSize));
+    __ addi(r9, r9, Operand(kTaggedSize));
+    __ push(scratch);
+    __ bdnz(&loop);
+
+    __ bind(&done_loop);
+#endif
   }
 
   // Underlying function needs to have bytecode available.
@@ -758,6 +808,7 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
     // r4: function
     // r7: argc
     // r8: argv, i.e. points to first arg
+#ifdef V8_REVERSE_JSARGS
     Label loop, done;
     __ cmpi(r7, Operand::Zero());
     __ beq(&done);
@@ -776,6 +827,24 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
     // Push the receiver.
     __ Push(r6);
 
+#else
+    // Push the receiver.
+    __ Push(r6);
+
+    Label loop, done;
+    __ cmpi(r7, Operand::Zero());
+    __ beq(&done);
+
+    __ mtctr(r7);
+    __ subi(r8, r8, Operand(kSystemPointerSize));
+    __ bind(&loop);
+    __ LoadPU(r9, MemOperand(r8, kSystemPointerSize));  // read next parameter
+    __ LoadP(r0, MemOperand(r9));  // dereference handle
+    __ push(r0);                   // push parameter
+    __ bdnz(&loop);
+    __ bind(&done);
+#endif
+
     // r3: argc
     // r4: function
     // r6: new.target
@@ -1285,8 +1354,12 @@ static void Generate_InterpreterPushArgs(MacroAssembler* masm,
   __ ShiftLeftImm(scratch, scratch, Operand(kSystemPointerSizeLog2));
   __ sub(start_address, start_address, scratch);
   // Push the arguments.
+#ifdef V8_REVERSE_JSARGS
   __ PushArray(start_address, num_args, scratch, r0,
                TurboAssembler::PushArrayOrder::kReverse);
+#else
+  __ PushArray(start_address, num_args, scratch, r0);
+#endif
 }
 
 // static
@@ -1303,15 +1376,19 @@ void Builtins::Generate_InterpreterPushArgsThenCallImpl(
   // -----------------------------------
   Label stack_overflow;
 
+#ifdef V8_REVERSE_JSARGS
   if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
     // The spread argument should not be pushed.
     __ subi(r3, r3, Operand(1));
   }
+#endif
 
   // Calculate number of arguments (add one for receiver).
   __ addi(r6, r3, Operand(1));
+
   Generate_StackOverflowCheck(masm, r6, ip, &stack_overflow);
 
+#ifdef V8_REVERSE_JSARGS
   if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
     // Don't copy receiver. Argument count is correct.
     __ mr(r6, r3);
@@ -1330,6 +1407,21 @@ void Builtins::Generate_InterpreterPushArgsThenCallImpl(
     // lies in the next interpreter register.
     __ LoadP(r5, MemOperand(r5, -kSystemPointerSize));
   }
+#else
+  // Push "undefined" as the receiver arg if we need to.
+  if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
+    __ PushRoot(RootIndex::kUndefinedValue);
+    __ mr(r6, r3);  // Argument count is correct.
+  }
+
+  // Push the arguments. r5, r6, r7 will be modified.
+  Generate_InterpreterPushArgs(masm, r6, r5, r7);
+
+  if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
+    __ Pop(r5);                   // Pass the spread in a register
+    __ subi(r3, r3, Operand(1));  // Subtract one for spread
+  }
+#endif
 
   // Call the target.
   if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
@@ -1362,6 +1454,7 @@ void Builtins::Generate_InterpreterPushArgsThenConstructImpl(
   __ addi(r8, r3, Operand(1));
   Generate_StackOverflowCheck(masm, r8, ip, &stack_overflow);
 
+#ifdef V8_REVERSE_JSARGS
   if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
     // The spread argument should not be pushed.
     __ subi(r3, r3, Operand(1));
@@ -1383,6 +1476,22 @@ void Builtins::Generate_InterpreterPushArgsThenConstructImpl(
   } else {
     __ AssertUndefinedOrAllocationSite(r5, r8);
   }
+#else
+
+  // Push a slot for the receiver to be constructed.
+  __ li(r0, Operand::Zero());
+  __ push(r0);
+
+  // Push the arguments (skip if none).
+  Generate_InterpreterPushArgs(masm, r3, r7, r8);
+  if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
+    __ Pop(r5);                   // Pass the spread in a register
+    __ subi(r3, r3, Operand(1));  // Subtract one for spread
+  } else {
+    __ AssertUndefinedOrAllocationSite(r5, r8);
+  }
+
+#endif
 
   if (mode == InterpreterPushArgsMode::kArrayFunction) {
     __ AssertFunction(r4);
@@ -1551,6 +1660,7 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
   int allocatable_register_count = config->num_allocatable_general_registers();
   Register scratch = ip;
   if (with_result) {
+#ifdef V8_REVERSE_JSARGS
     if (java_script_builtin) {
       __ mr(scratch, r3);
     } else {
@@ -1562,6 +1672,16 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
                               kSystemPointerSize +
                           BuiltinContinuationFrameConstants::kFixedFrameSize));
     }
+#else
+    // Overwrite the hole inserted by the deoptimizer with the return value from
+    // the LAZY deopt point.
+    __ StoreP(
+        r3,
+        MemOperand(sp, config->num_allocatable_general_registers() *
+                               kSystemPointerSize +
+                           BuiltinContinuationFrameConstants::kFixedFrameSize));
+    USE(scratch);
+#endif
   }
   for (int i = allocatable_register_count - 1; i >= 0; --i) {
     int code = config->GetAllocatableGeneralCode(i);
@@ -1570,6 +1690,7 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
       __ SmiUntag(Register::from_code(code));
     }
   }
+#ifdef V8_REVERSE_JSARGS
   if (java_script_builtin && with_result) {
     // Overwrite the hole inserted by the deoptimizer with the return value from
     // the LAZY deopt point. r0 contains the arguments count, the return value
@@ -1582,6 +1703,7 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
     __ subi(r3, r3,
             Operand(BuiltinContinuationFrameConstants::kFixedSlotCount));
   }
+#endif
   __ LoadP(
       fp,
       MemOperand(sp, BuiltinContinuationFrameConstants::kFixedFrameSizeFromFp));
@@ -1679,9 +1801,9 @@ void Builtins::Generate_InterpreterOnStackReplacement(MacroAssembler* masm) {
 void Builtins::Generate_FunctionPrototypeApply(MacroAssembler* masm) {
   // ----------- S t a t e -------------
   //  -- r3    : argc
-  //  -- sp[0] : receiver
+  //  -- sp[0] : argArray
   //  -- sp[4] : thisArg
-  //  -- sp[8] : argArray
+  //  -- sp[8] : receiver
   // -----------------------------------
 
   // 1. Load receiver into r4, argArray into r5 (if present), remove all
@@ -1691,7 +1813,9 @@ void Builtins::Generate_FunctionPrototypeApply(MacroAssembler* masm) {
     __ LoadRoot(r8, RootIndex::kUndefinedValue);
     __ mr(r5, r8);
 
+#ifdef V8_REVERSE_JSARGS
     Label done;
+
     __ LoadP(r4, MemOperand(sp));  // receiver
     __ cmpi(r3, Operand(1));
     __ blt(&done);
@@ -1701,6 +1825,24 @@ void Builtins::Generate_FunctionPrototypeApply(MacroAssembler* masm) {
     __ LoadP(r5, MemOperand(sp, 2 * kSystemPointerSize));  // argArray
 
     __ bind(&done);
+#else
+    Label done;
+    __ ShiftLeftImm(r4, r3, Operand(kSystemPointerSizeLog2));
+    __ LoadPX(r4, MemOperand(sp, r4));  // receiver
+
+    __ li(r0, Operand(1));
+    __ sub(r7, r3, r0, LeaveOE, SetRC);
+    __ blt(&done, cr0);
+    __ ShiftLeftImm(r8, r7, Operand(kSystemPointerSizeLog2));
+    __ LoadPX(r8, MemOperand(sp, r8));
+
+    __ sub(r7, r7, r0, LeaveOE, SetRC);
+    __ blt(&done, cr0);
+    __ ShiftLeftImm(r5, r7, Operand(kSystemPointerSizeLog2));
+    __ LoadPX(r5, MemOperand(sp, r5));
+
+    __ bind(&done);
+#endif
     __ ShiftLeftImm(ip, r3, Operand(kSystemPointerSizeLog2));
     __ add(sp, sp, ip);
     __ StoreP(r8, MemOperand(sp));
@@ -1736,6 +1878,7 @@ void Builtins::Generate_FunctionPrototypeApply(MacroAssembler* masm) {
 
 // static
 void Builtins::Generate_FunctionPrototypeCall(MacroAssembler* masm) {
+#ifdef V8_REVERSE_JSARGS
   // 1. Get the callable to call (passed as receiver) from the stack.
   __ Pop(r4);
 
@@ -1752,6 +1895,46 @@ void Builtins::Generate_FunctionPrototypeCall(MacroAssembler* masm) {
 
   // 3. Adjust the actual number of arguments.
   __ subi(r3, r3, Operand(1));
+#else
+  // 1. Make sure we have at least one argument.
+  // r3: actual number of arguments
+  {
+    Label done;
+    __ cmpi(r3, Operand::Zero());
+    __ bne(&done);
+    __ PushRoot(RootIndex::kUndefinedValue);
+    __ addi(r3, r3, Operand(1));
+    __ bind(&done);
+  }
+
+  // 2. Get the callable to call (passed as receiver) from the stack.
+  // r3: actual number of arguments
+  __ LoadReceiver(r4, r3);
+
+  // 3. Shift arguments and return address one slot down on the stack
+  //    (overwriting the original receiver).  Adjust argument count to make
+  //    the original first argument the new receiver.
+  // r3: actual number of arguments
+  // r4: callable
+  {
+    Register scratch = r6;
+    Label loop;
+    // Calculate the copy start address (destination). Copy end address is sp.
+    __ ShiftLeftImm(r5, r3, Operand(kSystemPointerSizeLog2));
+    __ add(r5, sp, r5);
+
+    __ mtctr(r3);
+    __ bind(&loop);
+    __ LoadP(scratch, MemOperand(r5, -kSystemPointerSize));
+    __ StoreP(scratch, MemOperand(r5));
+    __ subi(r5, r5, Operand(kSystemPointerSize));
+    __ bdnz(&loop);
+    // Adjust the actual number of arguments and remove the top element
+    // (which is a copy of the last argument).
+    __ subi(r3, r3, Operand(1));
+    __ pop();
+  }
+#endif
 
   // 4. Call the callable.
   __ Jump(masm->isolate()->builtins()->Call(), RelocInfo::CODE_TARGET);
@@ -1760,10 +1943,10 @@ void Builtins::Generate_FunctionPrototypeCall(MacroAssembler* masm) {
 void Builtins::Generate_ReflectApply(MacroAssembler* masm) {
   // ----------- S t a t e -------------
   //  -- r3     : argc
-  //  -- sp[0]  : receiver
-  //  -- sp[4]  : target         (if argc >= 1)
-  //  -- sp[8]  : thisArgument   (if argc >= 2)
-  //  -- sp[12] : argumentsList  (if argc == 3)
+  //  -- sp[0]  : argumentsList
+  //  -- sp[4]  : thisArgument
+  //  -- sp[8]  : target
+  //  -- sp[12] : receiver
   // -----------------------------------
 
   // 1. Load target into r4 (if present), argumentsList into r5 (if present),
@@ -1774,7 +1957,9 @@ void Builtins::Generate_ReflectApply(MacroAssembler* masm) {
     __ mr(r8, r4);
     __ mr(r5, r4);
 
+#ifdef V8_REVERSE_JSARGS
     Label done;
+
     __ cmpi(r3, Operand(1));
     __ blt(&done);
     __ LoadP(r4, MemOperand(sp, kSystemPointerSize));  // thisArg
@@ -1786,6 +1971,26 @@ void Builtins::Generate_ReflectApply(MacroAssembler* masm) {
     __ LoadP(r5, MemOperand(sp, 3 * kSystemPointerSize));  // argArray
 
     __ bind(&done);
+#else
+    Label done;
+    __ li(r0, Operand(1));
+    __ sub(r7, r3, r0, LeaveOE, SetRC);
+    __ blt(&done, cr0);
+    __ ShiftLeftImm(r4, r7, Operand(kSystemPointerSizeLog2));
+    __ LoadPX(r4, MemOperand(sp, r4));  // receiver
+
+    __ sub(r7, r7, r0, LeaveOE, SetRC);
+    __ blt(&done, cr0);
+    __ ShiftLeftImm(r8, r7, Operand(kSystemPointerSizeLog2));
+    __ LoadPX(r8, MemOperand(sp, r8));
+
+    __ sub(r7, r7, r0, LeaveOE, SetRC);
+    __ blt(&done, cr0);
+    __ ShiftLeftImm(r5, r7, Operand(kSystemPointerSizeLog2));
+    __ LoadPX(r5, MemOperand(sp, r5));
+
+    __ bind(&done);
+#endif
     __ ShiftLeftImm(ip, r3, Operand(kSystemPointerSizeLog2));
     __ add(sp, sp, ip);
     __ StoreP(r8, MemOperand(sp));
@@ -1809,11 +2014,12 @@ void Builtins::Generate_ReflectApply(MacroAssembler* masm) {
 void Builtins::Generate_ReflectConstruct(MacroAssembler* masm) {
   // ----------- S t a t e -------------
   //  -- r3     : argc
-  //  -- sp[0]  : receiver
-  //  -- sp[4]  : target
-  //  -- sp[8]  : argumentsList
-  //  -- sp[12] : new.target (optional)
+  //  -- sp[0]  : new.target (optional)
+  //  -- sp[4]  : argumentsList
+  //  -- sp[8]  : target
+  //  -- sp[12] : receiver
   // -----------------------------------
+  // NOTE: The order of args in the stack are reversed if V8_REVERSE_JSARGS
 
   // 1. Load target into r4 (if present), argumentsList into r5 (if present),
   // new.target into r6 (if present, otherwise use target), remove all
@@ -1823,7 +2029,9 @@ void Builtins::Generate_ReflectConstruct(MacroAssembler* masm) {
     __ LoadRoot(r4, RootIndex::kUndefinedValue);
     __ mr(r5, r4);
 
+#ifdef V8_REVERSE_JSARGS
     Label done;
+
     __ mr(r7, r4);
     __ cmpi(r3, Operand(1));
     __ blt(&done);
@@ -1839,6 +2047,31 @@ void Builtins::Generate_ReflectConstruct(MacroAssembler* masm) {
     __ ShiftLeftImm(r0, r3, Operand(kSystemPointerSizeLog2));
     __ add(sp, sp, r0);
     __ StoreP(r7, MemOperand(sp));
+#else
+    Label done;
+    __ ShiftLeftImm(ip, r3, Operand(kSystemPointerSizeLog2));
+    __ StorePX(r5, MemOperand(sp, ip));
+    __ li(r0, Operand(1));
+    __ sub(r7, r3, r0, LeaveOE, SetRC);
+    __ blt(&done, cr0);
+    __ ShiftLeftImm(r4, r7, Operand(kSystemPointerSizeLog2));
+    __ LoadPX(r4, MemOperand(sp, r4));  // receiver
+
+    __ mr(r6, r4);
+    __ sub(r7, r7, r0, LeaveOE, SetRC);
+    __ blt(&done, cr0);
+    __ ShiftLeftImm(r5, r7, Operand(kSystemPointerSizeLog2));
+    __ LoadPX(r5, MemOperand(sp, r5));
+
+    __ sub(r7, r7, r0, LeaveOE, SetRC);
+    __ blt(&done, cr0);
+    __ ShiftLeftImm(r6, r7, Operand(kSystemPointerSizeLog2));
+    __ LoadPX(r6, MemOperand(sp, r6));
+
+    __ bind(&done);
+    __ ShiftLeftImm(r0, r3, Operand(kSystemPointerSizeLog2));
+    __ add(sp, sp, r0);
+#endif
   }
 
   // ----------- S t a t e -------------
@@ -1927,6 +2160,7 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
   Label stack_overflow;
   Generate_StackOverflowCheck(masm, r7, scratch, &stack_overflow);
 
+#ifdef V8_REVERSE_JSARGS
   // Move the arguments already in the stack,
   // including the receiver and the return address.
   {
@@ -1945,6 +2179,7 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
     __ StorePU(r0, MemOperand(dest, kSystemPointerSize));
     __ bdnz(&copy);
   }
+#endif
 
   // Push arguments onto the stack (thisArgument is already on the stack).
   {
@@ -1961,7 +2196,11 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
     __ bne(&skip);
     __ LoadRoot(scratch, RootIndex::kUndefinedValue);
     __ bind(&skip);
+#ifdef V8_REVERSE_JSARGS
     __ StorePU(scratch, MemOperand(r8, kSystemPointerSize));
+#else
+    __ push(scratch);
+#endif
     __ bdnz(&loop);
     __ bind(&no_args);
     __ add(r3, r3, r7);
@@ -2049,6 +2288,7 @@ void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
     Generate_StackOverflowCheck(masm, r8, scratch, &stack_overflow);
 
     // Forward the arguments from the caller frame.
+#ifdef V8_REVERSE_JSARGS
     // Point to the first argument to copy (skipping the receiver).
     __ addi(r7, r7,
             Operand(CommonFrameConstants::kFixedFrameSizeAboveFp +
@@ -2074,11 +2314,15 @@ void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
       __ StorePU(r0, MemOperand(dest, kSystemPointerSize));
       __ bdnz(&copy);
     }
+#endif
     // Copy arguments from the caller frame.
     // TODO(victorgomes): Consider using forward order as potentially more cache
     // friendly.
     {
       Label loop;
+#ifndef V8_REVERSE_JSARGS
+      __ addi(r7, r7, Operand(CommonFrameConstants::kFixedFrameSizeAboveFp));
+#endif
       __ add(r3, r3, r8);
       __ addi(r5, r5, Operand(kSystemPointerSize));
       __ bind(&loop);
@@ -2086,7 +2330,11 @@ void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
         __ subi(r8, r8, Operand(1));
         __ ShiftLeftImm(scratch, r8, Operand(kSystemPointerSizeLog2));
         __ LoadPX(r0, MemOperand(r7, scratch));
+#ifdef V8_REVERSE_JSARGS
         __ StorePX(r0, MemOperand(r5, scratch));
+#else
+        __ push(r0);
+#endif
         __ cmpi(r8, Operand::Zero());
         __ bne(&loop);
       }
@@ -2250,6 +2498,7 @@ void Generate_PushBoundArguments(MacroAssembler* masm) {
       __ bind(&done);
     }
 
+#ifdef V8_REVERSE_JSARGS
     // Pop receiver.
     __ Pop(r8);
 
@@ -2272,6 +2521,44 @@ void Generate_PushBoundArguments(MacroAssembler* masm) {
 
     // Push receiver.
     __ Push(r8);
+#else
+    __ mr(scratch, sp);
+    __ mr(sp, r0);
+
+    // Relocate arguments down the stack.
+    //  -- r3 : the number of arguments (not including the receiver)
+    //  -- r9 : the previous stack pointer
+    //  -- r10: the size of the [[BoundArguments]]
+    {
+      Label skip, loop;
+      __ li(r8, Operand::Zero());
+      __ cmpi(r3, Operand::Zero());
+      __ beq(&skip);
+      __ mtctr(r3);
+      __ bind(&loop);
+      __ LoadPX(r0, MemOperand(scratch, r8));
+      __ StorePX(r0, MemOperand(sp, r8));
+      __ addi(r8, r8, Operand(kSystemPointerSize));
+      __ bdnz(&loop);
+      __ bind(&skip);
+    }
+
+    // Copy [[BoundArguments]] to the stack (below the arguments).
+    {
+      Label loop;
+      __ ShiftLeftImm(r10, r7, Operand(kTaggedSizeLog2));
+      __ addi(r10, r10, Operand(FixedArray::kHeaderSize - kHeapObjectTag));
+      __ add(r5, r5, r10);
+      __ mtctr(r7);
+      __ bind(&loop);
+      __ LoadAnyTaggedField(ip, MemOperand(r5, -kTaggedSize), r0);
+      __ StorePX(ip, MemOperand(sp, r8));
+      __ addi(r8, r8, Operand(kSystemPointerSize));
+      __ addi(r5, r5, Operand(-kTaggedSize));
+      __ bdnz(&loop);
+      __ add(r3, r3, r7);
+    }
+#endif
   }
   __ bind(&no_bound_arguments);
 }
@@ -2467,12 +2754,17 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
   //  -- r6 : new target (passed through to callee)
   // -----------------------------------
 
-  Label dont_adapt_arguments, stack_overflow;
+  Label dont_adapt_arguments, stack_overflow, skip_adapt_arguments;
   __ cmpli(r5, Operand(kDontAdaptArgumentsSentinel));
   __ beq(&dont_adapt_arguments);
   __ LoadTaggedPointerField(
       r7, FieldMemOperand(r4, JSFunction::kSharedFunctionInfoOffset));
   __ lwz(r7, FieldMemOperand(r7, SharedFunctionInfo::kFlagsOffset));
+#ifndef V8_REVERSE_JSARGS
+  __ TestBitMask(r7, SharedFunctionInfo::IsSafeToSkipArgumentsAdaptorBit::kMask,
+                 r0);
+  __ bne(&skip_adapt_arguments, cr0);
+#endif
 
   // -------------------------------------------
   // Adapt arguments.
@@ -2493,8 +2785,13 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
       // r4: function
       // r5: expected number of arguments
       // r6: new target (passed through to callee)
+#ifdef V8_REVERSE_JSARGS
       __ ShiftLeftImm(r3, r5, Operand(kSystemPointerSizeLog2));
       __ add(r3, r3, fp);
+#else
+      __ SmiToPtrArrayOffset(r3, r3);
+      __ add(r3, r3, fp);
+#endif
       // adjust for return address and receiver
       __ addi(r3, r3, Operand(2 * kSystemPointerSize));
       __ ShiftLeftImm(r7, r5, Operand(kSystemPointerSizeLog2));
@@ -2524,6 +2821,7 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
       EnterArgumentsAdaptorFrame(masm);
       Generate_StackOverflowCheck(masm, r5, r8, &stack_overflow);
 
+#ifdef V8_REVERSE_JSARGS
       // Fill the remaining expected arguments with undefined.
       // r0: actual number of arguments as a smi
       // r1: function
@@ -2568,6 +2866,47 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
       __ cmp(r3, fp);  // Compare before moving to next argument.
       __ subi(r3, r3, Operand(kSystemPointerSize));
       __ b(ne, &copy);
+#else
+      // Calculate copy start address into r0 and copy end address is fp.
+      // r3: actual number of arguments as a smi
+      // r4: function
+      // r5: expected number of arguments
+      // r6: new target (passed through to callee)
+      __ SmiToPtrArrayOffset(r3, r3);
+      __ add(r3, r3, fp);
+
+      // Copy the arguments (including the receiver) to the new stack frame.
+      // r3: copy start address
+      // r4: function
+      // r5: expected number of arguments
+      // r6: new target (passed through to callee)
+      Label copy;
+      __ bind(&copy);
+      // Adjust load for return address and receiver.
+      __ LoadP(r0, MemOperand(r3, 2 * kSystemPointerSize));
+      __ push(r0);
+      __ cmp(r3, fp);  // Compare before moving to next argument.
+      __ subi(r3, r3, Operand(kSystemPointerSize));
+      __ bne(&copy);
+
+      // Fill the remaining expected arguments with undefined.
+      // r4: function
+      // r5: expected number of arguments
+      // r6: new target (passed through to callee)
+      __ LoadRoot(r0, RootIndex::kUndefinedValue);
+      __ ShiftLeftImm(r7, r5, Operand(kSystemPointerSizeLog2));
+      __ sub(r7, fp, r7);
+      // Adjust for frame.
+      __ subi(r7, r7,
+              Operand(ArgumentsAdaptorFrameConstants::kFixedFrameSizeFromFp +
+                      kSystemPointerSize));
+
+      Label fill;
+      __ bind(&fill);
+      __ push(r0);
+      __ cmp(sp, r7);
+      __ bne(&fill);
+#endif
     }
 
     // Call the entry point.
@@ -2589,6 +2928,42 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
     __ blr();
   }
 
+  // -------------------------------------------
+  // Skip adapt arguments.
+  // -------------------------------------------
+  __ bind(&skip_adapt_arguments);
+  {
+    // The callee cannot observe the actual arguments, so it's safe to just
+    // pass the expected arguments by massaging the stack appropriately. See
+    // http://bit.ly/v8-faster-calls-with-arguments-mismatch for details.
+    Label under_application, over_application;
+    __ cmp(r3, r5);
+    __ blt(&under_application);
+
+    __ bind(&over_application);
+    {
+      // Remove superfluous parameters from the stack.
+      __ sub(r7, r3, r5);
+      __ mr(r3, r5);
+      __ ShiftLeftImm(r7, r7, Operand(kSystemPointerSizeLog2));
+      __ add(sp, sp, r7);
+      __ b(&dont_adapt_arguments);
+    }
+
+    __ bind(&under_application);
+    {
+      // Fill remaining expected arguments with undefined values.
+      Label fill;
+      __ LoadRoot(r7, RootIndex::kUndefinedValue);
+      __ bind(&fill);
+      __ addi(r3, r3, Operand(1));
+      __ push(r7);
+      __ cmp(r3, r5);
+      __ blt(&fill);
+      __ b(&dont_adapt_arguments);
+    }
+  }
+
   // -------------------------------------------
   // Dont adapt arguments.
   // -------------------------------------------
@@ -3094,11 +3469,12 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
   //  -- r5                  : arguments count (not including the receiver)
   //  -- r6                  : call data
   //  -- r3                  : holder
-  //  -- sp[0]               : receiver
-  //  -- sp[8]               : first argument
+  //  -- sp[0]               : last argument
   //  -- ...
-  //  -- sp[(argc) * 8]      : last argument
+  //  -- sp[(argc - 1)* 4]   : first argument
+  //  -- sp[(argc + 0) * 4]  : receiver
   // -----------------------------------
+  // NOTE: The order of args are reversed if V8_REVERSE_JSARGS
 
   Register api_function_address = r4;
   Register argc = r5;
@@ -3173,8 +3549,15 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
 
   // FunctionCallbackInfo::values_ (points at the first varargs argument passed
   // on the stack).
+#ifdef V8_REVERSE_JSARGS
   __ addi(scratch, scratch,
           Operand((FCA::kArgsLength + 1) * kSystemPointerSize));
+#else
+  __ addi(scratch, scratch,
+          Operand((FCA::kArgsLength - 1) * kSystemPointerSize));
+  __ ShiftLeftImm(ip, argc, Operand(kSystemPointerSizeLog2));
+  __ add(scratch, scratch, ip);
+#endif
   __ StoreP(scratch, MemOperand(sp, (kStackFrameExtraParamSlot + 2) *
                                         kSystemPointerSize));
 
diff --git a/src/builtins/s390/builtins-s390.cc b/src/builtins/s390/builtins-s390.cc
index 8cc3a949c3..8d3049a81c 100644
--- a/src/builtins/s390/builtins-s390.cc
+++ b/src/builtins/s390/builtins-s390.cc
@@ -122,6 +122,7 @@ void Generate_JSBuiltinsConstructStubHelper(MacroAssembler* masm) {
     __ Push(cp, r2);
     __ SmiUntag(r2);
 
+#ifdef V8_REVERSE_JSARGS
     // Set up pointer to last argument (skip receiver).
     __ la(r6, MemOperand(fp, StandardFrameConstants::kCallerSPOffset +
                                  kSystemPointerSize));
@@ -129,6 +130,15 @@ void Generate_JSBuiltinsConstructStubHelper(MacroAssembler* masm) {
     __ PushArray(r6, r2, r1, r0);
     // The receiver for the builtin/api call.
     __ PushRoot(RootIndex::kTheHoleValue);
+#else
+    // The receiver for the builtin/api call.
+    __ PushRoot(RootIndex::kTheHoleValue);
+    // Set up pointer to last argument.
+    __ la(r6, MemOperand(fp, StandardFrameConstants::kCallerSPOffset));
+
+    // Copy arguments and receiver to the expression stack.
+    __ PushArray(r6, r2, r1, r0);
+#endif
 
     // Call the function.
     // r2: number of arguments
@@ -226,6 +236,7 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
     // Restore new target.
     __ Pop(r5);
 
+#ifdef V8_REVERSE_JSARGS
     // Push the allocated receiver to the stack.
     __ Push(r2);
     // We need two copies because we may have to return the original one
@@ -238,6 +249,15 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
     // Set up pointer to first argument (skip receiver).
     __ la(r6, MemOperand(fp, StandardFrameConstants::kCallerSPOffset +
                                  kSystemPointerSize));
+#else
+    // Push the allocated receiver to the stack. We need two copies
+    // because we may have to return the original one and the calling
+    // conventions dictate that the called function pops the receiver.
+    __ Push(r2, r2);
+
+    // Set up pointer to last argument.
+    __ la(r6, MemOperand(fp, StandardFrameConstants::kCallerSPOffset));
+#endif
 
     // ----------- S t a t e -------------
     //  --                 r5: new target
@@ -270,8 +290,10 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
     // Copy arguments and receiver to the expression stack.
     __ PushArray(r6, r2, r1, r0);
 
+#ifdef V8_REVERSE_JSARGS
     // Push implicit receiver.
     __ Push(r8);
+#endif
 
     // Call the function.
     __ InvokeFunctionWithNewTarget(r3, r5, r2, CALL_FUNCTION);
@@ -406,11 +428,19 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
   __ CmpLogicalP(sp, scratch);
   __ blt(&stack_overflow);
 
+#ifndef V8_REVERSE_JSARGS
+  // Push receiver.
+  __ LoadTaggedPointerField(
+      scratch, FieldMemOperand(r3, JSGeneratorObject::kReceiverOffset));
+  __ Push(scratch);
+#endif
+
   // ----------- S t a t e -------------
   //  -- r3    : the JSGeneratorObject to resume
   //  -- r6    : generator function
   //  -- cp    : generator context
   //  -- lr    : return address
+  //  -- sp[0] : generator receiver
   // -----------------------------------
 
   // Copy the function arguments from the generator object's register file.
@@ -422,6 +452,7 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
       r4,
       FieldMemOperand(r3, JSGeneratorObject::kParametersAndRegistersOffset));
   {
+#ifdef V8_REVERSE_JSARGS
     Label done_loop, loop;
     __ LoadRR(r8, r5);
 
@@ -441,6 +472,34 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
     __ LoadAnyTaggedField(
         scratch, FieldMemOperand(r3, JSGeneratorObject::kReceiverOffset));
     __ Push(scratch);
+#else
+    Label loop, done_loop;
+    __ ShiftLeftP(r1, r5, Operand(kSystemPointerSizeLog2));
+    __ SubP(sp, r1);
+
+    __ ShiftLeftP(r5, r5, Operand(kTaggedSizeLog2));
+
+    // ip = stack offset
+    // r5 = parameter array offset
+    __ LoadImmP(ip, Operand::Zero());
+    __ SubP(r5, Operand(kTaggedSize));
+    __ blt(&done_loop);
+
+    __ lghi(r1, Operand(-kTaggedSize));
+
+    __ bind(&loop);
+
+    // parameter copy loop
+    __ LoadAnyTaggedField(r0, FieldMemOperand(r4, r5, FixedArray::kHeaderSize));
+    __ StoreP(r0, MemOperand(sp, ip));
+
+    // update offsets
+    __ lay(ip, MemOperand(ip, kSystemPointerSize));
+
+    __ BranchRelativeOnIdxHighP(r5, r1, &loop);
+
+    __ bind(&done_loop);
+#endif
   }
 
   // Underlying function needs to have bytecode available.
@@ -812,6 +871,8 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
     // r7: scratch reg to hold scaled argc
     // r8: scratch reg to hold arg handle
     // r9: scratch reg to hold index into argv
+
+#ifdef V8_REVERSE_JSARGS
     Label argLoop, argExit;
 
     __ ShiftLeftP(r9, r2, Operand(kSystemPointerSizeLog2));
@@ -834,6 +895,28 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
     // Push the receiver.
     __ Push(r5);
 
+#else
+    // Push the receiver.
+    __ Push(r5);
+
+    Label argLoop, argExit;
+
+    __ LoadRR(r9, r6);
+    __ ltgr(r7, r2);
+    __ beq(&argExit, Label::kNear);
+    __ bind(&argLoop);
+
+    __ LoadP(r8, MemOperand(r9));             // read next parameter
+    __ LoadP(r0, MemOperand(r8));             // dereference handle
+    __ Push(r0);
+    __ la(r9, MemOperand(r9, kSystemPointerSize));  // r9++;
+    // __ lay(r7, MemOperand(r7, -kSystemPointerSize));
+    __ SubP(r7, r7, Operand(1));
+    __ bgt(&argLoop);
+
+    __ bind(&argExit);
+#endif
+
     // Setup new.target, argc and function.
     __ LoadRR(r5, r3);
     __ LoadRR(r3, r4);
@@ -1339,8 +1422,12 @@ static void Generate_InterpreterPushArgs(MacroAssembler* masm,
   __ ShiftLeftP(scratch, scratch, Operand(kSystemPointerSizeLog2));
   __ SubP(start_address, start_address, scratch);
   // Push the arguments.
+#ifdef V8_REVERSE_JSARGS
   __ PushArray(start_address, num_args, r1, scratch,
                TurboAssembler::PushArrayOrder::kReverse);
+#else
+  __ PushArray(start_address, num_args, r1, scratch);
+#endif
 }
 
 // static
@@ -1356,15 +1443,19 @@ void Builtins::Generate_InterpreterPushArgsThenCallImpl(
   //  -- r3 : the target to call (can be any Object).
   // -----------------------------------
   Label stack_overflow;
+
+#ifdef V8_REVERSE_JSARGS
   if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
     // The spread argument should not be pushed.
     __ SubP(r2, r2, Operand(1));
   }
+#endif
 
   // Calculate number of arguments (AddP one for receiver).
   __ AddP(r5, r2, Operand(1));
   Generate_StackOverflowCheck(masm, r5, ip, &stack_overflow);
 
+#ifdef V8_REVERSE_JSARGS
   if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
     // Don't copy receiver. Argument count is correct.
     __ LoadRR(r5, r2);
@@ -1383,6 +1474,20 @@ void Builtins::Generate_InterpreterPushArgsThenCallImpl(
     // lies in the next interpreter register.
     __ LoadP(r4, MemOperand(r4, -kSystemPointerSize));
   }
+#else
+  // Push "undefined" as the receiver arg if we need to.
+  if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
+    __ PushRoot(RootIndex::kUndefinedValue);
+    __ LoadRR(r5, r2);  // Argument count is correct.
+  }
+
+  // Push the arguments.
+  Generate_InterpreterPushArgs(masm, r5, r4, r6);
+  if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
+    __ Pop(r4);                   // Pass the spread in a register
+    __ SubP(r2, r2, Operand(1));  // Subtract one for spread
+  }
+#endif
 
   // Call the target.
   if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
@@ -1415,6 +1520,7 @@ void Builtins::Generate_InterpreterPushArgsThenConstructImpl(
   __ AddP(r7, r2, Operand(1));
   Generate_StackOverflowCheck(masm, r7, ip, &stack_overflow);
 
+#ifdef V8_REVERSE_JSARGS
   if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
     // The spread argument should not be pushed.
     __ SubP(r2, r2, Operand(1));
@@ -1436,6 +1542,22 @@ void Builtins::Generate_InterpreterPushArgsThenConstructImpl(
   } else {
     __ AssertUndefinedOrAllocationSite(r4, r7);
   }
+#else
+  // Push a slot for the receiver to be constructed.
+  __ LoadImmP(r0, Operand::Zero());
+  __ push(r0);
+
+  // Push the arguments (skip if none).
+  Generate_InterpreterPushArgs(masm, r2, r6, r7);
+
+  if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
+    __ Pop(r4);                   // Pass the spread in a register
+    __ SubP(r2, r2, Operand(1));  // Subtract one for spread
+  } else {
+    __ AssertUndefinedOrAllocationSite(r4, r7);
+  }
+
+#endif
 
   if (mode == InterpreterPushArgsMode::kArrayFunction) {
     __ AssertFunction(r3);
@@ -1601,6 +1723,7 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
   int allocatable_register_count = config->num_allocatable_general_registers();
   Register scratch = ip;
   if (with_result) {
+#ifdef V8_REVERSE_JSARGS
     if (java_script_builtin) {
       __ LoadRR(scratch, r2);
     } else {
@@ -1612,6 +1735,16 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
                               kSystemPointerSize +
                           BuiltinContinuationFrameConstants::kFixedFrameSize));
     }
+#else
+    // Overwrite the hole inserted by the deoptimizer with the return value from
+    // the LAZY deopt point.
+    __ StoreP(
+        r2,
+        MemOperand(sp, config->num_allocatable_general_registers() *
+                               kSystemPointerSize +
+                           BuiltinContinuationFrameConstants::kFixedFrameSize));
+    USE(scratch);
+#endif
   }
   for (int i = allocatable_register_count - 1; i >= 0; --i) {
     int code = config->GetAllocatableGeneralCode(i);
@@ -1620,6 +1753,7 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
       __ SmiUntag(Register::from_code(code));
     }
   }
+#ifdef V8_REVERSE_JSARGS
   if (java_script_builtin && with_result) {
     // Overwrite the hole inserted by the deoptimizer with the return value from
     // the LAZY deopt point. r0 contains the arguments count, the return value
@@ -1632,6 +1766,7 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
     __ SubP(r2, r2,
             Operand(BuiltinContinuationFrameConstants::kFixedSlotCount));
   }
+#endif
   __ LoadP(
       fp,
       MemOperand(sp, BuiltinContinuationFrameConstants::kFixedFrameSizeFromFp));
@@ -1721,9 +1856,9 @@ void Builtins::Generate_InterpreterOnStackReplacement(MacroAssembler* masm) {
 void Builtins::Generate_FunctionPrototypeApply(MacroAssembler* masm) {
   // ----------- S t a t e -------------
   //  -- r2    : argc
-  //  -- sp[0] : receiver
+  //  -- sp[0] : argArray
   //  -- sp[4] : thisArg
-  //  -- sp[8] : argArray
+  //  -- sp[8] : receiver
   // -----------------------------------
 
   // 1. Load receiver into r3, argArray into r4 (if present), remove all
@@ -1732,6 +1867,8 @@ void Builtins::Generate_FunctionPrototypeApply(MacroAssembler* masm) {
   {
     __ LoadRoot(r7, RootIndex::kUndefinedValue);
     __ LoadRR(r4, r7);
+
+#ifdef V8_REVERSE_JSARGS
     Label done;
 
     __ LoadP(r3, MemOperand(sp));  // receiver
@@ -1743,6 +1880,23 @@ void Builtins::Generate_FunctionPrototypeApply(MacroAssembler* masm) {
     __ LoadP(r4, MemOperand(sp, 2 * kSystemPointerSize));  // argArray
 
     __ bind(&done);
+#else
+    Label done;
+    __ ShiftLeftP(r1, r2, Operand(kSystemPointerSizeLog2));
+    __ LoadP(r3, MemOperand(sp, r1));  // receiver
+
+    __ SubP(r6, r2, Operand(1));
+    __ blt(&done);
+    __ ShiftLeftP(r1, r6, Operand(kSystemPointerSizeLog2));
+    __ LoadP(r7, MemOperand(sp, r1));
+
+    __ SubP(r6, r6, Operand(1));
+    __ blt(&done);
+    __ ShiftLeftP(r1, r6, Operand(kSystemPointerSizeLog2));
+    __ LoadP(r4, MemOperand(sp, r1));
+
+    __ bind(&done);
+#endif
     __ ShiftLeftP(r1, r2, Operand(kSystemPointerSizeLog2));
     __ lay(sp, MemOperand(sp, r1));
     __ StoreP(r7, MemOperand(sp));
@@ -1778,6 +1932,7 @@ void Builtins::Generate_FunctionPrototypeApply(MacroAssembler* masm) {
 
 // static
 void Builtins::Generate_FunctionPrototypeCall(MacroAssembler* masm) {
+#ifdef V8_REVERSE_JSARGS
   // 1. Get the callable to call (passed as receiver) from the stack.
   __ Pop(r3);
 
@@ -1794,6 +1949,46 @@ void Builtins::Generate_FunctionPrototypeCall(MacroAssembler* masm) {
 
   // 3. Adjust the actual number of arguments.
   __ SubP(r2, r2, Operand(1));
+#else
+  // 1. Make sure we have at least one argument.
+  // r2: actual number of arguments
+  {
+    Label done;
+    __ CmpP(r2, Operand::Zero());
+    __ bne(&done, Label::kNear);
+    __ PushRoot(RootIndex::kUndefinedValue);
+    __ AddP(r2, Operand(1));
+    __ bind(&done);
+  }
+
+  // r2: actual number of arguments
+  // 2. Get the callable to call (passed as receiver) from the stack.
+  __ LoadReceiver(r3, r2);
+
+  // 3. Shift arguments and return address one slot down on the stack
+  //    (overwriting the original receiver).  Adjust argument count to make
+  //    the original first argument the new receiver.
+  // r2: actual number of arguments
+  // r3: callable
+  {
+    Register scratch = r5;
+    Label loop;
+    // Calculate the copy start address (destination). Copy end address is sp.
+    __ ShiftLeftP(r4, r2, Operand(kSystemPointerSizeLog2));
+    __ lay(r4, MemOperand(sp, r4));
+
+    __ bind(&loop);
+    __ LoadP(scratch, MemOperand(r4, -kSystemPointerSize));
+    __ StoreP(scratch, MemOperand(r4));
+    __ SubP(r4, Operand(kSystemPointerSize));
+    __ CmpP(r4, sp);
+    __ bne(&loop);
+    // Adjust the actual number of arguments and remove the top element
+    // (which is a copy of the last argument).
+    __ SubP(r2, Operand(1));
+    __ pop();
+  }
+#endif
 
   // 4. Call the callable.
   __ Jump(masm->isolate()->builtins()->Call(), RelocInfo::CODE_TARGET);
@@ -1802,10 +1997,10 @@ void Builtins::Generate_FunctionPrototypeCall(MacroAssembler* masm) {
 void Builtins::Generate_ReflectApply(MacroAssembler* masm) {
   // ----------- S t a t e -------------
   //  -- r2     : argc
-  //  -- sp[0]  : receiver
-  //  -- sp[4]  : target         (if argc >= 1)
-  //  -- sp[8]  : thisArgument   (if argc >= 2)
-  //  -- sp[12] : argumentsList  (if argc == 3)
+  //  -- sp[0]  : argumentsList
+  //  -- sp[4]  : thisArgument
+  //  -- sp[8]  : target
+  //  -- sp[12] : receiver
   // -----------------------------------
 
   // 1. Load target into r3 (if present), argumentsList into r4 (if present),
@@ -1816,6 +2011,7 @@ void Builtins::Generate_ReflectApply(MacroAssembler* masm) {
     __ LoadRR(r7, r3);
     __ LoadRR(r4, r3);
 
+#ifdef V8_REVERSE_JSARGS
     Label done;
 
     __ cghi(r2, Operand(1));
@@ -1829,6 +2025,25 @@ void Builtins::Generate_ReflectApply(MacroAssembler* masm) {
     __ LoadP(r4, MemOperand(sp, 3 * kSystemPointerSize));  // argArray
 
     __ bind(&done);
+#else
+    Label done;
+    __ SubP(r6, r2, Operand(1));
+    __ blt(&done);
+    __ ShiftLeftP(r1, r6, Operand(kSystemPointerSizeLog2));
+    __ LoadP(r3, MemOperand(sp, r1));  // receiver
+
+    __ SubP(r6, r6, Operand(1));
+    __ blt(&done);
+    __ ShiftLeftP(r1, r6, Operand(kSystemPointerSizeLog2));
+    __ LoadP(r7, MemOperand(sp, r1));
+
+    __ SubP(r6, r6, Operand(1));
+    __ blt(&done);
+    __ ShiftLeftP(r1, r6, Operand(kSystemPointerSizeLog2));
+    __ LoadP(r4, MemOperand(sp, r1));
+
+    __ bind(&done);
+#endif
     __ ShiftLeftP(r1, r2, Operand(kSystemPointerSizeLog2));
     __ lay(sp, MemOperand(sp, r1));
     __ StoreP(r7, MemOperand(sp));
@@ -1852,11 +2067,12 @@ void Builtins::Generate_ReflectApply(MacroAssembler* masm) {
 void Builtins::Generate_ReflectConstruct(MacroAssembler* masm) {
   // ----------- S t a t e -------------
   //  -- r2     : argc
-  //  -- sp[0]  : receiver
-  //  -- sp[4]  : target
-  //  -- sp[8]  : argumentsList
-  //  -- sp[12] : new.target (optional)
+  //  -- sp[0]  : new.target (optional)
+  //  -- sp[4]  : argumentsList
+  //  -- sp[8]  : target
+  //  -- sp[12] : receiver
   // -----------------------------------
+  // NOTE: The order of args in the stack are reversed if V8_REVERSE_JSARGS
 
   // 1. Load target into r3 (if present), argumentsList into r4 (if present),
   // new.target into r5 (if present, otherwise use target), remove all
@@ -1866,6 +2082,7 @@ void Builtins::Generate_ReflectConstruct(MacroAssembler* masm) {
     __ LoadRoot(r3, RootIndex::kUndefinedValue);
     __ LoadRR(r4, r3);
 
+#ifdef V8_REVERSE_JSARGS
     Label done;
 
     __ LoadRR(r6, r3);
@@ -1883,6 +2100,30 @@ void Builtins::Generate_ReflectConstruct(MacroAssembler* masm) {
     __ ShiftLeftP(r1, r2, Operand(kSystemPointerSizeLog2));
     __ lay(sp, MemOperand(sp, r1));
     __ StoreP(r6, MemOperand(sp));
+#else
+    Label done;
+    __ ShiftLeftP(r1, r2, Operand(kSystemPointerSizeLog2));
+    __ StoreP(r4, MemOperand(sp, r1));
+    __ SubP(r6, r2, Operand(1));
+    __ blt(&done);
+    __ ShiftLeftP(r1, r6, Operand(kSystemPointerSizeLog2));
+    __ LoadP(r3, MemOperand(sp, r1));  // receiver
+
+    __ LoadRR(r5, r3);
+    __ SubP(r6, r6, Operand(1));
+    __ blt(&done);
+    __ ShiftLeftP(r1, r6, Operand(kSystemPointerSizeLog2));
+    __ LoadP(r4, MemOperand(sp, r1));
+
+    __ SubP(r6, r6, Operand(1));
+    __ blt(&done);
+    __ ShiftLeftP(r1, r6, Operand(kSystemPointerSizeLog2));
+    __ LoadP(r5, MemOperand(sp, r1));
+
+    __ bind(&done);
+    __ ShiftLeftP(r1, r2, Operand(kSystemPointerSizeLog2));
+    __ lay(sp, MemOperand(sp, r1));
+#endif
   }
 
   // ----------- S t a t e -------------
@@ -1981,6 +2222,7 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
   Label stack_overflow;
   Generate_StackOverflowCheck(masm, r6, scratch, &stack_overflow);
 
+#ifdef V8_REVERSE_JSARGS
   // Move the arguments already in the stack,
   // including the receiver and the return address.
   {
@@ -2002,6 +2244,7 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
     __ bind(&check);
     __ b(ge, &copy);
   }
+#endif
 
   // Push arguments onto the stack (thisArgument is already on the stack).
   {
@@ -2018,8 +2261,12 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
     __ bne(&skip, Label::kNear);
     __ LoadRoot(scratch, RootIndex::kUndefinedValue);
     __ bind(&skip);
+#ifdef V8_REVERSE_JSARGS
     __ StoreP(scratch, MemOperand(r7));
     __ lay(r7, MemOperand(r7, kSystemPointerSize));
+#else
+    __ Push(scratch);
+#endif
     __ BranchOnCount(r1, &loop);
     __ bind(&no_args);
     __ AddP(r2, r2, r6);
@@ -2107,6 +2354,7 @@ void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
     Generate_StackOverflowCheck(masm, r7, scratch, &stack_overflow);
 
     // Forward the arguments from the caller frame.
+#ifdef V8_REVERSE_JSARGS
     __ LoadRR(r5, r5);
     // Point to the first argument to copy (skipping the receiver).
     __ AddP(r6, r6,
@@ -2137,19 +2385,26 @@ void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
       __ bind(&check);
       __ b(ge, &copy);
     }
-
+#endif
     // Copy arguments from the caller frame.
     // TODO(victorgomes): Consider using forward order as potentially more cache
     // friendly.
     {
       Label loop;
+#ifndef V8_REVERSE_JSARGS
+      __ AddP(r6, r6, Operand(CommonFrameConstants::kFixedFrameSizeAboveFp));
+#endif
       __ AddP(r2, r2, r7);
       __ bind(&loop);
       {
         __ SubP(r7, r7, Operand(1));
         __ ShiftLeftP(r1, r7, Operand(kSystemPointerSizeLog2));
         __ LoadP(scratch, MemOperand(r6, r1));
+#ifdef V8_REVERSE_JSARGS
         __ StoreP(scratch, MemOperand(r4, r1));
+#else
+        __ push(scratch);
+#endif
         __ CmpP(r7, Operand::Zero());
         __ bne(&loop);
       }
@@ -2313,6 +2568,7 @@ void Generate_PushBoundArguments(MacroAssembler* masm) {
       __ bind(&done);
     }
 
+#ifdef V8_REVERSE_JSARGS
     // Pop receiver.
     __ Pop(r7);
 
@@ -2334,6 +2590,42 @@ void Generate_PushBoundArguments(MacroAssembler* masm) {
 
     // Push receiver.
     __ Push(r7);
+#else
+    __ LoadRR(scratch, sp);
+    __ LoadRR(sp, r1);
+
+    // Relocate arguments down the stack.
+    //  -- r2 : the number of arguments (not including the receiver)
+    //  -- r8 : the previous stack pointer
+    {
+      Label skip, loop;
+      __ LoadImmP(r7, Operand::Zero());
+      __ CmpP(r2, Operand::Zero());
+      __ beq(&skip);
+      __ LoadRR(r1, r2);
+      __ bind(&loop);
+      __ LoadP(r0, MemOperand(scratch, r7));
+      __ StoreP(r0, MemOperand(sp, r7));
+      __ lay(r7, MemOperand(r7, kSystemPointerSize));
+      __ BranchOnCount(r1, &loop);
+      __ bind(&skip);
+    }
+
+    // Copy [[BoundArguments]] to the stack (below the arguments).
+    {
+      Label loop;
+      __ ShiftLeftP(r9, r6, Operand(kTaggedSizeLog2));
+      __ lay(r4, MemOperand(r4, r9, FixedArray::kHeaderSize - kHeapObjectTag));
+      __ LoadRR(r1, r6);
+      __ bind(&loop);
+      __ LoadAnyTaggedField(ip, MemOperand(r4, -kTaggedSize), r0);
+      __ lay(r4, MemOperand(r4, -kTaggedSize));
+      __ StoreP(ip, MemOperand(sp, r7));
+      __ lay(r7, MemOperand(r7, kSystemPointerSize));
+      __ BranchOnCount(r1, &loop);
+      __ AddP(r2, r2, r6);
+    }
+#endif
   }
   __ bind(&no_bound_arguments);
 }
@@ -2528,12 +2820,18 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
   //  -- r5 : new target (passed through to callee)
   // -----------------------------------
 
-  Label dont_adapt_arguments, stack_overflow;
+  Label dont_adapt_arguments, stack_overflow, skip_adapt_arguments;
   __ tmll(r4, Operand(kDontAdaptArgumentsSentinel));
   __ b(Condition(1), &dont_adapt_arguments);
   __ LoadTaggedPointerField(
       r6, FieldMemOperand(r3, JSFunction::kSharedFunctionInfoOffset));
   __ LoadlW(r6, FieldMemOperand(r6, SharedFunctionInfo::kFlagsOffset));
+#ifndef V8_REVERSE_JSARGS
+  __ tmlh(r6,
+          Operand(SharedFunctionInfo::IsSafeToSkipArgumentsAdaptorBit::kMask >>
+                  16));
+  __ bne(&skip_adapt_arguments);
+#endif
 
   // -------------------------------------------
   // Adapt arguments.
@@ -2554,8 +2852,13 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
       // r3: function
       // r4: expected number of arguments
       // r5: new target (passed through to callee)
+#ifdef V8_REVERSE_JSARGS
       __ ShiftLeftP(r2, r4, Operand(kSystemPointerSizeLog2));
       __ AddP(r2, fp);
+#else
+      __ SmiToPtrArrayOffset(r2, r2);
+      __ AddP(r2, fp);
+#endif
       // adjust for return address and receiver
       __ AddP(r2, r2, Operand(2 * kSystemPointerSize));
       __ ShiftLeftP(r6, r4, Operand(kSystemPointerSizeLog2));
@@ -2585,6 +2888,7 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
       EnterArgumentsAdaptorFrame(masm);
       Generate_StackOverflowCheck(masm, r4, r7, &stack_overflow);
 
+#ifdef V8_REVERSE_JSARGS
       // Fill the remaining expected arguments with undefined.
       // r0: actual number of arguments as a smi
       // r1: function
@@ -2629,6 +2933,46 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
       __ CmpP(r2, fp);  // Compare before moving to next argument.
       __ lay(r2, MemOperand(r2, -kSystemPointerSize));
       __ b(ne, &copy);
+#else
+      // Calculate copy start address into r0 and copy end address is fp.
+      // r2: actual number of arguments as a smi
+      // r3: function
+      // r4: expected number of arguments
+      // r5: new target (passed through to callee)
+      __ SmiToPtrArrayOffset(r2, r2);
+      __ lay(r2, MemOperand(r2, fp));
+
+      // Copy the arguments (including the receiver) to the new stack frame.
+      // r2: copy start address
+      // r3: function
+      // r4: expected number of arguments
+      // r5: new target (passed through to callee)
+      Label copy;
+      __ bind(&copy);
+      // Adjust load for return address and receiver.
+      __ LoadP(r0, MemOperand(r2, 2 * kSystemPointerSize));
+      __ push(r0);
+      __ CmpP(r2, fp);  // Compare before moving to next argument.
+      __ lay(r2, MemOperand(r2, -kSystemPointerSize));
+      __ bne(&copy);
+
+      // Fill the remaining expected arguments with undefined.
+      // r3: function
+      // r4: expected number of argumentus
+      __ LoadRoot(r0, RootIndex::kUndefinedValue);
+      __ ShiftLeftP(r6, r4, Operand(kSystemPointerSizeLog2));
+      __ SubP(r6, fp, r6);
+      // Adjust for frame.
+      __ SubP(r6, r6,
+              Operand(ArgumentsAdaptorFrameConstants::kFixedFrameSizeFromFp +
+                      kSystemPointerSize));
+
+      Label fill;
+      __ bind(&fill);
+      __ push(r0);
+      __ CmpP(sp, r6);
+      __ bne(&fill);
+#endif
     }
 
     // Call the entry point.
@@ -2650,6 +2994,42 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
     __ Ret();
   }
 
+  // -------------------------------------------
+  // Skip adapt arguments.
+  // -------------------------------------------
+  __ bind(&skip_adapt_arguments);
+  {
+    // The callee cannot observe the actual arguments, so it's safe to just
+    // pass the expected arguments by massaging the stack appropriately. See
+    // http://bit.ly/v8-faster-calls-with-arguments-mismatch for details.
+    Label under_application, over_application;
+    __ CmpP(r2, r4);
+    __ blt(&under_application);
+
+    __ bind(&over_application);
+    {
+      // Remove superfluous parameters from the stack.
+      __ SubP(r6, r2, r4);
+      __ lgr(r2, r4);
+      __ ShiftLeftP(r6, r6, Operand(kSystemPointerSizeLog2));
+      __ lay(sp, MemOperand(sp, r6));
+      __ b(&dont_adapt_arguments);
+    }
+
+    __ bind(&under_application);
+    {
+      // Fill remaining expected arguments with undefined values.
+      Label fill;
+      __ LoadRoot(r6, RootIndex::kUndefinedValue);
+      __ bind(&fill);
+      __ AddP(r2, r2, Operand(1));
+      __ push(r6);
+      __ CmpP(r2, r4);
+      __ blt(&fill);
+      __ b(&dont_adapt_arguments);
+    }
+  }
+
   // -------------------------------------------
   // Dont adapt arguments.
   // -------------------------------------------
@@ -3143,11 +3523,12 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
   //  -- r4                  : arguments count (not including the receiver)
   //  -- r5                  : call data
   //  -- r2                  : holder
-  //  -- sp[0]               : receiver
-  //  -- sp[8]               : first argument
+  //  -- sp[0]               : last argument
   //  -- ...
-  //  -- sp[(argc) * 8]      : last argument
+  //  -- sp[(argc - 1) * 4]  : first argument
+  //  -- sp[(argc + 0) * 4]  : receiver
   // -----------------------------------
+  // NOTE: The order of args are reversed if V8_REVERSE_JSARGS
 
   Register api_function_address = r3;
   Register argc = r4;
@@ -3222,8 +3603,15 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
 
   // FunctionCallbackInfo::values_ (points at the first varargs argument passed
   // on the stack).
+#ifdef V8_REVERSE_JSARGS
   __ AddP(scratch, scratch,
           Operand((FCA::kArgsLength + 1) * kSystemPointerSize));
+#else
+  __ AddP(scratch, scratch,
+          Operand((FCA::kArgsLength - 1) * kSystemPointerSize));
+  __ ShiftLeftP(r1, argc, Operand(kSystemPointerSizeLog2));
+  __ AddP(scratch, scratch, r1);
+#endif
   __ StoreP(scratch, MemOperand(sp, (kStackFrameExtraParamSlot + 2) *
                                         kSystemPointerSize));
 
diff --git a/src/builtins/x64/builtins-x64.cc b/src/builtins/x64/builtins-x64.cc
index f7eb4658d5..987059b19b 100644
--- a/src/builtins/x64/builtins-x64.cc
+++ b/src/builtins/x64/builtins-x64.cc
@@ -100,6 +100,7 @@ void Generate_JSBuiltinsConstructStubHelper(MacroAssembler* masm) {
     // correct position (including any undefined), instead of delaying this to
     // InvokeFunction.
 
+#ifdef V8_REVERSE_JSARGS
     // Set up pointer to first argument (skip receiver).
     __ leaq(rbx, Operand(rbp, StandardFrameConstants::kCallerSPOffset +
                                   kSystemPointerSize));
@@ -107,6 +108,14 @@ void Generate_JSBuiltinsConstructStubHelper(MacroAssembler* masm) {
     __ PushArray(rbx, rax, rcx);
     // The receiver for the builtin/api call.
     __ PushRoot(RootIndex::kTheHoleValue);
+#else
+    // The receiver for the builtin/api call.
+    __ PushRoot(RootIndex::kTheHoleValue);
+    // Set up pointer to last argument.
+    __ leaq(rbx, Operand(rbp, StandardFrameConstants::kCallerSPOffset));
+    // Copy arguments to the expression stack.
+    __ PushArray(rbx, rax, rcx);
+#endif
 
     // Call the function.
     // rax: number of arguments (untagged)
@@ -204,16 +213,26 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
   // Push the allocated receiver to the stack.
   __ Push(rax);
 
-  // We need two copies because we may have to return the original one
-  // and the calling conventions dictate that the called function pops the
-  // receiver. The second copy is pushed after the arguments, we saved in r8
-  // since rax needs to store the number of arguments before
-  // InvokingFunction.
-  __ movq(r8, rax);
+#ifdef V8_REVERSE_JSARGS
+    // We need two copies because we may have to return the original one
+    // and the calling conventions dictate that the called function pops the
+    // receiver. The second copy is pushed after the arguments, we saved in r8
+    // since rax needs to store the number of arguments before
+    // InvokingFunction.
+    __ movq(r8, rax);
 
-  // Set up pointer to first argument (skip receiver).
-  __ leaq(rbx, Operand(rbp, StandardFrameConstants::kCallerSPOffset +
-                                kSystemPointerSize));
+    // Set up pointer to first argument (skip receiver).
+    __ leaq(rbx, Operand(rbp, StandardFrameConstants::kCallerSPOffset +
+                                  kSystemPointerSize));
+#else
+    // We need two copies because we may have to return the original one
+    // and the calling conventions dictate that the called function pops the
+    // receiver.
+    __ Push(rax);
+
+    // Set up pointer to last argument.
+    __ leaq(rbx, Operand(rbp, StandardFrameConstants::kCallerSPOffset));
+#endif
 
   // Restore constructor function and argument count.
   __ movq(rdi, Operand(rbp, ConstructFrameConstants::kConstructorOffset));
@@ -232,8 +251,10 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
   // Copy arguments to the expression stack.
   __ PushArray(rbx, rax, rcx);
 
-  // Push implicit receiver.
-  __ Push(r8);
+#ifdef V8_REVERSE_JSARGS
+    // Push implicit receiver.
+    __ Push(r8);
+#endif
 
   // Call the function.
   __ InvokeFunction(rdi, rdx, rax, CALL_FUNCTION);
@@ -555,6 +576,11 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
     // Push the function onto the stack.
     __ Push(rdi);
 
+#ifndef V8_REVERSE_JSARGS
+    // Push the receiver onto the stack.
+    __ Push(arg_reg_4);
+#endif
+
 #ifdef V8_TARGET_OS_WIN
     // Load the previous frame pointer to access C arguments on stack
     __ movq(kScratchRegister, Operand(rbp, 0));
@@ -565,7 +591,9 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
     // Load the number of arguments and setup pointer to the arguments.
     __ movq(rax, r8);
     __ movq(rbx, r9);
+#ifdef V8_REVERSE_JSARGS
     __ movq(r9, arg_reg_4);  // Temporarily saving the receiver.
+#endif
 #endif  // V8_TARGET_OS_WIN
 
     // Current stack contents:
@@ -595,6 +623,7 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
     // Copy arguments to the stack in a loop.
     // Register rbx points to array of pointers to handle locations.
     // Push the values of these handles.
+#ifdef V8_REVERSE_JSARGS
     Label loop, entry;
     __ movq(rcx, rax);
     __ jmp(&entry, Label::kNear);
@@ -607,6 +636,18 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
 
     // Push the receiver.
     __ Push(r9);
+#else
+    Label loop, entry;
+    __ Set(rcx, 0);  // Set loop variable to 0.
+    __ jmp(&entry, Label::kNear);
+    __ bind(&loop);
+    __ movq(kScratchRegister, Operand(rbx, rcx, times_system_pointer_size, 0));
+    __ Push(Operand(kScratchRegister, 0));  // dereference handle
+    __ addq(rcx, Immediate(1));
+    __ bind(&entry);
+    __ cmpq(rcx, rax);
+    __ j(not_equal, &loop, Label::kNear);
+#endif
 
     // Invoke the builtin code.
     Handle<Code> builtin = is_construct
@@ -699,6 +740,12 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
   // Pop return address.
   __ PopReturnAddressTo(rax);
 
+#ifndef V8_REVERSE_JSARGS
+  // Push receiver.
+  __ PushTaggedPointerField(
+      FieldOperand(rdx, JSGeneratorObject::kReceiverOffset), decompr_scratch1);
+#endif
+
   // ----------- S t a t e -------------
   //  -- rax    : return address
   //  -- rdx    : the JSGeneratorObject to resume
@@ -716,6 +763,7 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
       rbx, FieldOperand(rdx, JSGeneratorObject::kParametersAndRegistersOffset));
 
   {
+#ifdef V8_REVERSE_JSARGS
     {
       Label done_loop, loop;
       __ movq(r9, rcx);
@@ -735,6 +783,21 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
     __ PushTaggedPointerField(
         FieldOperand(rdx, JSGeneratorObject::kReceiverOffset),
         decompr_scratch1);
+#else
+    Label done_loop, loop;
+    __ Set(r9, 0);
+
+    __ bind(&loop);
+    __ cmpl(r9, rcx);
+    __ j(greater_equal, &done_loop, Label::kNear);
+    __ PushTaggedAnyField(
+        FieldOperand(rbx, r9, times_tagged_size, FixedArray::kHeaderSize),
+        decompr_scratch1);
+    __ addl(r9, Immediate(1));
+    __ jmp(&loop);
+
+    __ bind(&done_loop);
+#endif
   }
 
   // Underlying function needs to have bytecode available.
@@ -1264,8 +1327,12 @@ static void Generate_InterpreterPushArgs(MacroAssembler* masm,
           Operand(start_address, scratch, times_system_pointer_size,
                   kSystemPointerSize));
   // Push the arguments.
+#ifdef V8_REVERSE_JSARGS
   __ PushArray(start_address, num_args, scratch,
                TurboAssembler::PushArrayOrder::kReverse);
+#else
+  __ PushArray(start_address, num_args, scratch);
+#endif
 }
 
 // static
@@ -1282,10 +1349,12 @@ void Builtins::Generate_InterpreterPushArgsThenCallImpl(
   // -----------------------------------
   Label stack_overflow;
 
+#ifdef V8_REVERSE_JSARGS
   if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
     // The spread argument should not be pushed.
     __ decl(rax);
   }
+#endif
 
   __ leal(rcx, Operand(rax, 1));  // Add one for receiver.
 
@@ -1295,6 +1364,7 @@ void Builtins::Generate_InterpreterPushArgsThenCallImpl(
   // Pop return address to allow tail-call after pushing arguments.
   __ PopReturnAddressTo(kScratchRegister);
 
+#ifdef V8_REVERSE_JSARGS
   if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
     // Don't copy receiver.
     __ decq(rcx);
@@ -1314,6 +1384,21 @@ void Builtins::Generate_InterpreterPushArgsThenCallImpl(
     // is below that.
     __ movq(rbx, Operand(rbx, -kSystemPointerSize));
   }
+#else
+  // Push "undefined" as the receiver arg if we need to.
+  if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
+    __ PushRoot(RootIndex::kUndefinedValue);
+    __ decl(rcx);  // Subtract one for receiver.
+  }
+
+  // rbx and rdx will be modified.
+  Generate_InterpreterPushArgs(masm, rcx, rbx, rdx);
+
+  if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
+    __ Pop(rbx);                 // Pass the spread in a register
+    __ decl(rax);                // Subtract one for spread
+  }
+#endif
 
   // Call the target.
   __ PushReturnAddressFrom(kScratchRegister);  // Re-push return address.
@@ -1356,6 +1441,7 @@ void Builtins::Generate_InterpreterPushArgsThenConstructImpl(
   // Pop return address to allow tail-call after pushing arguments.
   __ PopReturnAddressTo(kScratchRegister);
 
+#ifdef V8_REVERSE_JSARGS
   if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
     // The spread argument should not be pushed.
     __ decl(rax);
@@ -1366,10 +1452,22 @@ void Builtins::Generate_InterpreterPushArgsThenConstructImpl(
 
   // Push slot for the receiver to be constructed.
   __ Push(Immediate(0));
+#else
+  // Push slot for the receiver to be constructed.
+  __ Push(Immediate(0));
+
+  // rcx and r8 will be modified.
+  Generate_InterpreterPushArgs(masm, rax, rcx, r8);
+#endif
 
   if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
+#ifdef V8_REVERSE_JSARGS
     // Pass the spread in the register rbx.
     __ movq(rbx, Operand(rcx, -kSystemPointerSize));
+#else
+    __ Pop(rbx);                 // Pass the spread in a register
+    __ decl(rax);                // Subtract one for spread
+#endif
     // Push return address in preparation for the tail-call.
     __ PushReturnAddressFrom(kScratchRegister);
   } else {
@@ -1538,6 +1636,7 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
   const RegisterConfiguration* config(RegisterConfiguration::Default());
   int allocatable_register_count = config->num_allocatable_general_registers();
   if (with_result) {
+#ifdef V8_REVERSE_JSARGS
     if (java_script_builtin) {
       // kScratchRegister is not included in the allocateable registers.
       __ movq(kScratchRegister, rax);
@@ -1550,6 +1649,15 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
                            BuiltinContinuationFrameConstants::kFixedFrameSize),
           rax);
     }
+#else
+    // Overwrite the hole inserted by the deoptimizer with the return value from
+    // the LAZY deopt point.
+    __ movq(
+        Operand(rsp, config->num_allocatable_general_registers() *
+                             kSystemPointerSize +
+                         BuiltinContinuationFrameConstants::kFixedFrameSize),
+        rax);
+#endif
   }
   for (int i = allocatable_register_count - 1; i >= 0; --i) {
     int code = config->GetAllocatableGeneralCode(i);
@@ -1558,6 +1666,7 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
       __ SmiUntag(Register::from_code(code));
     }
   }
+#ifdef V8_REVERSE_JSARGS
   if (with_result && java_script_builtin) {
     // Overwrite the hole inserted by the deoptimizer with the return value from
     // the LAZY deopt point. rax contains the arguments count, the return value
@@ -1566,6 +1675,7 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
                     BuiltinContinuationFrameConstants::kFixedFrameSize),
             kScratchRegister);
   }
+#endif
   __ movq(
       rbp,
       Operand(rsp, BuiltinContinuationFrameConstants::kFixedFrameSizeFromFp));
@@ -1695,6 +1805,7 @@ void Builtins::Generate_FunctionPrototypeCall(MacroAssembler* masm) {
   // rsp[8 * (n + 1)] : Argument n
   // rax contains the number of arguments, n, not counting the receiver.
 
+#ifdef V8_REVERSE_JSARGS
   // 1. Get the callable to call (passed as receiver) from the stack.
   {
     StackArgumentsAccessor args(rax);
@@ -1720,6 +1831,42 @@ void Builtins::Generate_FunctionPrototypeCall(MacroAssembler* masm) {
   __ PushReturnAddressFrom(rbx);
   __ decq(rax);  // One fewer argument (first argument is new receiver).
 
+#else
+  // 1. Make sure we have at least one argument.
+  {
+    Label done;
+    __ testq(rax, rax);
+    __ j(not_zero, &done, Label::kNear);
+    __ PopReturnAddressTo(rbx);
+    __ PushRoot(RootIndex::kUndefinedValue);
+    __ PushReturnAddressFrom(rbx);
+    __ incq(rax);
+    __ bind(&done);
+  }
+
+  // 2. Get the callable to call (passed as receiver) from the stack.
+  {
+    StackArgumentsAccessor args(rax);
+    __ movq(rdi, args.GetReceiverOperand());
+  }
+
+  // 3. Shift arguments and return address one slot down on the stack
+  //    (overwriting the original receiver).  Adjust argument count to make
+  //    the original first argument the new receiver.
+  {
+    Label loop;
+    __ movq(rcx, rax);
+    StackArgumentsAccessor args(rcx);
+    __ bind(&loop);
+    __ movq(rbx, args[1]);
+    __ movq(args[0], rbx);
+    __ decq(rcx);
+    __ j(not_zero, &loop);              // While non-zero.
+    __ DropUnderReturnAddress(1, rbx);  // Drop one slot under return address.
+    __ decq(rax);  // One fewer argument (first argument is new receiver).
+  }
+#endif
+
   // 5. Call the callable.
   // Since we did not create a frame for Function.prototype.call() yet,
   // we use a normal Call builtin here.
@@ -1877,12 +2024,20 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
   //  -- rdi : function (passed through to callee)
   // -----------------------------------
 
-  Label dont_adapt_arguments, stack_overflow;
+  Label dont_adapt_arguments, stack_overflow, skip_adapt_arguments;
   __ cmpq(rbx, Immediate(kDontAdaptArgumentsSentinel));
   __ j(equal, &dont_adapt_arguments);
   __ LoadTaggedPointerField(
       rcx, FieldOperand(rdi, JSFunction::kSharedFunctionInfoOffset));
 
+#ifndef V8_REVERSE_JSARGS
+  // This optimization is disabled when the arguments are reversed.
+  __ testl(
+      FieldOperand(rcx, SharedFunctionInfo::kFlagsOffset),
+      Immediate(SharedFunctionInfo::IsSafeToSkipArgumentsAdaptorBit::kMask));
+  __ j(not_zero, &skip_adapt_arguments);
+#endif
+
   // -------------------------------------------
   // Adapt arguments.
   // -------------------------------------------
@@ -1899,7 +2054,11 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
     {
       // Copy receiver and all expected arguments.
       const int offset = StandardFrameConstants::kCallerSPOffset;
+#ifdef V8_REVERSE_JSARGS
       __ leaq(r8, Operand(rbp, rbx, times_system_pointer_size, offset));
+#else
+      __ leaq(r8, Operand(rbp, rax, times_system_pointer_size, offset));
+#endif
       __ Set(rax, -1);  // account for receiver
 
       Label copy;
@@ -1915,6 +2074,7 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
     // Too few parameters: Actual < expected.
     __ bind(&under_application);
     {
+#ifdef V8_REVERSE_JSARGS
       // Fill remaining expected arguments with undefined values.
       Label fill;
       __ LoadRoot(kScratchRegister, RootIndex::kUndefinedValue);
@@ -1940,6 +2100,29 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
 
       // Update actual number of arguments.
       __ movq(rax, rbx);
+#else   // !V8_REVERSE_JSARGS
+        // Copy receiver and all actual arguments.
+      const int offset = StandardFrameConstants::kCallerSPOffset;
+      __ leaq(r9, Operand(rbp, rax, times_system_pointer_size, offset));
+      __ Set(r8, -1);  // account for receiver
+
+      Label copy;
+      __ bind(&copy);
+      __ incq(r8);
+      __ Push(Operand(r9, 0));
+      __ subq(r9, Immediate(kSystemPointerSize));
+      __ cmpq(r8, rax);
+      __ j(less, &copy);
+
+      // Fill remaining expected arguments with undefined values.
+      Label fill;
+      __ LoadRoot(kScratchRegister, RootIndex::kUndefinedValue);
+      __ bind(&fill);
+      __ incq(rax);
+      __ Push(kScratchRegister);
+      __ cmpq(rax, rbx);
+      __ j(less, &fill);
+#endif  // !V8_REVERSE_JSARGS
     }
 
     // Call the entry point.
@@ -1960,6 +2143,44 @@ void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
     __ ret(0);
   }
 
+  // -------------------------------------------
+  // Skip adapt arguments.
+  // -------------------------------------------
+  __ bind(&skip_adapt_arguments);
+  {
+    // The callee cannot observe the actual arguments, so it's safe to just
+    // pass the expected arguments by massaging the stack appropriately. See
+    // http://bit.ly/v8-faster-calls-with-arguments-mismatch for details.
+    Label under_application, over_application, invoke;
+    __ PopReturnAddressTo(rcx);
+    __ cmpq(rax, rbx);
+    __ j(less, &under_application, Label::kNear);
+
+    __ bind(&over_application);
+    {
+      // Remove superfluous parameters from the stack.
+      __ xchgq(rax, rbx);
+      __ subq(rbx, rax);
+      __ leaq(rsp, Operand(rsp, rbx, times_system_pointer_size, 0));
+      __ jmp(&invoke, Label::kNear);
+    }
+
+    __ bind(&under_application);
+    {
+      // Fill remaining expected arguments with undefined values.
+      Label fill;
+      __ LoadRoot(kScratchRegister, RootIndex::kUndefinedValue);
+      __ bind(&fill);
+      __ incq(rax);
+      __ Push(kScratchRegister);
+      __ cmpq(rax, rbx);
+      __ j(less, &fill);
+    }
+
+    __ bind(&invoke);
+    __ PushReturnAddressFrom(rcx);
+  }
+
   // -------------------------------------------
   // Don't adapt arguments.
   // -------------------------------------------
@@ -2012,6 +2233,7 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
   __ StackOverflowCheck(rcx, r8, &stack_overflow, Label::kNear);
 
   // Push additional arguments onto the stack.
+#ifdef V8_REVERSE_JSARGS
   // Move the arguments already in the stack,
   // including the receiver and the return address.
   {
@@ -2058,6 +2280,30 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
     __ bind(&done);
     __ addq(rax, current);
   }
+#else  // !V8_REVERSE_JSARGS
+  {
+    Register value = scratch;
+    __ PopReturnAddressTo(r8);
+    __ Set(r9, 0);
+    Label done, push, loop;
+    __ bind(&loop);
+    __ cmpl(r9, rcx);
+    __ j(equal, &done, Label::kNear);
+    // Turn the hole into undefined as we go.
+    __ LoadAnyTaggedField(value, FieldOperand(rbx, r9, times_tagged_size,
+                                              FixedArray::kHeaderSize));
+    __ CompareRoot(value, RootIndex::kTheHoleValue);
+    __ j(not_equal, &push, Label::kNear);
+    __ LoadRoot(value, RootIndex::kUndefinedValue);
+    __ bind(&push);
+    __ Push(value);
+    __ incl(r9);
+    __ jmp(&loop);
+    __ bind(&done);
+    __ PushReturnAddressFrom(r8);
+    __ addq(rax, r9);
+  }
+#endif
 
   // Tail-call to the actual Call or Construct builtin.
   __ Jump(code, RelocInfo::CODE_TARGET);
@@ -2142,6 +2388,7 @@ void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
     __ StackOverflowCheck(r8, r12, &stack_overflow, Label::kNear);
 
     // Forward the arguments from the caller frame.
+#ifdef V8_REVERSE_JSARGS
     // Move the arguments already in the stack,
     // including the receiver and the return address.
     {
@@ -2188,6 +2435,21 @@ void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
               kScratchRegister);
       __ j(not_zero, &loop);
     }
+#else
+    {
+      Label loop;
+      __ addl(rax, r8);
+      __ PopReturnAddressTo(rcx);
+      __ bind(&loop);
+      {
+        __ decl(r8);
+        __ Push(Operand(rbx, r8, times_system_pointer_size,
+                        kFPOnStackSize + kPCOnStackSize));
+        __ j(not_zero, &loop);
+      }
+      __ PushReturnAddressFrom(rcx);
+    }
+#endif
   }
   __ jmp(&stack_done, Label::kNear);
   __ bind(&stack_overflow);
@@ -2359,6 +2621,7 @@ void Generate_PushBoundArguments(MacroAssembler* masm) {
       __ bind(&done);
     }
 
+#ifdef V8_REVERSE_JSARGS
     // Save Return Address and Receiver into registers.
     __ Pop(r8);
     __ Pop(r10);
@@ -2386,6 +2649,54 @@ void Generate_PushBoundArguments(MacroAssembler* masm) {
     // Recover Receiver and Return Address.
     __ Push(r10);
     __ Push(r8);
+#else   // !V8_REVERSE_JSARGS
+    // Reserve stack space for the [[BoundArguments]].
+    __ movq(kScratchRegister, rbx);
+    __ AllocateStackSpace(kScratchRegister);
+
+    // Adjust effective number of arguments to include return address.
+    __ incl(rax);
+
+    // Relocate arguments and return address down the stack.
+    {
+      Label loop;
+      __ Set(rcx, 0);
+      __ addq(rbx, rsp);
+      __ bind(&loop);
+      __ movq(kScratchRegister,
+              Operand(rbx, rcx, times_system_pointer_size, 0));
+      __ movq(Operand(rsp, rcx, times_system_pointer_size, 0),
+              kScratchRegister);
+      __ incl(rcx);
+      __ cmpl(rcx, rax);
+      __ j(less, &loop);
+    }
+
+    // Copy [[BoundArguments]] to the stack (below the arguments).
+    {
+      Label loop;
+      __ LoadTaggedPointerField(
+          rcx, FieldOperand(rdi, JSBoundFunction::kBoundArgumentsOffset));
+      __ SmiUntagField(rbx, FieldOperand(rcx, FixedArray::kLengthOffset));
+      __ bind(&loop);
+      // Instead of doing decl(rbx) here subtract kTaggedSize from the header
+      // offset in order be able to move decl(rbx) right before the loop
+      // condition. This is necessary in order to avoid flags corruption by
+      // pointer decompression code.
+      __ LoadAnyTaggedField(
+          r12, FieldOperand(rcx, rbx, times_tagged_size,
+                            FixedArray::kHeaderSize - kTaggedSize));
+      __ movq(Operand(rsp, rax, times_system_pointer_size, 0), r12);
+      __ leal(rax, Operand(rax, 1));
+      __ decl(rbx);
+      __ j(greater, &loop);
+    }
+
+    // Adjust effective number of arguments (rax contains the number of
+    // arguments from the call plus return address plus the number of
+    // [[BoundArguments]]), so we need to subtract one for the return address.
+    __ decl(rax);
+#endif  // !V8_REVERSE_JSARGS
   }
   __ bind(&no_bound_arguments);
 }
@@ -3180,6 +3491,7 @@ void Builtins::Generate_GenericJSToWasmWrapper(MacroAssembler* masm) {
 
   Register current_param = rbx;
   Register param_limit = rdx;
+#ifdef V8_REVERSE_JSARGS
   constexpr int kReceiverOnStackSize = kSystemPointerSize;
   __ movq(current_param,
           Immediate(kFPOnStackSize + kPCOnStackSize + kReceiverOnStackSize));
@@ -3188,6 +3500,13 @@ void Builtins::Generate_GenericJSToWasmWrapper(MacroAssembler* masm) {
   __ addq(param_limit,
           Immediate(kFPOnStackSize + kPCOnStackSize + kReceiverOnStackSize));
   const int increment = kSystemPointerSize;
+#else
+  __ movq(current_param, param_count);
+  __ shlq(current_param, Immediate(kSystemPointerSizeLog2));
+  __ addq(current_param, Immediate(kFPOnStackSize));
+  __ movq(param_limit, Immediate(kFPOnStackSize));
+  const int increment = -kSystemPointerSize;
+#endif
   Register param = rax;
   // We have to check the types of the params. The ValueType array contains
   // first the return then the param types.
@@ -3902,8 +4221,13 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
 
   // FunctionCallbackInfo::values_ (points at the first varargs argument passed
   // on the stack).
+#ifdef V8_REVERSE_JSARGS
   __ leaq(scratch,
           Operand(scratch, (FCA::kArgsLength + 1) * kSystemPointerSize));
+#else
+  __ leaq(scratch, Operand(scratch, argc, times_system_pointer_size,
+                           (FCA::kArgsLength - 1) * kSystemPointerSize));
+#endif
   __ movq(StackSpaceOperand(1), scratch);
 
   // FunctionCallbackInfo::length_.
diff --git a/src/codegen/arm/macro-assembler-arm.h b/src/codegen/arm/macro-assembler-arm.h
index a4d6632a07..46e108baa1 100644
--- a/src/codegen/arm/macro-assembler-arm.h
+++ b/src/codegen/arm/macro-assembler-arm.h
@@ -740,7 +740,11 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
   // TODO(victorgomes): Remove this function once we stick with the reversed
   // arguments order.
   MemOperand ReceiverOperand(Register argc) {
+#ifdef V8_REVERSE_JSARGS
     return MemOperand(sp, 0);
+#else
+    return MemOperand(sp, argc, LSL, kSystemPointerSizeLog2);
+#endif
   }
 
   // ---------------------------------------------------------------------------
diff --git a/src/codegen/arm64/macro-assembler-arm64.cc b/src/codegen/arm64/macro-assembler-arm64.cc
index 69242484bc..77b33ce507 100644
--- a/src/codegen/arm64/macro-assembler-arm64.cc
+++ b/src/codegen/arm64/macro-assembler-arm64.cc
@@ -2315,7 +2315,11 @@ void MacroAssembler::InvokeFunctionCode(Register function, Register new_target,
 }
 
 Operand MacroAssembler::ReceiverOperand(Register arg_count) {
+#ifdef V8_REVERSE_JSARGS
   return Operand(0);
+#else
+  return Operand(arg_count, LSL, kXRegSizeLog2);
+#endif
 }
 
 void MacroAssembler::InvokeFunctionWithNewTarget(
diff --git a/src/codegen/arm64/macro-assembler-arm64.h b/src/codegen/arm64/macro-assembler-arm64.h
index b453a17394..8231976adb 100644
--- a/src/codegen/arm64/macro-assembler-arm64.h
+++ b/src/codegen/arm64/macro-assembler-arm64.h
@@ -1788,6 +1788,8 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
     DecodeField<Field>(reg, reg);
   }
 
+  // TODO(victorgomes): inline this function once we remove V8_REVERSE_JSARGS
+  // flag.
   Operand ReceiverOperand(const Register arg_count);
 
   // ---- SMI and Number Utilities ----
diff --git a/src/codegen/code-stub-assembler.cc b/src/codegen/code-stub-assembler.cc
index ca340c69c8..78bfbc04ef 100644
--- a/src/codegen/code-stub-assembler.cc
+++ b/src/codegen/code-stub-assembler.cc
@@ -12765,28 +12765,50 @@ CodeStubArguments::CodeStubArguments(CodeStubAssembler* assembler,
       argc_(argc),
       base_(),
       fp_(fp != nullptr ? fp : assembler_->LoadFramePointer()) {
+#ifdef V8_REVERSE_JSARGS
   TNode<IntPtrT> offset = assembler_->IntPtrConstant(
       (StandardFrameConstants::kFixedSlotCountAboveFp + 1) *
       kSystemPointerSize);
+#else
+  TNode<IntPtrT> offset = assembler_->ElementOffsetFromIndex(
+      argc_, SYSTEM_POINTER_ELEMENTS,
+      (StandardFrameConstants::kFixedSlotCountAboveFp - 1) *
+          kSystemPointerSize);
+#endif
   // base_ points to the first argument, not the receiver
   // whether present or not.
   base_ = assembler_->RawPtrAdd(fp_, offset);
 }
 
 TNode<Object> CodeStubArguments::GetReceiver() const {
+#ifdef V8_REVERSE_JSARGS
   intptr_t offset = -kSystemPointerSize;
+#else
+  intptr_t offset = kSystemPointerSize;
+#endif
   return assembler_->LoadFullTagged(base_, assembler_->IntPtrConstant(offset));
 }
 
 void CodeStubArguments::SetReceiver(TNode<Object> object) const {
+#ifdef V8_REVERSE_JSARGS
   intptr_t offset = -kSystemPointerSize;
+#else
+  intptr_t offset = kSystemPointerSize;
+#endif
   assembler_->StoreFullTaggedNoWriteBarrier(
       base_, assembler_->IntPtrConstant(offset), object);
 }
 
 TNode<RawPtrT> CodeStubArguments::AtIndexPtr(TNode<IntPtrT> index) const {
+#ifdef V8_REVERSE_JSARGS
   TNode<IntPtrT> offset =
       assembler_->ElementOffsetFromIndex(index, SYSTEM_POINTER_ELEMENTS, 0);
+#else
+  TNode<IntPtrT> negated_index =
+      assembler_->IntPtrOrSmiSub(assembler_->IntPtrConstant(0), index);
+  TNode<IntPtrT> offset = assembler_->ElementOffsetFromIndex(
+      negated_index, SYSTEM_POINTER_ELEMENTS, 0);
+#endif
   return assembler_->RawPtrAdd(base_, offset);
 }
 
@@ -12832,7 +12854,11 @@ void CodeStubArguments::ForEach(
   }
   TNode<RawPtrT> start = AtIndexPtr(first);
   TNode<RawPtrT> end = AtIndexPtr(last);
+#ifdef V8_REVERSE_JSARGS
   const int increment = kSystemPointerSize;
+#else
+  const int increment = -kSystemPointerSize;
+#endif
   assembler_->BuildFastLoop<RawPtrT>(
       vars, start, end,
       [&](TNode<RawPtrT> current) {
@@ -13264,8 +13290,13 @@ TNode<Object> CodeStubAssembler::CallRuntimeNewArray(
   // Runtime_NewArray receives arguments in the JS order (to avoid unnecessary
   // copy). Except the last two (new_target and allocation_site) which are add
   // on top of the stack later.
+#ifdef V8_REVERSE_JSARGS
   return CallRuntime(Runtime::kNewArray, context, length, receiver, new_target,
                      allocation_site);
+#else
+  return CallRuntime(Runtime::kNewArray, context, receiver, length, new_target,
+                     allocation_site);
+#endif
 }
 
 void CodeStubAssembler::TailCallRuntimeNewArray(TNode<Context> context,
@@ -13276,8 +13307,13 @@ void CodeStubAssembler::TailCallRuntimeNewArray(TNode<Context> context,
   // Runtime_NewArray receives arguments in the JS order (to avoid unnecessary
   // copy). Except the last two (new_target and allocation_site) which are add
   // on top of the stack later.
+#ifdef V8_REVERSE_JSARGS
   return TailCallRuntime(Runtime::kNewArray, context, length, receiver,
                          new_target, allocation_site);
+#else
+  return TailCallRuntime(Runtime::kNewArray, context, receiver, length,
+                         new_target, allocation_site);
+#endif
 }
 
 TNode<JSArray> CodeStubAssembler::ArrayCreate(TNode<Context> context,
diff --git a/src/codegen/compiler.cc b/src/codegen/compiler.cc
index bb51b3be1e..d649a39ce6 100644
--- a/src/codegen/compiler.cc
+++ b/src/codegen/compiler.cc
@@ -606,6 +606,8 @@ void UpdateSharedFunctionFlagsAfterCompilation(FunctionLiteral* literal,
 
   shared_info.set_class_scope_has_private_brand(
       literal->class_scope_has_private_brand());
+  shared_info.set_is_safe_to_skip_arguments_adaptor(
+      literal->SafeToSkipArgumentsAdaptor());
   shared_info.set_has_static_private_methods_or_accessors(
       literal->has_static_private_methods_or_accessors());
 
diff --git a/src/codegen/ia32/macro-assembler-ia32.cc b/src/codegen/ia32/macro-assembler-ia32.cc
index b615c59185..1e00bc08e7 100644
--- a/src/codegen/ia32/macro-assembler-ia32.cc
+++ b/src/codegen/ia32/macro-assembler-ia32.cc
@@ -33,9 +33,16 @@ namespace internal {
 
 Operand StackArgumentsAccessor::GetArgumentOperand(int index) const {
   DCHECK_GE(index, 0);
+#ifdef V8_REVERSE_JSARGS
   // arg[0] = esp + kPCOnStackSize;
   // arg[i] = arg[0] + i * kSystemPointerSize;
   return Operand(esp, kPCOnStackSize + index * kSystemPointerSize);
+#else
+  // arg[0] = (esp + kPCOnStackSize) + argc * kSystemPointerSize;
+  // arg[i] = arg[0] - i * kSystemPointerSize;
+  return Operand(esp, argc_, times_system_pointer_size,
+                 kPCOnStackSize - index * kSystemPointerSize);
+#endif
 }
 
 // -------------------------------------------------------------------------
@@ -1264,7 +1271,13 @@ void MacroAssembler::CallDebugOnFunctionCall(Register fun, Register new_target,
   Push(fun);
   Push(fun);
   // Arguments are located 2 words below the base pointer.
+#ifdef V8_REVERSE_JSARGS
   Operand receiver_op = Operand(ebp, kSystemPointerSize * 2);
+#else
+  Operand receiver_op =
+      Operand(ebp, actual_parameter_count, times_system_pointer_size,
+              kSystemPointerSize * 2);
+#endif
   Push(receiver_op);
   CallRuntime(Runtime::kDebugOnFunctionCall);
   Pop(fun);
diff --git a/src/codegen/interface-descriptors.h b/src/codegen/interface-descriptors.h
index f086f23960..346a584d0e 100644
--- a/src/codegen/interface-descriptors.h
+++ b/src/codegen/interface-descriptors.h
@@ -111,7 +111,8 @@ enum class StackArgumentOrder {
   kJS,  // Arguments in the stack are pushed in the same order as the one used
         // by JS-to-JS function calls. This should be used if calling a
         // JSFunction or if the builtin is expected to be called directly from a
-        // JSFunction. This order is reversed compared to kDefault.
+        // JSFunction. When V8_REVERSE_JSARGS is set, this order is reversed
+        // compared to kDefault.
 };
 
 class V8_EXPORT_PRIVATE CallInterfaceDescriptorData {
@@ -505,7 +506,9 @@ STATIC_ASSERT(kMaxTFSBuiltinRegisterParams <= kMaxBuiltinRegisterParams);
                                     ##__VA_ARGS__)
 
 // When the extra arguments described here are located in the stack, they are
-// just above the return address in the frame (first arguments).
+// just above the return address in the frame. Therefore, they are either the
+// first arguments when V8_REVERSE_JSARGS is enabled, or otherwise the last
+// arguments.
 #define DEFINE_JS_PARAMETERS(...)                           \
   static constexpr int kDescriptorFlags =                   \
       CallInterfaceDescriptorData::kAllowVarArgs;           \
@@ -1159,6 +1162,7 @@ class ArrayNoArgumentConstructorDescriptor
                      ArrayNArgumentsConstructorDescriptor)
 };
 
+#ifdef V8_REVERSE_JSARGS
 class ArraySingleArgumentConstructorDescriptor
     : public ArrayNArgumentsConstructorDescriptor {
  public:
@@ -1176,6 +1180,25 @@ class ArraySingleArgumentConstructorDescriptor
   DECLARE_DESCRIPTOR(ArraySingleArgumentConstructorDescriptor,
                      ArrayNArgumentsConstructorDescriptor)
 };
+#else
+class ArraySingleArgumentConstructorDescriptor
+    : public ArrayNArgumentsConstructorDescriptor {
+ public:
+  // This descriptor declares same register arguments as the parent
+  // ArrayNArgumentsConstructorDescriptor and it declares indices for
+  // JS arguments passed on the expression stack.
+  DEFINE_PARAMETERS(kFunction, kAllocationSite, kActualArgumentsCount,
+                    kReceiverParameter, kArraySizeSmiParameter)
+  DEFINE_PARAMETER_TYPES(MachineType::AnyTagged(),  // kFunction
+                         MachineType::AnyTagged(),  // kAllocationSite
+                         MachineType::Int32(),      // kActualArgumentsCount
+                         // JS arguments on the stack
+                         MachineType::AnyTagged(),  // kReceiverParameter
+                         MachineType::AnyTagged())  // kArraySizeSmiParameter
+  DECLARE_DESCRIPTOR(ArraySingleArgumentConstructorDescriptor,
+                     ArrayNArgumentsConstructorDescriptor)
+};
+#endif
 
 class CompareDescriptor : public CallInterfaceDescriptor {
  public:
diff --git a/src/codegen/mips/macro-assembler-mips.h b/src/codegen/mips/macro-assembler-mips.h
index d91a4a7bb8..6cc906f07e 100644
--- a/src/codegen/mips/macro-assembler-mips.h
+++ b/src/codegen/mips/macro-assembler-mips.h
@@ -914,11 +914,21 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
   // TODO(victorgomes): Remove this function once we stick with the reversed
   // arguments order.
   void LoadReceiver(Register dest, Register argc) {
+#ifdef V8_REVERSE_JSARGS
     Lw(dest, MemOperand(sp, 0));
+#else
+    Lsa(dest, sp, argc, kPointerSizeLog2);
+    Lw(dest, MemOperand(dest, 0));
+#endif
   }
 
   void StoreReceiver(Register rec, Register argc, Register scratch) {
+#ifdef V8_REVERSE_JSARGS
     Sw(rec, MemOperand(sp, 0));
+#else
+    Lsa(scratch, sp, argc, kPointerSizeLog2);
+    Sw(rec, MemOperand(scratch, 0));
+#endif
   }
 
   // Swap two registers.  If the scratch register is omitted then a slightly
diff --git a/src/codegen/mips64/macro-assembler-mips64.h b/src/codegen/mips64/macro-assembler-mips64.h
index a0d5e59bf0..888eee3c01 100644
--- a/src/codegen/mips64/macro-assembler-mips64.h
+++ b/src/codegen/mips64/macro-assembler-mips64.h
@@ -921,11 +921,21 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
   // TODO(victorgomes): Remove this function once we stick with the reversed
   // arguments order.
   void LoadReceiver(Register dest, Register argc) {
+#ifdef V8_REVERSE_JSARGS
     Ld(dest, MemOperand(sp, 0));
+#else
+    Dlsa(dest, sp, argc, kPointerSizeLog2);
+    Ld(dest, MemOperand(dest, 0));
+#endif
   }
 
   void StoreReceiver(Register rec, Register argc, Register scratch) {
+#ifdef V8_REVERSE_JSARGS
     Sd(rec, MemOperand(sp, 0));
+#else
+    Dlsa(scratch, sp, argc, kPointerSizeLog2);
+    Sd(rec, MemOperand(scratch, 0));
+#endif
   }
 
   bool IsNear(Label* L, Condition cond, int rs_reg);
diff --git a/src/codegen/ppc/macro-assembler-ppc.h b/src/codegen/ppc/macro-assembler-ppc.h
index db0d6857ac..6fee10b160 100644
--- a/src/codegen/ppc/macro-assembler-ppc.h
+++ b/src/codegen/ppc/macro-assembler-ppc.h
@@ -729,11 +729,21 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
   // TODO(victorgomes): Remove this function once we stick with the reversed
   // arguments order.
   void LoadReceiver(Register dest, Register argc) {
+#ifdef V8_REVERSE_JSARGS
     LoadP(dest, MemOperand(sp, 0));
+#else
+    ShiftLeftImm(dest, argc, Operand(kSystemPointerSizeLog2));
+    LoadPX(dest, MemOperand(sp, dest));
+#endif
   }
 
   void StoreReceiver(Register rec, Register argc, Register scratch) {
+#ifdef V8_REVERSE_JSARGS
     StoreP(rec, MemOperand(sp, 0));
+#else
+    ShiftLeftImm(scratch, argc, Operand(kSystemPointerSizeLog2));
+    StorePX(rec, MemOperand(sp, scratch));
+#endif
   }
 
   // ---------------------------------------------------------------------------
diff --git a/src/codegen/s390/macro-assembler-s390.h b/src/codegen/s390/macro-assembler-s390.h
index f81dfb503b..22526f0f36 100644
--- a/src/codegen/s390/macro-assembler-s390.h
+++ b/src/codegen/s390/macro-assembler-s390.h
@@ -1073,11 +1073,21 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
   // TODO(victorgomes): Remove this function once we stick with the reversed
   // arguments order.
   void LoadReceiver(Register dest, Register argc) {
+#ifdef V8_REVERSE_JSARGS
     LoadP(dest, MemOperand(sp, 0));
+#else
+    ShiftLeftP(dest, argc, Operand(kSystemPointerSizeLog2));
+    LoadP(dest, MemOperand(sp, dest));
+#endif
   }
 
   void StoreReceiver(Register rec, Register argc, Register scratch) {
+#ifdef V8_REVERSE_JSARGS
     StoreP(rec, MemOperand(sp, 0));
+#else
+    ShiftLeftP(scratch, argc, Operand(kSystemPointerSizeLog2));
+    StoreP(rec, MemOperand(sp, scratch));
+#endif
   }
 
   void CallRuntime(const Runtime::Function* f, int num_arguments,
diff --git a/src/codegen/x64/macro-assembler-x64.cc b/src/codegen/x64/macro-assembler-x64.cc
index 9f5917c23a..8cd27cb515 100644
--- a/src/codegen/x64/macro-assembler-x64.cc
+++ b/src/codegen/x64/macro-assembler-x64.cc
@@ -37,9 +37,16 @@ namespace internal {
 
 Operand StackArgumentsAccessor::GetArgumentOperand(int index) const {
   DCHECK_GE(index, 0);
+#ifdef V8_REVERSE_JSARGS
   // arg[0] = rsp + kPCOnStackSize;
   // arg[i] = arg[0] + i * kSystemPointerSize;
   return Operand(rsp, kPCOnStackSize + index * kSystemPointerSize);
+#else
+  // arg[0] = (rsp + kPCOnStackSize) + argc * kSystemPointerSize;
+  // arg[i] = arg[0] - i * kSystemPointerSize;
+  return Operand(rsp, argc_, times_system_pointer_size,
+                 kPCOnStackSize - index * kSystemPointerSize);
+#endif
 }
 
 void MacroAssembler::Load(Register destination, ExternalReference source) {
@@ -2580,7 +2587,13 @@ void MacroAssembler::CallDebugOnFunctionCall(Register fun, Register new_target,
   Push(fun);
   Push(fun);
   // Arguments are located 2 words below the base pointer.
+#ifdef V8_REVERSE_JSARGS
   Operand receiver_op = Operand(rbp, kSystemPointerSize * 2);
+#else
+  Operand receiver_op =
+      Operand(rbp, actual_parameter_count, times_system_pointer_size,
+              kSystemPointerSize * 2);
+#endif
   Push(receiver_op);
   CallRuntime(Runtime::kDebugOnFunctionCall);
   Pop(fun);
diff --git a/src/compiler/escape-analysis-reducer.cc b/src/compiler/escape-analysis-reducer.cc
index f4ab1c9709..1f4a57c82a 100644
--- a/src/compiler/escape-analysis-reducer.cc
+++ b/src/compiler/escape-analysis-reducer.cc
@@ -305,6 +305,7 @@ void EscapeAnalysisReducer::Finalize() {
                 formal_parameter_count,
                 Type::Constant(params.formal_parameter_count(),
                                jsgraph()->graph()->zone()));
+#ifdef V8_REVERSE_JSARGS
             Node* offset_to_first_elem = jsgraph()->Constant(
                 CommonFrameConstants::kFixedSlotCountAboveFp);
             if (!NodeProperties::IsTyped(offset_to_first_elem)) {
@@ -326,6 +327,22 @@ void EscapeAnalysisReducer::Finalize() {
                   jsgraph()->simplified()->NumberAdd(), offset,
                   formal_parameter_count);
             }
+#else
+            // {offset} is a reverted index starting from 1. The base address is
+            // adapted to allow offsets starting from 1.
+            Node* offset = jsgraph()->graph()->NewNode(
+                jsgraph()->simplified()->NumberSubtract(), arguments_length,
+                index);
+            if (type == CreateArgumentsType::kRestParameter) {
+              // In the case of rest parameters we should skip the formal
+              // parameters.
+              NodeProperties::SetType(offset,
+                                      TypeCache::Get()->kArgumentsLengthType);
+              offset = jsgraph()->graph()->NewNode(
+                  jsgraph()->simplified()->NumberSubtract(), offset,
+                  formal_parameter_count);
+            }
+#endif
             NodeProperties::SetType(offset,
                                     TypeCache::Get()->kArgumentsLengthType);
             NodeProperties::ReplaceValueInput(load, arguments_frame, 0);
diff --git a/src/compiler/heap-refs.h b/src/compiler/heap-refs.h
index b268593a48..b22fcdfb01 100644
--- a/src/compiler/heap-refs.h
+++ b/src/compiler/heap-refs.h
@@ -794,6 +794,7 @@ class ScopeInfoRef : public HeapObjectRef {
   V(bool, HasBuiltinId)                                  \
   V(bool, construct_as_builtin)                          \
   V(bool, HasBytecodeArray)                              \
+  V(bool, is_safe_to_skip_arguments_adaptor)             \
   V(int, StartPosition)                                  \
   V(bool, is_compiled)                                   \
   V(bool, IsUserJavaScript)
diff --git a/src/compiler/js-generic-lowering.cc b/src/compiler/js-generic-lowering.cc
index 0b38bd538d..edf26e7f35 100644
--- a/src/compiler/js-generic-lowering.cc
+++ b/src/compiler/js-generic-lowering.cc
@@ -846,7 +846,9 @@ void JSGenericLowering::LowerJSConstruct(Node* node) {
     Node* stub_arity = jsgraph()->Int32Constant(arg_count);
     Node* slot = jsgraph()->Int32Constant(p.feedback().index());
     Node* receiver = jsgraph()->UndefinedConstant();
+#ifdef V8_REVERSE_JSARGS
     Node* feedback_vector = node->RemoveInput(n.FeedbackVectorIndex());
+#endif
     // Register argument inputs are followed by stack argument inputs (such as
     // feedback_vector). Both are listed in ascending order. Note that
     // the receiver is implicitly placed on the stack and is thus inserted
@@ -855,10 +857,16 @@ void JSGenericLowering::LowerJSConstruct(Node* node) {
     node->InsertInput(zone(), 0, stub_code);
     node->InsertInput(zone(), 3, stub_arity);
     node->InsertInput(zone(), 4, slot);
+#ifdef V8_REVERSE_JSARGS
     node->InsertInput(zone(), 5, feedback_vector);
     node->InsertInput(zone(), 6, receiver);
     // After: {code, target, new_target, arity, slot, vector, receiver,
     // ...args}.
+#else
+    node->InsertInput(zone(), 5, receiver);
+    // After: {code, target, new_target, arity, slot, receiver, ...args,
+    // vector}.
+#endif
 
     NodeProperties::ChangeOp(node, common()->Call(call_descriptor));
   } else {
@@ -907,7 +915,9 @@ void JSGenericLowering::LowerJSConstructWithArrayLike(Node* node) {
     Node* stub_code = jsgraph()->HeapConstant(callable.code());
     Node* receiver = jsgraph()->UndefinedConstant();
     Node* slot = jsgraph()->Int32Constant(p.feedback().index());
+#ifdef V8_REVERSE_JSARGS
     Node* feedback_vector = node->RemoveInput(n.FeedbackVectorIndex());
+#endif
     // Register argument inputs are followed by stack argument inputs (such as
     // feedback_vector). Both are listed in ascending order. Note that
     // the receiver is implicitly placed on the stack and is thus inserted
@@ -915,10 +925,16 @@ void JSGenericLowering::LowerJSConstructWithArrayLike(Node* node) {
     // TODO(jgruber): Implement a simpler way to specify these mutations.
     node->InsertInput(zone(), 0, stub_code);
     node->InsertInput(zone(), 4, slot);
+#ifdef V8_REVERSE_JSARGS
     node->InsertInput(zone(), 5, feedback_vector);
     node->InsertInput(zone(), 6, receiver);
     // After: {code, target, new_target, arguments_list, slot, vector,
     // receiver}.
+#else
+    node->InsertInput(zone(), 5, receiver);
+    // After: {code, target, new_target, arguments_list, slot, receiver,
+    // vector}.
+#endif
 
     NodeProperties::ChangeOp(node, common()->Call(call_descriptor));
   } else {
@@ -974,8 +990,10 @@ void JSGenericLowering::LowerJSConstructWithSpread(Node* node) {
     // on the stack here.
     Node* stub_arity = jsgraph()->Int32Constant(arg_count - kTheSpread);
     Node* receiver = jsgraph()->UndefinedConstant();
+#ifdef V8_REVERSE_JSARGS
     Node* feedback_vector = node->RemoveInput(n.FeedbackVectorIndex());
     Node* spread = node->RemoveInput(n.LastArgumentIndex());
+#endif
 
     // Register argument inputs are followed by stack argument inputs (such as
     // feedback_vector). Both are listed in ascending order. Note that
@@ -985,11 +1003,17 @@ void JSGenericLowering::LowerJSConstructWithSpread(Node* node) {
     node->InsertInput(zone(), 0, stub_code);
     node->InsertInput(zone(), 3, stub_arity);
     node->InsertInput(zone(), 4, slot);
+#ifdef V8_REVERSE_JSARGS
     node->InsertInput(zone(), 5, spread);
     node->InsertInput(zone(), 6, feedback_vector);
     node->InsertInput(zone(), 7, receiver);
     // After: {code, target, new_target, arity, slot, spread, vector, receiver,
     // ...args}.
+#else
+    node->InsertInput(zone(), 5, receiver);
+    // After: {code, target, new_target, arity, slot, receiver, ...args, spread,
+    // vector}.
+#endif
 
     NodeProperties::ChangeOp(node, common()->Call(call_descriptor));
   } else {
@@ -1173,14 +1197,20 @@ void JSGenericLowering::LowerJSCallWithSpread(Node* node) {
 
     // Shuffling inputs.
     // Before: {target, receiver, ...args, spread, vector}.
+#ifdef V8_REVERSE_JSARGS
     Node* feedback_vector = node->RemoveInput(n.FeedbackVectorIndex());
+#endif
     Node* spread = node->RemoveInput(n.LastArgumentIndex());
     node->InsertInput(zone(), 0, stub_code);
     node->InsertInput(zone(), 2, stub_arity);
     node->InsertInput(zone(), 3, spread);
     node->InsertInput(zone(), 4, slot);
+#ifdef V8_REVERSE_JSARGS
     node->InsertInput(zone(), 5, feedback_vector);
     // After: {code, target, arity, spread, slot, vector, receiver, ...args}.
+#else
+    // After: {code, target, arity, spread, slot, receiver, ...args, vector}.
+#endif
 
     NodeProperties::ChangeOp(node, common()->Call(call_descriptor));
   } else {
diff --git a/src/compiler/js-typed-lowering.cc b/src/compiler/js-typed-lowering.cc
index 9927cc0b70..ecfbb506ea 100644
--- a/src/compiler/js-typed-lowering.cc
+++ b/src/compiler/js-typed-lowering.cc
@@ -1492,6 +1492,8 @@ namespace {
 void ReduceBuiltin(JSGraph* jsgraph, Node* node, int builtin_index, int arity,
                    CallDescriptor::Flags flags) {
   // Patch {node} to a direct CEntry call.
+  //
+  // When V8_REVERSE_JSARGS is set:
   // ----------- A r g u m e n t s -----------
   // -- 0: CEntry
   // --- Stack args ---
@@ -1505,6 +1507,21 @@ void ReduceBuiltin(JSGraph* jsgraph, Node* node, int builtin_index, int arity,
   // -- 6 + n: the C entry point
   // -- 6 + n + 1: argc (Int32)
   // -----------------------------------
+  //
+  // Otherwise:
+  // ----------- A r g u m e n t s -----------
+  // -- 0: CEntry
+  // --- Stack args ---
+  // -- 1: receiver
+  // -- [2, 2 + n[: the n actual arguments passed to the builtin
+  // -- 2 + n: padding
+  // -- 2 + n + 1: argc, including the receiver and implicit args (Smi)
+  // -- 2 + n + 2: target
+  // -- 2 + n + 3: new_target
+  // --- Register args ---
+  // -- 2 + n + 4: the C entry point
+  // -- 2 + n + 5: argc (Int32)
+  // -----------------------------------
 
   // The logic contained here is mirrored in Builtins::Generate_Adaptor.
   // Keep these in sync.
@@ -1541,11 +1558,19 @@ void ReduceBuiltin(JSGraph* jsgraph, Node* node, int builtin_index, int arity,
   Node* argc_node = jsgraph->Constant(argc);
 
   static const int kStubAndReceiver = 2;
+#ifdef V8_REVERSE_JSARGS
   node->InsertInput(zone, 1, new_target);
   node->InsertInput(zone, 2, target);
   node->InsertInput(zone, 3, argc_node);
   node->InsertInput(zone, 4, jsgraph->PaddingConstant());
   int cursor = arity + kStubAndReceiver + BuiltinArguments::kNumExtraArgs;
+#else
+  int cursor = arity + kStubAndReceiver;
+  node->InsertInput(zone, cursor++, jsgraph->PaddingConstant());
+  node->InsertInput(zone, cursor++, argc_node);
+  node->InsertInput(zone, cursor++, target);
+  node->InsertInput(zone, cursor++, new_target);
+#endif
 
   Address entry = Builtins::CppEntryOf(builtin_index);
   ExternalReference entry_ref = ExternalReference::Create(entry);
@@ -1778,18 +1803,51 @@ Reduction JSTypedLowering::ReduceJSCall(Node* node) {
 #else
     if (NeedsArgumentAdaptorFrame(*shared, arity)) {
       node->RemoveInput(n.FeedbackVectorIndex());
-      // Patch {node} to an indirect call via the ArgumentsAdaptorTrampoline.
-      Callable callable = CodeFactory::ArgumentAdaptor(isolate());
-      node->InsertInput(graph()->zone(), 0,
-                        jsgraph()->HeapConstant(callable.code()));
-      node->InsertInput(graph()->zone(), 2, new_target);
-      node->InsertInput(graph()->zone(), 3, jsgraph()->Constant(arity));
-      node->InsertInput(
-          graph()->zone(), 4,
-          jsgraph()->Constant(shared->internal_formal_parameter_count()));
-      NodeProperties::ChangeOp(
-          node, common()->Call(Linkage::GetStubCallDescriptor(
-                    graph()->zone(), callable.descriptor(), 1 + arity, flags)));
+
+      // Check if it's safe to skip the arguments adaptor for {shared},
+      // that is whether the target function anyways cannot observe the
+      // actual arguments. Details can be found in this document at
+      // https://bit.ly/v8-faster-calls-with-arguments-mismatch and
+      // on the tracking bug at https://crbug.com/v8/8895
+      if (shared->is_safe_to_skip_arguments_adaptor()) {
+        // Currently we only support skipping arguments adaptor frames
+        // for strict mode functions, since there's Function.arguments
+        // legacy accessor, which is still available in sloppy mode.
+        DCHECK_EQ(LanguageMode::kStrict, shared->language_mode());
+
+        // Massage the arguments to match the expected number of arguments.
+        int expected_argument_count = shared->internal_formal_parameter_count();
+        for (; arity > expected_argument_count; --arity) {
+          node->RemoveInput(arity + 1);
+        }
+        for (; arity < expected_argument_count; ++arity) {
+          node->InsertInput(graph()->zone(), arity + 2,
+                            jsgraph()->UndefinedConstant());
+        }
+
+        // Patch {node} to a direct call.
+        node->InsertInput(graph()->zone(), arity + 2, new_target);
+        node->InsertInput(graph()->zone(), arity + 3,
+                          jsgraph()->Constant(arity));
+        NodeProperties::ChangeOp(node,
+                                 common()->Call(Linkage::GetJSCallDescriptor(
+                                     graph()->zone(), false, 1 + arity,
+                                     flags | CallDescriptor::kCanUseRoots)));
+      } else {
+        // Patch {node} to an indirect call via the ArgumentsAdaptorTrampoline.
+        Callable callable = CodeFactory::ArgumentAdaptor(isolate());
+        node->InsertInput(graph()->zone(), 0,
+                          jsgraph()->HeapConstant(callable.code()));
+        node->InsertInput(graph()->zone(), 2, new_target);
+        node->InsertInput(graph()->zone(), 3, jsgraph()->Constant(arity));
+        node->InsertInput(
+            graph()->zone(), 4,
+            jsgraph()->Constant(shared->internal_formal_parameter_count()));
+        NodeProperties::ChangeOp(
+            node,
+            common()->Call(Linkage::GetStubCallDescriptor(
+                graph()->zone(), callable.descriptor(), 1 + arity, flags)));
+      }
 #endif
     } else if (shared->HasBuiltinId() &&
                Builtins::IsCpp(shared->builtin_id())) {
diff --git a/src/compiler/linkage.cc b/src/compiler/linkage.cc
index ee025896c0..cde4b96c87 100644
--- a/src/compiler/linkage.cc
+++ b/src/compiler/linkage.cc
@@ -339,7 +339,11 @@ CallDescriptor* Linkage::GetJSCallDescriptor(Zone* zone, bool is_osr,
 
   // All parameters to JS calls go on the stack.
   for (int i = 0; i < js_parameter_count; i++) {
+#ifdef V8_REVERSE_JSARGS
     int spill_slot_index = -i - 1;
+#else
+    int spill_slot_index = i - js_parameter_count;
+#endif
     locations.AddParam(LinkageLocation::ForCallerFrameSlot(
         spill_slot_index, MachineType::AnyTagged()));
   }
diff --git a/src/compiler/linkage.h b/src/compiler/linkage.h
index ad68a57957..6352d39302 100644
--- a/src/compiler/linkage.h
+++ b/src/compiler/linkage.h
@@ -320,12 +320,16 @@ class V8_EXPORT_PRIVATE CallDescriptor final
   }
 
   int GetStackIndexFromSlot(int slot_index) const {
+#ifdef V8_REVERSE_JSARGS
     switch (GetStackArgumentOrder()) {
       case StackArgumentOrder::kDefault:
         return -slot_index - 1;
       case StackArgumentOrder::kJS:
         return slot_index + static_cast<int>(StackParameterCount());
     }
+#else
+    return -slot_index - 1;
+#endif
   }
 
   // The total number of inputs to this call, which includes the target,
diff --git a/src/compiler/wasm-compiler.cc b/src/compiler/wasm-compiler.cc
index d6b7113b27..3e8d876b49 100644
--- a/src/compiler/wasm-compiler.cc
+++ b/src/compiler/wasm-compiler.cc
@@ -6799,9 +6799,11 @@ class WasmWrapperGraphBuilder : public WasmGraphBuilder {
                                 args.begin());
         break;
       }
+      case WasmImportCallKind::kJSFunctionArityMismatchSkipAdaptor:
+        UNREACHABLE();
 #else
       // =======================================================================
-      // === JS Functions with mismatching arity ===============================
+      // === JS Functions with arguments adapter ===============================
       // =======================================================================
       case WasmImportCallKind::kJSFunctionArityMismatch: {
         base::SmallVector<Node*, 16> args(wasm_count + 9);
@@ -6843,6 +6845,47 @@ class WasmWrapperGraphBuilder : public WasmGraphBuilder {
         args[pos++] = effect();
         args[pos++] = control();
 
+        DCHECK_EQ(pos, args.size());
+        call = graph()->NewNode(mcgraph()->common()->Call(call_descriptor), pos,
+                                args.begin());
+        break;
+      }
+      // =======================================================================
+      // === JS Functions without arguments adapter ============================
+      // =======================================================================
+      case WasmImportCallKind::kJSFunctionArityMismatchSkipAdaptor: {
+        base::SmallVector<Node*, 16> args(expected_arity + 7);
+        int pos = 0;
+        Node* function_context =
+            gasm_->Load(MachineType::TaggedPointer(), callable_node,
+                        wasm::ObjectAccess::ContextOffsetInTaggedJSFunction());
+        args[pos++] = callable_node;  // target callable.
+
+        // Determine receiver at runtime.
+        args[pos++] =
+            BuildReceiverNode(callable_node, native_context, undefined_node);
+
+        auto call_descriptor = Linkage::GetJSCallDescriptor(
+            graph()->zone(), false, expected_arity + 1,
+            CallDescriptor::kNoFlags);
+
+        // Convert wasm numbers to JS values.
+        if (expected_arity <= wasm_count) {
+          pos = AddArgumentNodes(VectorOf(args), pos, expected_arity, sig_);
+        } else {
+          pos = AddArgumentNodes(VectorOf(args), pos, wasm_count, sig_);
+          for (int i = wasm_count; i < expected_arity; ++i) {
+            args[pos++] = undefined_node;
+          }
+        }
+
+        args[pos++] = undefined_node;  // new target
+        args[pos++] =
+            mcgraph()->Int32Constant(expected_arity);  // argument count
+        args[pos++] = function_context;
+        args[pos++] = effect();
+        args[pos++] = control();
+
         DCHECK_EQ(pos, args.size());
         call = graph()->NewNode(mcgraph()->common()->Call(call_descriptor), pos,
                                 args.begin());
@@ -7362,6 +7405,14 @@ std::pair<WasmImportCallKind, Handle<JSReceiver>> ResolveWasmImportCall(
       Compiler::Compile(function, Compiler::CLEAR_EXCEPTION,
                         &is_compiled_scope);
     }
+#ifndef V8_REVERSE_JSARGS
+    // This optimization is disabled when the arguments are reversed. It will be
+    // subsumed when the argumens adaptor frame is removed.
+    if (shared->is_safe_to_skip_arguments_adaptor()) {
+      return std::make_pair(
+          WasmImportCallKind::kJSFunctionArityMismatchSkipAdaptor, callable);
+    }
+#endif
 
     return std::make_pair(WasmImportCallKind::kJSFunctionArityMismatch,
                           callable);
diff --git a/src/compiler/wasm-compiler.h b/src/compiler/wasm-compiler.h
index c431f53efe..967d75edab 100644
--- a/src/compiler/wasm-compiler.h
+++ b/src/compiler/wasm-compiler.h
@@ -58,12 +58,15 @@ wasm::WasmCompilationResult ExecuteTurbofanWasmCompilation(
 // type of the target function/callable and whether the signature matches the
 // argument arity.
 enum class WasmImportCallKind : uint8_t {
-  kLinkError,                // static Wasm->Wasm type error
-  kRuntimeTypeError,         // runtime Wasm->JS type error
-  kWasmToCapi,               // fast Wasm->C-API call
-  kWasmToWasm,               // fast Wasm->Wasm call
-  kJSFunctionArityMatch,     // fast Wasm->JS call
-  kJSFunctionArityMismatch,  // Wasm->JS, needs adapter frame
+  kLinkError,                           // static Wasm->Wasm type error
+  kRuntimeTypeError,                    // runtime Wasm->JS type error
+  kWasmToCapi,                          // fast Wasm->C-API call
+  kWasmToWasm,                          // fast Wasm->Wasm call
+  kJSFunctionArityMatch,                // fast Wasm->JS call
+  kJSFunctionArityMismatch,             // Wasm->JS, needs adapter frame
+  kJSFunctionArityMismatchSkipAdaptor,  // Wasm->JS, arity mismatch calling
+                                        // strict mode function where we don't
+                                        // need the ArgumentsAdaptorTrampoline.
   // Math functions imported from JavaScript that are intrinsified
   kFirstMathIntrinsic,
   kF64Acos = kFirstMathIntrinsic,
diff --git a/src/deoptimizer/deoptimizer.cc b/src/deoptimizer/deoptimizer.cc
index 63b4431128..a1aec10d54 100644
--- a/src/deoptimizer/deoptimizer.cc
+++ b/src/deoptimizer/deoptimizer.cc
@@ -104,6 +104,7 @@ class FrameWriter {
 
   void PushStackJSArguments(TranslatedFrame::iterator& iterator,
                             int parameters_count) {
+#ifdef V8_REVERSE_JSARGS
     std::vector<TranslatedFrame::iterator> parameters;
     parameters.reserve(parameters_count);
     for (int i = 0; i < parameters_count; ++i, ++iterator) {
@@ -112,6 +113,11 @@ class FrameWriter {
     for (auto& parameter : base::Reversed(parameters)) {
       PushTranslatedValue(parameter, "stack parameter");
     }
+#else
+    for (int i = 0; i < parameters_count; ++i, ++iterator) {
+      PushTranslatedValue(iterator, "stack parameter");
+    }
+#endif
   }
 
   unsigned top_offset() const { return top_offset_; }
@@ -1760,6 +1766,7 @@ void Deoptimizer::DoComputeBuiltinContinuation(
     frame_writer.PushRawObject(roots.the_hole_value(), "padding\n");
   }
 
+#ifdef V8_REVERSE_JSARGS
   if (mode == BuiltinContinuationMode::STUB) {
     DCHECK_EQ(Builtins::CallInterfaceDescriptorFor(builtin_name)
                   .GetStackArgumentOrder(),
@@ -1799,6 +1806,34 @@ void Deoptimizer::DoComputeBuiltinContinuation(
     frame_writer.PushStackJSArguments(
         value_iterator, frame_info.translated_stack_parameter_count());
   }
+#else
+  for (uint32_t i = 0; i < frame_info.translated_stack_parameter_count();
+       ++i, ++value_iterator) {
+    frame_writer.PushTranslatedValue(value_iterator, "stack parameter");
+  }
+
+  switch (mode) {
+    case BuiltinContinuationMode::STUB:
+      break;
+    case BuiltinContinuationMode::JAVASCRIPT:
+      break;
+    case BuiltinContinuationMode::JAVASCRIPT_WITH_CATCH: {
+      frame_writer.PushRawObject(roots.the_hole_value(),
+                                 "placeholder for exception on lazy deopt\n");
+    } break;
+    case BuiltinContinuationMode::JAVASCRIPT_HANDLE_EXCEPTION: {
+      intptr_t accumulator_value =
+          input_->GetRegister(kInterpreterAccumulatorRegister.code());
+      frame_writer.PushRawObject(Object(accumulator_value),
+                                 "exception (from accumulator)\n");
+    } break;
+  }
+
+  if (frame_info.frame_has_result_stack_slot()) {
+    frame_writer.PushRawObject(roots.the_hole_value(),
+                               "placeholder for return result on lazy deopt\n");
+  }
+#endif
 
   DCHECK_EQ(output_frame->GetLastArgumentSlotOffset(),
             frame_writer.top_offset());
@@ -3123,13 +3158,19 @@ void TranslatedState::CreateArgumentsElementsTranslatedValues(
     frame.Add(TranslatedValue::NewTagged(this, roots.the_hole_value()));
   }
   int argc = length - number_of_holes;
+#ifdef V8_REVERSE_JSARGS
   int start_index = number_of_holes;
   if (type == CreateArgumentsType::kRestParameter) {
     start_index = std::max(0, formal_parameter_count_);
   }
+#endif
   for (int i = 0; i < argc; i++) {
     // Skip the receiver.
+#ifdef V8_REVERSE_JSARGS
     int offset = i + start_index + 1;
+#else
+    int offset = argc - i - 1;
+#endif
 #ifdef V8_NO_ARGUMENTS_ADAPTOR
     Address arguments_frame = offset > formal_parameter_count_
                                   ? stack_frame_pointer_
diff --git a/src/diagnostics/objects-debug.cc b/src/diagnostics/objects-debug.cc
index 6ee2d39f45..988e794460 100644
--- a/src/diagnostics/objects-debug.cc
+++ b/src/diagnostics/objects-debug.cc
@@ -872,6 +872,11 @@ void SharedFunctionInfo::SharedFunctionInfoVerify(ReadOnlyRoots roots) {
       CHECK(!construct_as_builtin());
     }
   }
+
+  // At this point we only support skipping arguments adaptor frames
+  // for strict mode functions (see https://crbug.com/v8/8895).
+  CHECK_IMPLIES(is_safe_to_skip_arguments_adaptor(),
+                language_mode() == LanguageMode::kStrict);
 }
 
 void JSGlobalProxy::JSGlobalProxyVerify(Isolate* isolate) {
diff --git a/src/diagnostics/objects-printer.cc b/src/diagnostics/objects-printer.cc
index d65c0eeb4b..f9a987ec34 100644
--- a/src/diagnostics/objects-printer.cc
+++ b/src/diagnostics/objects-printer.cc
@@ -1237,6 +1237,9 @@ void JSFunction::JSFunctionPrint(std::ostream& os) {  // NOLINT
 
   os << "\n - formal_parameter_count: "
      << shared().internal_formal_parameter_count();
+  if (shared().is_safe_to_skip_arguments_adaptor()) {
+    os << "\n - safe_to_skip_arguments_adaptor";
+  }
   os << "\n - kind: " << shared().kind();
   os << "\n - context: " << Brief(context());
   os << "\n - code: " << Brief(code());
@@ -1321,6 +1324,9 @@ void SharedFunctionInfo::SharedFunctionInfoPrint(std::ostream& os) {  // NOLINT
   }
   os << "\n - function_map_index: " << function_map_index();
   os << "\n - formal_parameter_count: " << internal_formal_parameter_count();
+  if (is_safe_to_skip_arguments_adaptor()) {
+    os << "\n - safe_to_skip_arguments_adaptor";
+  }
   os << "\n - expected_nof_properties: " << expected_nof_properties();
   os << "\n - language_mode: " << language_mode();
   os << "\n - data: " << Brief(function_data(kAcquireLoad));
diff --git a/src/execution/arguments.h b/src/execution/arguments.h
index 39877cf4d2..d2798e6f76 100644
--- a/src/execution/arguments.h
+++ b/src/execution/arguments.h
@@ -62,9 +62,11 @@ class Arguments {
   inline Address* address_of_arg_at(int index) const {
     DCHECK_LE(static_cast<uint32_t>(index), static_cast<uint32_t>(length_));
     uintptr_t offset = index * kSystemPointerSize;
+#ifdef V8_REVERSE_JSARGS
     if (arguments_type == ArgumentsType::kJS) {
       offset = (length_ - index - 1) * kSystemPointerSize;
     }
+#endif
     return reinterpret_cast<Address*>(reinterpret_cast<Address>(arguments_) -
                                       offset);
   }
@@ -75,13 +77,17 @@ class Arguments {
   // Arguments on the stack are in reverse order (compared to an array).
   FullObjectSlot first_slot() const {
     int index = length() - 1;
+#ifdef V8_REVERSE_JSARGS
     if (arguments_type == ArgumentsType::kJS) index = 0;
+#endif
     return slot_at(index);
   }
 
   FullObjectSlot last_slot() const {
     int index = 0;
+#ifdef V8_REVERSE_JSARGS
     if (arguments_type == ArgumentsType::kJS) index = length() - 1;
+#endif
     return slot_at(index);
   }
 
diff --git a/src/execution/frame-constants.h b/src/execution/frame-constants.h
index 1c0a1f65f0..3b90ea576f 100644
--- a/src/execution/frame-constants.h
+++ b/src/execution/frame-constants.h
@@ -21,15 +21,18 @@ namespace internal {
 // header, with slot index 2 corresponding to the current function context and 3
 // corresponding to the frame marker/JSFunction.
 //
+// If V8_REVERSE_JSARGS is set, then the parameters are reversed in the stack,
+// i.e., the first parameter (the receiver) is just above the return address.
+//
 //  slot      JS frame
 //       +-----------------+--------------------------------
-//  -n-1 |   parameter n   |                            ^
+//  -n-1 |   parameter 0   |                            ^
 //       |- - - - - - - - -|                            |
-//  -n   |  parameter n-1  |                          Caller
+//  -n   |                 |                          Caller
 //  ...  |       ...       |                       frame slots
-//  -2   |   parameter 1   |                       (slot < 0)
+//  -2   |  parameter n-1  |                       (slot < 0)
 //       |- - - - - - - - -|                            |
-//  -1   |   parameter 0   |                            v
+//  -1   |   parameter n   |                            v
 //  -----+-----------------+--------------------------------
 //   0   |   return addr   |   ^                        ^
 //       |- - - - - - - - -|   |                        |
@@ -79,13 +82,13 @@ class CommonFrameConstants : public AllStatic {
 //
 //  slot      JS frame
 //       +-----------------+--------------------------------
-//  -n-1 |   parameter n   |                            ^
+//  -n-1 |   parameter 0   |                            ^
 //       |- - - - - - - - -|                            |
-//  -n   |  parameter n-1  |                          Caller
+//  -n   |                 |                          Caller
 //  ...  |       ...       |                       frame slots
-//  -2   |   parameter 1   |                       (slot < 0)
+//  -2   |  parameter n-1  |                       (slot < 0)
 //       |- - - - - - - - -|                            |
-//  -1   |   parameter 0   |                            v
+//  -1   |   parameter n   |                            v
 //  -----+-----------------+--------------------------------
 //   0   |   return addr   |   ^                        ^
 //       |- - - - - - - - -|   |                        |
@@ -130,13 +133,13 @@ class StandardFrameConstants : public CommonFrameConstants {
 //
 //  slot      JS frame
 //       +-----------------+--------------------------------
-//  -n-1 |   parameter n   |                            ^
+//  -n-1 |   parameter 0   |                            ^
 //       |- - - - - - - - -|                            |
-//  -n   |  parameter n-1  |                          Caller
+//  -n   |                 |                          Caller
 //  ...  |       ...       |                       frame slots
-//  -2   |   parameter 1   |                       (slot < 0)
+//  -2   |  parameter n-1  |                       (slot < 0)
 //       |- - - - - - - - -|                            |
-//  -1   |   parameter 0   |                            v
+//  -1   |   parameter n   |                            v
 //  -----+-----------------+--------------------------------
 //   0   |   return addr   |   ^                        ^
 //       |- - - - - - - - -|   |                        |
@@ -302,8 +305,13 @@ class InterpreterFrameConstants : public StandardFrameConstants {
       STANDARD_FRAME_EXTRA_PUSHED_VALUE_OFFSET(1);
   DEFINE_STANDARD_FRAME_SIZES(2);
 
+#ifdef V8_REVERSE_JSARGS
   static constexpr int kFirstParamFromFp =
       StandardFrameConstants::kCallerSPOffset;
+#else
+  static constexpr int kLastParamFromFp =
+      StandardFrameConstants::kCallerSPOffset;
+#endif
   static constexpr int kRegisterFileFromFp =
       -kFixedFrameSizeFromFp - kSystemPointerSize;
   static constexpr int kExpressionsOffset = kRegisterFileFromFp;
diff --git a/src/execution/frames-inl.h b/src/execution/frames-inl.h
index 3cee9d5855..5ad142130d 100644
--- a/src/execution/frames-inl.h
+++ b/src/execution/frames-inl.h
@@ -130,8 +130,17 @@ inline Object BuiltinExitFrame::receiver_slot_object() const {
   // fp[4]: argc.
   // fp[5]: hole.
   // ------- JS stack arguments ------
-  // fp[6]: receiver
+  // fp[6]: receiver, if V8_REVERSE_JSARGS.
+  // fp[2 + argc - 1]: receiver, if not V8_REVERSE_JSARGS.
+#ifdef V8_REVERSE_JSARGS
   const int receiverOffset = BuiltinExitFrameConstants::kFirstArgumentOffset;
+#else
+  Object argc_slot = argc_slot_object();
+  DCHECK(argc_slot.IsSmi());
+  int argc = Smi::ToInt(argc_slot);
+  const int receiverOffset = BuiltinExitFrameConstants::kNewTargetOffset +
+                             (argc - 1) * kSystemPointerSize;
+#endif
   return Object(base::Memory<Address>(fp() + receiverOffset));
 }
 
@@ -201,7 +210,12 @@ Address CommonFrameWithJSLinkage::GetParameterSlot(int index) const {
   DCHECK(index < ComputeParametersCount() ||
          ComputeParametersCount() == kDontAdaptArgumentsSentinel);
 #endif
+#ifdef V8_REVERSE_JSARGS
   int parameter_offset = (index + 1) * kSystemPointerSize;
+#else
+  int param_count = ComputeParametersCount();
+  int parameter_offset = (param_count - index - 1) * kSystemPointerSize;
+#endif
   return caller_sp() + parameter_offset;
 }
 
diff --git a/src/execution/frames.cc b/src/execution/frames.cc
index 3288f53c8d..5d8a2ac4f5 100644
--- a/src/execution/frames.cc
+++ b/src/execution/frames.cc
@@ -1331,10 +1331,16 @@ Object JavaScriptBuiltinContinuationFrame::context() const {
 
 void JavaScriptBuiltinContinuationWithCatchFrame::SetException(
     Object exception) {
+#ifdef V8_REVERSE_JSARGS
   int argc = ComputeParametersCount();
   Address exception_argument_slot =
       fp() + BuiltinContinuationFrameConstants::kFixedFrameSizeAboveFp +
       (argc - 1) * kSystemPointerSize;
+#else
+  Address exception_argument_slot =
+      fp() + BuiltinContinuationFrameConstants::kFixedFrameSizeAboveFp +
+      kSystemPointerSize;  // Skip over return value slot.
+#endif
 
   // Only allow setting exception if previous value was the hole.
   CHECK_EQ(ReadOnlyRoots(isolate()).the_hole_value(),
@@ -1636,6 +1642,23 @@ DeoptimizationData OptimizedFrame::GetDeoptimizationData(
   return DeoptimizationData();
 }
 
+#ifndef V8_REVERSE_JSARGS
+Object OptimizedFrame::receiver() const {
+  Code code = LookupCode();
+  if (code.kind() == CodeKind::BUILTIN) {
+    intptr_t argc = static_cast<int>(
+        Memory<intptr_t>(fp() + StandardFrameConstants::kArgCOffset));
+    intptr_t args_size =
+        (StandardFrameConstants::kFixedSlotCountAboveFp + argc) *
+        kSystemPointerSize;
+    Address receiver_ptr = fp() + args_size;
+    return *FullObjectSlot(receiver_ptr);
+  } else {
+    return JavaScriptFrame::receiver();
+  }
+}
+#endif
+
 void OptimizedFrame::GetFunctions(
     std::vector<SharedFunctionInfo>* functions) const {
   DCHECK(functions->empty());
diff --git a/src/execution/frames.h b/src/execution/frames.h
index 43f9d383c2..01421a743f 100644
--- a/src/execution/frames.h
+++ b/src/execution/frames.h
@@ -821,6 +821,11 @@ class OptimizedFrame : public JavaScriptFrame {
 
   DeoptimizationData GetDeoptimizationData(int* deopt_index) const;
 
+#ifndef V8_REVERSE_JSARGS
+  // When the arguments are reversed in the stack, receiver() is
+  // inherited from JavaScriptFrame.
+  Object receiver() const override;
+#endif
   int ComputeParametersCount() const override;
 
   static int StackSlotOffsetRelativeToFp(int slot_index);
diff --git a/src/interpreter/bytecode-register-optimizer.cc b/src/interpreter/bytecode-register-optimizer.cc
index 3d9c9e1dac..5bea9c8c02 100644
--- a/src/interpreter/bytecode-register-optimizer.cc
+++ b/src/interpreter/bytecode-register-optimizer.cc
@@ -233,7 +233,11 @@ BytecodeRegisterOptimizer::BytecodeRegisterOptimizer(
   // a vector of register metadata.
   // There is at least one parameter, which is the JS receiver.
   DCHECK_NE(parameter_count, 0);
+#ifdef V8_REVERSE_JSARGS
   int first_slot_index = parameter_count - 1;
+#else
+  int first_slot_index = 0;
+#endif
   register_info_table_offset_ =
       -Register::FromParameterIndex(first_slot_index, parameter_count).index();
 
diff --git a/src/interpreter/bytecode-register.cc b/src/interpreter/bytecode-register.cc
index e8eb347f16..13d831e8b7 100644
--- a/src/interpreter/bytecode-register.cc
+++ b/src/interpreter/bytecode-register.cc
@@ -8,10 +8,17 @@ namespace v8 {
 namespace internal {
 namespace interpreter {
 
+#ifdef V8_REVERSE_JSARGS
 static const int kFirstParamRegisterIndex =
     (InterpreterFrameConstants::kRegisterFileFromFp -
      InterpreterFrameConstants::kFirstParamFromFp) /
     kSystemPointerSize;
+#else
+static const int kLastParamRegisterIndex =
+    (InterpreterFrameConstants::kRegisterFileFromFp -
+     InterpreterFrameConstants::kLastParamFromFp) /
+    kSystemPointerSize;
+#endif
 static const int kFunctionClosureRegisterIndex =
     (InterpreterFrameConstants::kRegisterFileFromFp -
      StandardFrameConstants::kFunctionOffset) /
@@ -36,14 +43,22 @@ static const int kCallerPCOffsetRegisterIndex =
 Register Register::FromParameterIndex(int index, int parameter_count) {
   DCHECK_GE(index, 0);
   DCHECK_LT(index, parameter_count);
+#ifdef V8_REVERSE_JSARGS
   int register_index = kFirstParamRegisterIndex - index;
+#else
+  int register_index = kLastParamRegisterIndex - parameter_count + index + 1;
+#endif
   DCHECK_LT(register_index, 0);
   return Register(register_index);
 }
 
 int Register::ToParameterIndex(int parameter_count) const {
   DCHECK(is_parameter());
+#ifdef V8_REVERSE_JSARGS
   return kFirstParamRegisterIndex - index();
+#else
+  return index() - kLastParamRegisterIndex + parameter_count - 1;
+#endif
 }
 
 Register Register::function_closure() {
diff --git a/src/interpreter/interpreter-assembler.cc b/src/interpreter/interpreter-assembler.cc
index 596783b64f..20f6fef07a 100644
--- a/src/interpreter/interpreter-assembler.cc
+++ b/src/interpreter/interpreter-assembler.cc
@@ -773,9 +773,15 @@ void InterpreterAssembler::CallJSAndDispatch(TNode<Object> function,
 
   if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
     // The first argument parameter (the receiver) is implied to be undefined.
+#ifdef V8_REVERSE_JSARGS
     TailCallStubThenBytecodeDispatch(callable.descriptor(), code_target,
                                      context, function, arg_count, args...,
                                      UndefinedConstant());
+#else
+    TailCallStubThenBytecodeDispatch(callable.descriptor(), code_target,
+                                     context, function, arg_count,
+                                     UndefinedConstant(), args...);
+#endif
   } else {
     TailCallStubThenBytecodeDispatch(callable.descriptor(), code_target,
                                      context, function, arg_count, args...);
@@ -1400,8 +1406,14 @@ TNode<FixedArray> InterpreterAssembler::ExportParametersAndRegisterFile(
     // Iterate over parameters and write them into the array.
     Label loop(this, &var_index), done_loop(this);
 
+#ifdef V8_REVERSE_JSARGS
     TNode<IntPtrT> reg_base =
         IntPtrConstant(Register::FromParameterIndex(0, 1).ToOperand() + 1);
+#else
+    TNode<IntPtrT> reg_base = IntPtrAdd(
+        IntPtrConstant(Register::FromParameterIndex(0, 1).ToOperand() - 1),
+        formal_parameter_count_intptr);
+#endif
 
     Goto(&loop);
     BIND(&loop);
@@ -1410,7 +1422,11 @@ TNode<FixedArray> InterpreterAssembler::ExportParametersAndRegisterFile(
       GotoIfNot(UintPtrLessThan(index, formal_parameter_count_intptr),
                 &done_loop);
 
+#ifdef V8_REVERSE_JSARGS
       TNode<IntPtrT> reg_index = IntPtrAdd(reg_base, index);
+#else
+      TNode<IntPtrT> reg_index = IntPtrSub(reg_base, index);
+#endif
       TNode<Object> value = LoadRegister(reg_index);
 
       StoreFixedArrayElement(array, index, value);
diff --git a/src/interpreter/interpreter-generator.cc b/src/interpreter/interpreter-generator.cc
index 3b7172867e..bfa58de021 100644
--- a/src/interpreter/interpreter-generator.cc
+++ b/src/interpreter/interpreter-generator.cc
@@ -1404,17 +1404,32 @@ class InterpreterJSCallAssembler : public InterpreterAssembler {
             LoadRegisterAtOperandIndex(kFirstArgumentOperandIndex));
         break;
       case 2:
+#ifdef V8_REVERSE_JSARGS
         CallJSAndDispatch(
             function, context, Int32Constant(arg_count), receiver_mode,
             LoadRegisterAtOperandIndex(kFirstArgumentOperandIndex + 1),
             LoadRegisterAtOperandIndex(kFirstArgumentOperandIndex));
+#else
+        CallJSAndDispatch(
+            function, context, Int32Constant(arg_count), receiver_mode,
+            LoadRegisterAtOperandIndex(kFirstArgumentOperandIndex),
+            LoadRegisterAtOperandIndex(kFirstArgumentOperandIndex + 1));
+#endif
         break;
       case 3:
+#ifdef V8_REVERSE_JSARGS
         CallJSAndDispatch(
             function, context, Int32Constant(arg_count), receiver_mode,
             LoadRegisterAtOperandIndex(kFirstArgumentOperandIndex + 2),
             LoadRegisterAtOperandIndex(kFirstArgumentOperandIndex + 1),
             LoadRegisterAtOperandIndex(kFirstArgumentOperandIndex));
+#else
+        CallJSAndDispatch(
+            function, context, Int32Constant(arg_count), receiver_mode,
+            LoadRegisterAtOperandIndex(kFirstArgumentOperandIndex),
+            LoadRegisterAtOperandIndex(kFirstArgumentOperandIndex + 1),
+            LoadRegisterAtOperandIndex(kFirstArgumentOperandIndex + 2));
+#endif
         break;
       default:
         UNREACHABLE();
diff --git a/src/objects/shared-function-info-inl.h b/src/objects/shared-function-info-inl.h
index caf14e8bc3..e46e958cf1 100644
--- a/src/objects/shared-function-info-inl.h
+++ b/src/objects/shared-function-info-inl.h
@@ -220,6 +220,9 @@ BIT_FIELD_ACCESSORS(SharedFunctionInfo, flags, is_toplevel,
 BIT_FIELD_ACCESSORS(SharedFunctionInfo, flags,
                     is_oneshot_iife_or_properties_are_final,
                     SharedFunctionInfo::IsOneshotIifeOrPropertiesAreFinalBit)
+BIT_FIELD_ACCESSORS(SharedFunctionInfo, flags,
+                    is_safe_to_skip_arguments_adaptor,
+                    SharedFunctionInfo::IsSafeToSkipArgumentsAdaptorBit)
 BIT_FIELD_ACCESSORS(SharedFunctionInfo, flags,
                     private_name_lookup_skips_outer_class,
                     SharedFunctionInfo::PrivateNameLookupSkipsOuterClassBit)
diff --git a/src/objects/shared-function-info.cc b/src/objects/shared-function-info.cc
index 885d88e689..53ce42420d 100644
--- a/src/objects/shared-function-info.cc
+++ b/src/objects/shared-function-info.cc
@@ -498,6 +498,8 @@ void SharedFunctionInfo::InitFromFunctionLiteral(
   if (lit->ShouldEagerCompile()) {
     shared_info->set_has_duplicate_parameters(lit->has_duplicate_parameters());
     shared_info->UpdateAndFinalizeExpectedNofPropertiesFromEstimate(lit);
+    shared_info->set_is_safe_to_skip_arguments_adaptor(
+        lit->SafeToSkipArgumentsAdaptor());
     DCHECK_NULL(lit->produced_preparse_data());
 
     // If we're about to eager compile, we'll have the function literal
@@ -505,6 +507,7 @@ void SharedFunctionInfo::InitFromFunctionLiteral(
     return;
   }
 
+  shared_info->set_is_safe_to_skip_arguments_adaptor(false);
   shared_info->UpdateExpectedNofPropertiesFromEstimate(lit);
 
   Handle<UncompiledData> data;
diff --git a/src/objects/shared-function-info.h b/src/objects/shared-function-info.h
index be6705e327..d17ea256fd 100644
--- a/src/objects/shared-function-info.h
+++ b/src/objects/shared-function-info.h
@@ -468,6 +468,17 @@ class SharedFunctionInfo : public HeapObject {
   // Whether or not the number of expected properties may change.
   DECL_BOOLEAN_ACCESSORS(are_properties_final)
 
+  // Indicates that the function represented by the shared function info
+  // cannot observe the actual parameters passed at a call site, which
+  // means the function doesn't use the arguments object, doesn't use
+  // rest parameters, and is also in strict mode (meaning that there's
+  // no way to get to the actual arguments via the non-standard "arguments"
+  // accessor on sloppy mode functions). This can be used to speed up calls
+  // to this function even in the presence of arguments mismatch.
+  // See http://bit.ly/v8-faster-calls-with-arguments-mismatch for more
+  // information on this.
+  DECL_BOOLEAN_ACCESSORS(is_safe_to_skip_arguments_adaptor)
+
   // Indicates that the function has been reported for binary code coverage.
   DECL_BOOLEAN_ACCESSORS(has_reported_binary_coverage)
 
diff --git a/src/objects/shared-function-info.tq b/src/objects/shared-function-info.tq
index 838703454c..17ec1f2fea 100644
--- a/src/objects/shared-function-info.tq
+++ b/src/objects/shared-function-info.tq
@@ -37,6 +37,7 @@ bitfield struct SharedFunctionInfoFlags extends uint32 {
   has_reported_binary_coverage: bool: 1 bit;
   is_top_level: bool: 1 bit;
   is_oneshot_iife_or_properties_are_final: bool: 1 bit;
+  is_safe_to_skip_arguments_adaptor: bool: 1 bit;
   private_name_lookup_skips_outer_class: bool: 1 bit;
 }
 
diff --git a/src/runtime/runtime-array.cc b/src/runtime/runtime-array.cc
index 623064fd8a..3e72d5e816 100644
--- a/src/runtime/runtime-array.cc
+++ b/src/runtime/runtime-array.cc
@@ -47,8 +47,13 @@ RUNTIME_FUNCTION(Runtime_NewArray) {
   DCHECK_LE(3, args.length());
   int const argc = args.length() - 3;
   // argv points to the arguments constructed by the JavaScript call.
+#ifdef V8_REVERSE_JSARGS
   JavaScriptArguments argv(argc, args.address_of_arg_at(0));
   CONVERT_ARG_HANDLE_CHECKED(JSFunction, constructor, argc);
+#else
+  JavaScriptArguments argv(argc, args.address_of_arg_at(1));
+  CONVERT_ARG_HANDLE_CHECKED(JSFunction, constructor, 0);
+#endif
   CONVERT_ARG_HANDLE_CHECKED(JSReceiver, new_target, argc + 1);
   CONVERT_ARG_HANDLE_CHECKED(HeapObject, type_info, argc + 2);
   // TODO(bmeurer): Use MaybeHandle to pass around the AllocationSite.
diff --git a/src/wasm/module-instantiate.cc b/src/wasm/module-instantiate.cc
index e8b0a4f8e6..0dd5d27930 100644
--- a/src/wasm/module-instantiate.cc
+++ b/src/wasm/module-instantiate.cc
@@ -1081,7 +1081,9 @@ bool InstanceBuilder::ProcessImportedFunction(
       // The imported function is a callable.
 
       int expected_arity = static_cast<int>(expected_sig->parameter_count());
-      if (kind == compiler::WasmImportCallKind::kJSFunctionArityMismatch) {
+      if (kind == compiler::WasmImportCallKind::kJSFunctionArityMismatch ||
+          kind == compiler::WasmImportCallKind::
+                      kJSFunctionArityMismatchSkipAdaptor) {
         Handle<JSFunction> function = Handle<JSFunction>::cast(js_receiver);
         SharedFunctionInfo shared = function->shared();
         expected_arity = shared.internal_formal_parameter_count();
@@ -1456,7 +1458,9 @@ void InstanceBuilder::CompileImportWrappers(
 
     int expected_arity = static_cast<int>(sig->parameter_count());
     if (resolved.first ==
-        compiler::WasmImportCallKind::kJSFunctionArityMismatch) {
+            compiler::WasmImportCallKind::kJSFunctionArityMismatch ||
+        resolved.first ==
+            compiler::WasmImportCallKind::kJSFunctionArityMismatchSkipAdaptor) {
       Handle<JSFunction> function = Handle<JSFunction>::cast(resolved.second);
       SharedFunctionInfo shared = function->shared();
       expected_arity = shared.internal_formal_parameter_count();
diff --git a/src/wasm/wasm-objects.cc b/src/wasm/wasm-objects.cc
index d06caef486..16bd084efc 100644
--- a/src/wasm/wasm-objects.cc
+++ b/src/wasm/wasm-objects.cc
@@ -1508,9 +1508,12 @@ void WasmInstanceObject::ImportWasmJSFunctionIntoTable(
     callable = resolved.second;  // Update to ultimate target.
     DCHECK_NE(compiler::WasmImportCallKind::kLinkError, kind);
     wasm::CompilationEnv env = native_module->CreateCompilationEnv();
-    // {expected_arity} should only be used if kind != kJSFunctionArityMismatch.
+    // {expected_arity} should only be used if kind != kJSFunctionArityMismatch
+    // or kind != kJSFunctionArityMismatchSkipAdaptor.
     int expected_arity = -1;
-    if (kind == compiler::WasmImportCallKind ::kJSFunctionArityMismatch) {
+    if (kind == compiler::WasmImportCallKind ::kJSFunctionArityMismatch ||
+        kind == compiler::WasmImportCallKind ::
+                    kJSFunctionArityMismatchSkipAdaptor) {
       expected_arity = Handle<JSFunction>::cast(callable)
                            ->shared()
                            .internal_formal_parameter_count();
@@ -2016,7 +2019,13 @@ Handle<WasmJSFunction> WasmJSFunction::New(Isolate* isolate,
       SharedFunctionInfo shared = Handle<JSFunction>::cast(callable)->shared();
       expected_arity = shared.internal_formal_parameter_count();
       if (expected_arity != parameter_count) {
+#ifdef V8_REVERSE_JSARGS
         kind = CK::kJSFunctionArityMismatch;
+#else
+        kind = shared.is_safe_to_skip_arguments_adaptor()
+                   ? CK::kJSFunctionArityMismatchSkipAdaptor
+                   : CK::kJSFunctionArityMismatch;
+#endif
       }
     }
     // TODO(wasm): Think about caching and sharing the wasm-to-JS wrappers per
